[{"title":"Python教程笔记(3/4)","date":"2018-03-01T07:49:18.000Z","path":"2018/03/01/myPython_b/","text":"IO编程本章的IO编程都是同步模式 文件读写Python内置了读写文件的函数，用法和C是兼容的。 读文件 要以读文件的模式打开一个文件对象，使用Python内置的open()函数，传入文件名和标示符： 1&gt;&gt;&gt; f = open('/Users/michael/test.txt', 'r') 标示符’r’表示读，这样，我们就成功地打开了一个文件。 如果文件不存在，open()函数就会抛出一个IOError的错误，并且给出错误码和详细的信息告诉你文件不存在： 1234&gt;&gt;&gt; f=open('/Users/michael/notfound.txt', 'r')Traceback (most recent call last): File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;FileNotFoundError: [Errno 2] No such file or directory: '/Users/michael/notfound.txt' 如果文件打开成功，接下来，调用read()方法可以一次读取文件的全部内容，Python把内容读到内存，用一个str对象表示： 12&gt;&gt;&gt; f.read()'Hello, world!' 最后一步是调用close()方法关闭文件。文件使用完毕后必须关闭，因为文件对象会占用操作系统的资源，并且操作系统同一时间能打开的文件数量也是有限的： 1&gt;&gt;&gt; f.close() 由于文件读写时都有可能产生IOError，一旦出错，后面的f.close()就不会调用。所以，为了保证无论是否出错都能正确地关闭文件，我们可以使用try ... finally来实现： 12345678910111213try: f = open('/path/to/file', 'r') print(f.read())finally: if f: f.close()``` 但是每次都这么写实在太繁琐，所以，Python引入了`with`语句来自动帮我们调用`close()`方法：```Pythonwith open('/path/to/file', 'r') as f: print(f.read()) 这和前面的try ... finally是一样的，但是代码更佳简洁，并且不必调用f.close()方法。 调用read()会一次性读取文件的全部内容，如果文件有10G，内存就爆了，所以，要保险起见，可以反复调用read(size)方法，每次最多读取size个字节的内容。另外，调用readline()可以每次读取一行内容，调用readlines()一次读取所有内容并按行返回list。因此，要根据需要决定怎么调用。 如果文件很小，read()一次性读取最方便；如果不能确定文件大小，反复调用read(size)比较保险；如果是配置文件，调用readlines()最方便： 12for line in f.readlines(): print(line.strip()) # 把末尾的'\\n'删掉 file-like Object像open()函数返回的这种有个read()方法的对象，在Python中统称为file-like Object。除了file外，还可以是内存的字节流，网络流，自定义流等等。file-like Object不要求从特定类继承，只要写个read()方法就行。 StringIO就是在内存中创建的file-like Object，常用作临时缓冲。 二进制文件 前面讲的默认都是读取文本文件，并且是UTF-8编码的文本文件。要读取二进制文件，比如图片、视频等等，用&#39;rb&#39;模式打开文件即可： 123&gt;&gt;&gt; f = open('/Users/michael/test.jpg', 'rb')&gt;&gt;&gt; f.read()b'\\xff\\xd8\\xff\\xe1\\x00\\x18Exif\\x00\\x00...' # 十六进制表示的字节 写文件 写文件和读文件是一样的，唯一区别是调用open()函数时，传入标识符&#39;w&#39;或者&#39;wb&#39;表示写文本文件或写二进制文件： 123&gt;&gt;&gt; f = open('/Users/michael/test.txt', 'w')&gt;&gt;&gt; f.write('Hello, world!')&gt;&gt;&gt; f.close() 你可以反复调用write()来写入文件，但是务必要调用f.close()来关闭文件。当我们写文件时，操作系统往往不会立刻把数据写入磁盘，而是放到内存缓存起来，空闲的时候再慢慢写入。只有调用close()方法时，操作系统才保证把没有写入的数据全部写入磁盘。忘记调用close()的后果是数据可能只写了一部分到磁盘，剩下的丢失了。所以，还是用with语句来得保险： 12with open('/Users/michael/test.txt', 'w') as f: f.write('Hello, world!') 要写入特定编码的文本文件，请给open()函数传入encoding参数，将字符串自动转换成指定编码。 细心的童鞋会发现，以&#39;w&#39;模式写入文件时，如果文件已存在，会直接覆盖（相当于删掉后新写入一个文件）。如果我们希望追加到文件末尾怎么办？可以传入&#39;a&#39;以追加（append）模式写入。 所有模式的定义及含义可以参考Python的官方文档。 StringIO和BytesIOStringIO 很多时候，数据读写不一定是文件，也可以在内存中读写。 StringIO顾名思义就是在内存中读写str。 要把str写入StringIO，我们需要先创建一个StringIO，然后，像文件一样写入即可： 12345678910&gt;&gt;&gt; from io import StringIO&gt;&gt;&gt; f = StringIO()&gt;&gt;&gt; f.write('hello')5&gt;&gt;&gt; f.write(' ')1&gt;&gt;&gt; f.write('world!')6&gt;&gt;&gt; print(f.getvalue())hello world! getvalue()方法用于获得写入后的str。 要读取StringIO，可以用一个str初始化StringIO，然后，像读文件一样读取： 1234567891011&gt;&gt;&gt; from io import StringIO&gt;&gt;&gt; f = StringIO('Hello!\\nHi!\\nGoodbye!')&gt;&gt;&gt; while True:... s = f.readline()... if s == '':... break... print(s.strip())...Hello!Hi!Goodbye! BytesIO StringIO操作的只能是str，如果要操作二进制数据，就需要使用BytesIO。BytesIO实现了在内存中读写bytes，我们创建一个BytesIO，然后写入一些bytes： 123456&gt;&gt;&gt; from io import BytesIO&gt;&gt;&gt; f = BytesIO()&gt;&gt;&gt; f.write('中文'.encode('utf-8'))6&gt;&gt;&gt; print(f.getvalue())b'\\xe4\\xb8\\xad\\xe6\\x96\\x87' 请注意，写入的不是str，而是经过UTF-8编码的bytes。 和StringIO类似，可以用一个bytes初始化BytesIO，然后，像读文件一样读取： 1234&gt;&gt;&gt; from io import BytesIO&gt;&gt;&gt; f = BytesIO(b'\\xe4\\xb8\\xad\\xe6\\x96\\x87')&gt;&gt;&gt; f.read()b'\\xe4\\xb8\\xad\\xe6\\x96\\x87' 操作文件和目录Python内置的os模块也可以直接调用操作系统提供的接口函数。 打开Python交互式命令行，我们来看看如何使用os模块的基本功能： 123&gt;&gt;&gt; import os&gt;&gt;&gt; os.name # 操作系统类型'nt' 如果是posix，说明系统是Linux、Unix或Mac OS X，如果是nt，就是Windows系统。 要获取详细的系统信息，可以调用uname()函数： 12&gt;&gt;&gt; os.uname()posix.uname_result(sysname='Darwin', nodename='MichaelMacPro.local', release='14.3.0', version='Darwin Kernel Version 14.3.0: Mon Mar 23 11:59:05 PDT 2015; root:xnu-2782.20.48~5/RELEASE_X86_64', machine='x86_64') 注意uname()函数在Windows上不提供，也就是说，os模块的某些函数是跟操作系统相关的。 环境变量 在操作系统中定义的环境变量，全部保存在os.environ这个变量中，可以直接查看： 12&gt;&gt;&gt; os.environenviron(&#123;'VERSIONER_PYTHON_PREFER_32_BIT': 'no', 'TERM_PROGRAM_VERSION': '326', 'LOGNAME': 'michael', 'USER': 'michael', 'PATH': '/usr/bin:/bin:/usr/sbin:/sbin:/usr/local/bin:/opt/X11/bin:/usr/local/mysql/bin', ...&#125;) 要获取某个环境变量的值，可以调用os.environ.get(&#39;key&#39;)： 1234&gt;&gt;&gt; os.environ.get('PATH')'/usr/bin:/bin:/usr/sbin:/sbin:/usr/local/bin:/opt/X11/bin:/usr/local/mysql/bin'&gt;&gt;&gt; os.environ.get('x', 'default')'default' 第二句是什么意思？ 操作文件和目录 操作文件和目录的函数一部分放在os模块中，一部分放在os.path模块中，这一点要注意一下。查看、创建和删除目录可以这么调用： 12345678910# 查看当前目录的绝对路径MAC:&gt;&gt;&gt; os.path.abspath('.')'/Users/michael'# 在某个目录下创建一个新目录，首先把新目录的完整路径表示出来:&gt;&gt;&gt; os.path.join('/Users/michael', 'testdir')'/Users/michael/testdir'# 创建一个目录:&gt;&gt;&gt; os.mkdir('/Users/michael/testdir')# 删掉一个目录:&gt;&gt;&gt; os.rmdir('/Users/michael/testdir') 把两个路径合成一个时，不要直接拼字符串，而要通过os.path.join()函数，这样可以正确处理不同操作系统的路径分隔符。在Linux/Unix/Mac下，os.path.join()返回这样的字符串： 1part-1/part-2 而Windows下会返回这样的字符串： 1part-1\\part-2 同样的道理，要拆分路径时，也不要直接去拆字符串，而要通过os.path.split()函数，这样可以把一个路径拆分为两部分，后一部分总是最后级别的目录或文件名： 12&gt;&gt;&gt; os.path.split('/Users/michael/testdir/file.txt')('/Users/michael/testdir', 'file.txt') os.path.splitext()可以直接让你得到文件扩展名，很多时候非常方便： 12&gt;&gt;&gt; os.path.splitext('/path/to/file.txt')('/path/to/file', '.txt') 这些合并、拆分路径的函数并不要求目录和文件要真实存在，它们只对字符串进行操作。 文件操作使用下面的函数。假定当前目录下有一个test.txt文件： 1234# 对文件重命名:&gt;&gt;&gt; os.rename('test.txt', 'test.py')# 删掉文件:&gt;&gt;&gt; os.remove('test.py') 但是复制文件的函数居然在os模块中不存在！原因是复制文件并非由操作系统提供的系统调用。理论上讲，我们通过上一节的读写文件可以完成文件复制，只不过要多写很多代码。 幸运的是shutil模块提供了copyfile()的函数，你还可以在shutil模块中找到很多实用函数，它们可以看做是os模块的补充。 最后看看如何利用Python的特性来过滤文件。比如我们要列出当前目录下的所有目录，只需要一行代码： 12&gt;&gt;&gt; [x for x in os.listdir('.') if os.path.isdir(x)]['.lein', '.local', '.m2', '.npm', '.ssh', '.Trash', '.vim', 'Applications', 'Desktop', ...] 要列出所有的.py文件，也只需一行代码： 123&gt;&gt;&gt; [x for x in os.listdir('.') if os.path.isfile(x) and os.path.splitext(x)[1]=='.py']['apis.py', 'config.py', 'models.py', 'pymonitor.py', 'test_db.py', 'urls.py', 'wsgiapp.py'] Python的os模块封装了操作系统的目录和文件操作，要注意这些函数有的在os模块中，有的在os.path模块中。 练习编写一个程序，能在当前目录以及当前目录的所有子目录下查找文件名包含指定字符串的文件，并打印出相对路径。 1234567891011121314151617def findFile(str, path='.'): for f in os.listdir(path): fPath = os.path.join(path, f) if os.path.isfile(fPath) and str in f: print(fPath) if os.path.isdir(fPath): findFile(str, fPath)``` ---## 序列化在程序运行的过程中，所有的变量都是在内存中，比如，定义一个`dict`：```pythond = dict(name='Bob', age=20, score=88) 可以随时修改变量，比如把name改成&#39;Bill&#39;，但是一旦程序结束，变量所占用的内存就被操作系统全部回收。如果没有把修改后的&#39;Bill&#39;存储到磁盘上，下次重新运行程序，变量又被初始化为&#39;Bob&#39;。 我们把变量从内存中变成可存储或传输的过程称之为 序列化，在Python中叫pickling，在其他语言中也被称之为serialization，marshalling，flattening等等，都是一个意思。 序列化之后，就可以把序列化后的内容写入磁盘，或者通过网络传输到别的机器上。反过来，把变量内容从序列化的对象重新读到内存里称之为反序列化，即unpickling。 Python提供了pickle模块来实现序列化。 首先，我们尝试把一个对象序列化并写入文件： 1234&gt;&gt;&gt; import pickle&gt;&gt;&gt; d = dict(name='Bob', age=20, score=88)&gt;&gt;&gt; pickle.dumps(d)b'\\x80\\x03&#125;q\\x00(X\\x03\\x00\\x00\\x00ageq\\x01K\\x14X\\x05\\x00\\x00\\x00scoreq\\x02KXX\\x04\\x00\\x00\\x00nameq\\x03X\\x03\\x00\\x00\\x00Bobq\\x04u.' pickle.dumps()方法把任意对象序列化成一个bytes，然后，就可以把这个bytes写入文件。或者用另一个方法pickle.dump()直接把对象序列化后写入一个file-like Object： 123&gt;&gt;&gt; f = open('dump.txt', 'wb')&gt;&gt;&gt; pickle.dump(d, f)&gt;&gt;&gt; f.close() 看看写入的dump.txt文件，一堆乱七八糟的内容，这些都是Python保存的对象内部信息。 当我们要把对象从磁盘读到内存时，可以先把内容读到一个bytes，然后用pickle.loads()方法反序列化出对象，也可以直接用pickle.load()方法从一个file-like Object中直接反序列化出对象。我们打开另一个Python命令行来反序列化刚才保存的对象： 12345&gt;&gt;&gt; f = open('dump.txt', 'rb')&gt;&gt;&gt; d = pickle.load(f)&gt;&gt;&gt; f.close()&gt;&gt;&gt; d&#123;'age': 20, 'score': 88, 'name': 'Bob'&#125; 变量的内容又回来了！ 当然，这个变量和原来的变量是完全不相干的对象，它们只是内容相同而已。 Pickle的问题和所有其他编程语言特有的序列化问题一样，就是它只能用于Python，并且可能不同版本的Python彼此都不兼容，因此，只能用Pickle保存那些不重要的数据，不能成功地反序列化也没关系。 JSON如果我们要在不同的编程语言之间传递对象，就必须把对象序列化为标准格式，比如XML，但更好的方法是序列化为JSON，因为JSON表示出来就是一个字符串，可以被所有语言读取，也可以方便地存储到磁盘或者通过网络传输。JSON不仅是标准格式，并且比XML更快，而且可以直接在Web页面中读取，非常方便。 JSON表示的对象就是标准的JavaScript语言的对象，JSON和Python内置的数据类型对应如下： JSON类型 Python类型 {} dict [] list “string” str 1234.56 int或float true/false True/False null None Python内置的json模块提供了非常完善的Python对象到JSON格式的转换。我们先看看如何把Python对象变成一个JSON： 1234&gt;&gt;&gt; import json&gt;&gt;&gt; d = dict(name='Bob', age=20, score=88)&gt;&gt;&gt; json.dumps(d)'&#123;\"age\": 20, \"score\": 88, \"name\": \"Bob\"&#125;' dumps()方法返回一个str，内容就是标准的JSON。类似的，dump()方法可以直接把JSON写入一个file-like Object。 要把JSON反序列化为Python对象，用loads()或者对应的load()方法，前者把JSON的字符串反序列化，后者从file-like Object中读取字符串并反序列化： 123&gt;&gt;&gt; json_str = '&#123;\"age\": 20, \"score\": 88, \"name\": \"Bob\"&#125;'&gt;&gt;&gt; json.loads(json_str)&#123;'age': 20, 'score': 88, 'name': 'Bob'&#125; 由于JSON标准规定JSON编码是UTF-8，所以我们总是能正确地在Python的str与JSON的字符串之间转换。 JSON进阶 Python的dict对象可以直接序列化为JSON的{}，不过，很多时候，我们更喜欢用class表示对象，比如定义Student类，然后序列化： 12345678910import jsonclass Student(object): def __init__(self, name, age, score): self.name = name self.age = age self.score = scores = Student('Bob', 20, 88)print(json.dumps(s)) 运行代码，毫不留情地得到一个TypeError： 123Traceback (most recent call last): ...TypeError: &lt;__main__.Student object at 0x10603cc50&gt; is not JSON serializable 错误的原因是Student对象不是一个可序列化为JSON的对象。如果连class的实例对象都无法序列化为JSON，这肯定不合理！ 别急，我们仔细看看dumps()方法的参数列表，可以发现，除了第一个必须的obj参数外，dumps()方法还提供了一大堆的可选参数： https://docs.python.org/3/library/json.html#json.dumps 这些可选参数就是让我们来定制JSON序列化。前面的代码之所以无法把Student类实例序列化为JSON，是因为默认情况下，dumps()方法不知道如何将Student实例变为一个JSON的{}对象。 可选参数default就是把任意一个对象变成一个可序列为JSON的对象，我们只需要为Student专门写一个转换函数，再把函数传进去即可： 123456def student2dict(std): return &#123; 'name': std.name, 'age': std.age, 'score': std.score &#125; 这样，Student实例首先被student2dict()函数转换成dict，然后再被顺利序列化为JSON： 12&gt;&gt;&gt; print(json.dumps(s, default=student2dict))&#123;\"age\": 20, \"name\": \"Bob\", \"score\": 88&#125; 不过，下次如果遇到一个Teacher类的实例，照样无法序列化为JSON。我们可以偷个懒，把任意class的实例变为dict： 1print(json.dumps(s, default=lambda obj: obj.__dict__)) 因为通常class的实例都有一个__dict__属性，它就是一个dict，用来存储实例变量。也有少数例外，比如定义了__slots__的class。 同样的道理，如果我们要把JSON反序列化为一个Student对象实例，loads()方法首先转换出一个dict对象，然后，我们传入的object_hook函数负责把dict转换为Student实例： 12def dict2student(d): return Student(d['name'], d['age'], d['score']) 运行结果如下： 123&gt;&gt;&gt; json_str = '&#123;\"age\": 20, \"score\": 88, \"name\": \"Bob\"&#125;'&gt;&gt;&gt; print(json.loads(json_str, object_hook=dict2student))&lt;__main__.Student object at 0x10cd3c190&gt; 打印出的是反序列化的Student实例对象。 进程和线程对于操作系统来说，一个任务就是一个进程（Process），比如打开一个浏览器就是启动一个浏览器进程，打开一个记事本就启动了一个记事本进程，打开两个记事本就启动了两个记事本进程，打开一个Word就启动了一个Word进程。 有些进程还不止同时干一件事，比如Word，它可以同时进行打字、拼写检查、打印等事情。在一个进程内部，要同时干多件事，就需要同时运行多个“子任务”，我们把进程内的这些“子任务”称为线程（Thread）。 多进程要让Python程序实现多进程（multiprocessing），我们先了解操作系统的相关知识。 Unix/Linux操作系统提供了一个fork()系统调用，它非常特殊。普通的函数调用，调用一次，返回一次，但是fork()调用一次，返回两次，因为操作系统自动把当前进程（称为父进程）复制了一份（称为子进程），然后，分别在父进程和子进程内返回。 子进程永远返回0，而父进程返回子进程的ID。这样做的理由是，一个父进程可以fork出很多子进程，所以，父进程要记下每个子进程的ID，而子进程只需要调用getppid()就可以拿到父进程的ID。 Python的os模块封装了常见的系统调用，其中就包括fork，可以在Python程序中轻松创建子进程： 123456789import osprint('Process (%s) start...' % os.getpid())# Only works on Unix/Linux/Mac:pid = os.fork()if pid == 0: print('I am child process (%s) and my parent is %s.' % (os.getpid(), os.getppid()))else: print('I (%s) just created a child process (%s).' % (os.getpid(), pid)) 运行结果如下： 123Process (876) start...I (876) just created a child process (877).I am child process (877) and my parent is 876. 由于Windows没有fork调用，上面的代码在Windows上无法运行。由于Mac系统是基于BSD（Unix的一种）内核，所以，在Mac下运行是没有问题的。 有了fork调用，一个进程在接到新任务时就可以复制出一个子进程来处理新任务，常见的Apache服务器就是由父进程监听端口，每当有新的http请求时，就fork出子进程来处理新的http请求。 multiprocessing 如果你打算编写多进程的服务程序，Unix/Linux无疑是正确的选择。由于Windows没有fork调用，难道在Windows上无法用Python编写多进程的程序？ 由于Python是跨平台的，自然也应该提供一个跨平台的多进程支持。multiprocessing模块就是跨平台版本的多进程模块。multiprocessing模块提供了一个Process类来代表一个进程对象，下面的例子演示了启动一个子进程并等待其结束： 1234567891011121314from multiprocessing import Processimport os# 子进程要执行的代码def run_proc(name): print('Run child process %s (%s)...' % (name, os.getpid()))if __name__=='__main__': print('Parent process %s.' % os.getpid()) p = Process(target=run_proc, args=('test',)) print('Child process will start.') p.start() p.join() print('Child process end.') 执行结果如下： 1234Parent process 928.Process will start.Run child process test (929)...Process end. 创建子进程时，只需要传入一个执行函数和函数的参数，创建一个Process实例， 用start()方法启动，这样创建进程比fork()还要简单。join()方法可以等待子进程结束后再继续往下运行，通常用于进程间的同步。 Pool 如果要启动大量的子进程，可以用进程池的方式批量创建子进程： 12345678910111213141516171819from multiprocessing import Poolimport os, time, randomdef long_time_task(name): print('Run task %s (%s)...' % (name, os.getpid())) start = time.time() time.sleep(random.random() * 3) end = time.time() print('Task %s runs %0.2f seconds.' % (name, (end - start)))if __name__=='__main__': print('Parent process %s.' % os.getpid()) p = Pool(4) for i in range(5): p.apply_async(long_time_task, args=(i,)) print('Waiting for all subprocesses done...') p.close() p.join() print('All subprocesses done.') 执行结果如下： 12345678910111213Parent process 669.Waiting for all subprocesses done...Run task 0 (671)...Run task 1 (672)...Run task 2 (673)...Run task 3 (674)...Task 2 runs 0.14 seconds.Run task 4 (673)...Task 1 runs 0.27 seconds.Task 3 runs 0.86 seconds.Task 0 runs 1.41 seconds.Task 4 runs 1.91 seconds.All subprocesses done. 代码解读： 对Pool对象调用join()方法会等待所有子进程执行完毕，调用join()之前必须先调用close()，调用close()之后就不能继续添加新的Process了。 请注意输出的结果，task 0，1，2，3是立刻执行的，而task 4要等待前面某个task完成后才执行，这是因为Pool的默认大小在我的电脑上是4，因此，最多同时执行4个进程。这是Pool有意设计的限制，并不是操作系统的限制。如果改成： 1p = Pool(5) 就可以同时跑5个进程。 由于Pool的默认大小是CPU的核数，如果你不幸拥有8核CPU，你要提交至少9个子进程才能看到上面的等待效果。 子进程很多时候，子进程并不是自身，而是一个外部进程。我们创建了子进程后，还需要控制子进程的输入和输出。 subprocess模块可以让我们非常方便地启动一个子进程，然后控制其输入和输出。 下面的例子演示了如何在Python代码中运行命令nslookup www.python.org，这和命令行直接运行的效果是一样的： 12345import subprocessprint('$ nslookup www.python.org')r = subprocess.call(['nslookup', 'www.python.org'])print('Exit code:', r) 运行结果： 12345678910$ nslookup www.python.orgServer: 192.168.19.4Address: 192.168.19.4#53Non-authoritative answer:www.python.org canonical name = python.map.fastly.net.Name: python.map.fastly.netAddress: 199.27.79.223Exit code: 0 如果子进程还需要输入，则可以通过communicate()方法输入： 1234567import subprocessprint('$ nslookup')p = subprocess.Popen(['nslookup'], stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE)output, err = p.communicate(b'set q=mx\\npython.org\\nexit\\n')print(output.decode('utf-8'))print('Exit code:', p.returncode) 上面的代码相当于在命令行执行命令nslookup，然后手动输入： 123set q=mxpython.orgexit 运行结果如下： 123456789101112$ nslookupServer: 192.168.19.4Address: 192.168.19.4#53Non-authoritative answer:python.org mail exchanger = 50 mail.python.org.Authoritative answers can be found from:mail.python.org internet address = 82.94.164.166mail.python.org has AAAA address 2001:888:2000:d::a6Exit code: 0 进程间通信Process之间肯定是需要通信的，操作系统提供了很多机制来实现进程间的通信。Python的multiprocessing模块包装了底层的机制，提供了Queue、Pipes等多种方式来交换数据。 我们以Queue为例，在父进程中创建两个子进程，一个往Queue里写数据，一个从Queue里读数据： 12345678910111213141516171819202122232425262728293031from multiprocessing import Process, Queueimport os, time, random# 写数据进程执行的代码:def write(q): print('Process to write: %s' % os.getpid()) for value in ['A', 'B', 'C']: print('Put %s to queue...' % value) q.put(value) time.sleep(random.random())# 读数据进程执行的代码:def read(q): print('Process to read: %s' % os.getpid()) while True: value = q.get(True) print('Get %s from queue.' % value)if __name__=='__main__': # 父进程创建Queue，并传给各个子进程： q = Queue() pw = Process(target=write, args=(q,)) pr = Process(target=read, args=(q,)) # 启动子进程pw，写入: pw.start() # 启动子进程pr，读取: pr.start() # 等待pw结束: pw.join() # pr进程里是死循环，无法等待其结束，只能强行终止: pr.terminate() 运行结果如下： 12345678Process to write: 50563Put A to queue...Process to read: 50564Get A from queue.Put B to queue...Get B from queue.Put C to queue...Get C from queue. 由于Windows没有fork调用，因此，multiprocessing需要“模拟”出fork的效果，父进程所有Python对象都必须通过pickle序列化再传到子进程去，所有，如果multiprocessing在Windows下调用失败了，要先考虑是不是pickle失败了。 多线程多任务可以由多进程完成，也可以由一个进程内的多线程完成。 由于线程是操作系统直接支持的执行单元，因此，高级语言通常都内置多线程的支持，Python也不例外，并且，Python的线程是真正的Posix Thread，而不是模拟出来的线程。 Python的标准库提供了两个模块：_thread和threading，_thread是低级模块，threading是高级模块，对_thread进行了封装。绝大多数情况下，我们只需要使用threading这个高级模块。 启动一个线程就是把一个函数传入并创建Thread实例，然后调用start()开始执行： 1234567891011121314151617import time, threading# 新线程执行的代码:def loop(): print('thread %s is running...' % threading.current_thread().name) n = 0 while n &lt; 5: n = n + 1 print('thread %s &gt;&gt;&gt; %s' % (threading.current_thread().name, n)) time.sleep(1) print('thread %s ended.' % threading.current_thread().name)print('thread %s is running...' % threading.current_thread().name)t = threading.Thread(target=loop, name='LoopThread')t.start()t.join()print('thread %s ended.' % threading.current_thread().name) 执行结果如下： 123456789thread MainThread is running...thread LoopThread is running...thread LoopThread &gt;&gt;&gt; 1thread LoopThread &gt;&gt;&gt; 2thread LoopThread &gt;&gt;&gt; 3thread LoopThread &gt;&gt;&gt; 4thread LoopThread &gt;&gt;&gt; 5thread LoopThread ended.thread MainThread ended. 由于任何进程默认就会启动一个线程，我们把该线程称为主线程，主线程又可以启动新的线程，Python的threading模块有个current_thread()函数，它永远返回当前线程的实例。主线程实例的名字叫MainThread，子线程的名字在创建时指定，我们用LoopThread命名子线程。名字仅仅在打印时用来显示，完全没有其他意义，如果不起名字Python就自动给线程命名为Thread-1，Thread-2…… Lock 多线程和多进程最大的不同在于，多进程中，同一个变量，各自有一份拷贝存在于每个进程中，互不影响，而多线程中，所有变量都由所有线程共享，所以，任何一个变量都可以被任何一个线程修改，因此，线程之间共享数据最大的危险在于多个线程同时改一个变量，把内容给改乱了。 来看看多个线程同时操作一个变量怎么把内容给改乱了： 12345678910111213141516171819202122import time, threading# 假定这是你的银行存款:balance = 0def change_it(n): # 先存后取，结果应该为0: global balance balance = balance + n balance = balance - ndef run_thread(n): for i in range(100000): change_it(n)t1 = threading.Thread(target=run_thread, args=(5,))t2 = threading.Thread(target=run_thread, args=(8,))t1.start()t2.start()t1.join()t2.join()print(balance) 我们定义了一个共享变量balance，初始值为0，并且启动两个线程，先存后取，理论上结果应该为0，但是，由于线程的调度是由操作系统决定的，当t1、t2交替执行时，只要循环次数足够多，balance的结果就不一定是0了。 原因是因为高级语言的一条语句在CPU执行时是若干条语句，即使一个简单的计算： 1balance = balance + n 也分两步： 计算balance + n，存入临时变量中； 将临时变量的值赋给balance。 也就是可以看成： 12x = balance + nbalance = x 由于x是局部变量，两个线程各自都有自己的x，当代码正常执行时： 12345678910111213初始值 balance = 0t1: x1 = balance + 5 # x1 = 0 + 5 = 5t1: balance = x1 # balance = 5t1: x1 = balance - 5 # x1 = 5 - 5 = 0t1: balance = x1 # balance = 0t2: x2 = balance + 8 # x2 = 0 + 8 = 8t2: balance = x2 # balance = 8t2: x2 = balance - 8 # x2 = 8 - 8 = 0t2: balance = x2 # balance = 0结果 balance = 0 但是t1和t2是交替运行的，如果操作系统以下面的顺序执行t1、t2： 123456789101112131415初始值 balance = 0t1: x1 = balance + 5 # x1 = 0 + 5 = 5t2: x2 = balance + 8 # x2 = 0 + 8 = 8t2: balance = x2 # balance = 8t1: balance = x1 # balance = 5t1: x1 = balance - 5 # x1 = 5 - 5 = 0t1: balance = x1 # balance = 0t2: x2 = balance - 8 # x2 = 0 - 8 = -8t2: balance = x2 # balance = -8结果 balance = -8 究其原因，是因为修改balance需要多条语句，而执行这几条语句时，线程可能中断，从而导致多个线程把同一个对象的内容改乱了。 如果我们要确保balance计算正确，就要给change_it()上一把锁，当某个线程开始执行change_it()时，我们说，该线程因为获得了锁，因此其他线程不能同时执行change_it()，只能等待，直到锁被释放后，获得该锁以后才能改。由于锁只有一个，无论多少线程，同一时刻最多只有一个线程持有该锁，所以，不会造成修改的冲突。创建一个锁就是通过threading.Lock()来实现： 12345678910111213balance = 0lock = threading.Lock()def run_thread(n): for i in range(100000): # 先要获取锁: lock.acquire() try: # 放心地改吧: change_it(n) finally: # 改完了一定要释放锁: lock.release() 当多个线程同时执行lock.acquire()时，只有一个线程能成功地获取锁，然后继续执行代码，其他线程就继续等待直到获得锁为止。 获得锁的线程用完后一定要释放锁，否则那些苦苦等待锁的线程将永远等待下去，成为死线程。所以我们用try...finally来确保锁一定会被释放。 锁的好处就是确保了某段关键代码只能由一个线程从头到尾完整地执行，坏处当然也很多，首先是阻止了多线程并发执行，包含锁的某段代码实际上只能以单线程模式执行，效率就大大地下降了。其次，由于可以存在多个锁，不同的线程持有不同的锁，并试图获取对方持有的锁时，可能会造成死锁，导致多个线程全部挂起，既不能执行，也无法结束，只能靠操作系统强制终止。 多核CPU 如果你不幸拥有一个多核CPU，你肯定在想，多核应该可以同时执行多个线程。如果写一个死循环的话，会出现什么情况呢？ 打开Mac OS X的Activity Monitor，或者Windows的Task Manager，都可以监控某个进程的CPU使用率。 我们可以监控到一个死循环线程会100%占用一个CPU。 如果有两个死循环线程，在多核CPU中，可以监控到会占用200%的CPU，也就是占用两个CPU核心。 要想把N核CPU的核心全部跑满，就必须启动N个死循环线程。 试试用Python写个死循环： 12345678910import threading, multiprocessingdef loop(): x = 0 while True: x = x ^ 1for i in range(multiprocessing.cpu_count()): t = threading.Thread(target=loop) t.start() 启动与CPU核心数量相同的N个线程，在4核CPU上可以监控到CPU占用率仅有102%，也就是仅使用了一核。 但是用C、C++或Java来改写相同的死循环，直接可以把全部核心跑满，4核就跑到400%，8核就跑到800%，为什么Python不行呢？ 因为Python的线程虽然是真正的线程，但解释器执行代码时，有一个GIL锁：Global Interpreter Lock，任何Python线程执行前，必须先获得GIL锁，然后，每执行100条字节码，解释器就自动释放GIL锁，让别的线程有机会执行。这个GIL全局锁实际上把所有线程的执行代码都给上了锁，所以，多线程在Python中只能交替执行，即使100个线程跑在100核CPU上，也只能用到1个核。 GIL是Python解释器设计的历史遗留问题，通常我们用的解释器是官方实现的CPython，要真正利用多核，除非重写一个不带GIL的解释器。 所以，在Python中，可以使用多线程，但不要指望能有效利用多核。如果一定要通过多线程利用多核，那只能通过C扩展来实现，不过这样就失去了Python简单易用的特点。 不过，也不用过于担心，Python虽然不能利用多线程实现多核任务，但可以通过多进程实现多核任务。多个Python进程有各自独立的GIL锁，互不影响。 ThreadLocal在多线程环境下，每个线程都有自己的数据。一个线程使用自己的局部变量比使用全局变量好，因为局部变量只有线程自己能看见，不会影响其他线程，而全局变量的修改必须加锁。 但是局部变量也有问题，就是在函数调用的时候，传递起来很麻烦： 12345678910111213def process_student(name): std = Student(name) # std是局部变量，但是每个函数都要用它，因此必须传进去： do_task_1(std) do_task_2(std)def do_task_1(std): do_subtask_1(std) do_subtask_2(std)def do_task_2(std): do_subtask_2(std) do_subtask_2(std) 每个函数一层一层调用都这么传参数那还得了？用全局变量？也不行，因为每个线程处理不同的Student对象，不能共享。 如果用一个全局dict存放所有的Student对象，然后以thread自身作为key获得线程对应的Student对象如何？ 123456789101112131415161718global_dict = &#123;&#125;def std_thread(name): std = Student(name) # 把std放到全局变量global_dict中： global_dict[threading.current_thread()] = std do_task_1() do_task_2()def do_task_1(): # 不传入std，而是根据当前线程查找： std = global_dict[threading.current_thread()] ...def do_task_2(): # 任何函数都可以查找出当前线程的std变量： std = global_dict[threading.current_thread()] ... 这种方式理论上是可行的，它最大的优点是消除了std对象在每层函数中的传递问题，但是，每个函数获取std的代码有点丑。 有没有更简单的方式？ ThreadLocal应运而生，不用查找dict，ThreadLocal帮你自动做这件事： 123456789101112131415161718192021import threading# 创建全局ThreadLocal对象:local_school = threading.local()def process_student(): # 获取当前线程关联的student: std = local_school.student print('Hello, %s (in %s)' % (std, threading.current_thread().name))def process_thread(name): # 绑定ThreadLocal的student: local_school.student = name process_student()t1 = threading.Thread(target= process_thread, args=('Alice',), name='Thread-A')t2 = threading.Thread(target= process_thread, args=('Bob',), name='Thread-B')t1.start()t2.start()t1.join()t2.join() 执行结果： 12Hello, Alice (in Thread-A)Hello, Bob (in Thread-B) 全局变量local_school就是一个ThreadLocal对象，每个Thread对它都可以读写student属性，但互不影响。你可以把local_school看成全局变量，但每个属性如local_school.student都是线程的局部变量，可以任意读写而互不干扰，也不用管理锁的问题，ThreadLocal内部会处理。可以理解为全局变量local_school是一个dict，不但可以用local_school.student，还可以绑定其他变量，如local_school.teacher等等。 ThreadLocal最常用的地方就是为每个线程绑定一个数据库连接，HTTP请求，用户身份信息等，这样一个线程的所有调用到的处理函数都可以非常方便地访问这些资源。 一个ThreadLocal变量虽然是全局变量，但每个线程都只能读写自己线程的独立副本，互不干扰。ThreadLocal解决了参数在一个线程中各个函数之间互相传递的问题。 进程 vs 线程我们介绍了多进程和多线程，这是实现多任务最常用的两种方式。现在，我们来讨论一下这两种方式的优缺点。 首先，要实现多任务，通常我们会设计Master-Worker模式，Master负责分配任务，Worker负责执行任务，因此，多任务环境下，通常是一个Master，多个Worker。 如果用多进程实现Master-Worker，主进程就是Master，其他进程就是Worker。如果用多线程实现Master-Worker，主线程就是Master，其他线程就是Worker。 多进程模式最大的优点就是稳定性高，因为一个子进程崩溃了，不会影响主进程和其他子进程。（当然主进程挂了所有进程就全挂了，但是Master进程只负责分配任务，挂掉的概率低）著名的Apache最早就是采用多进程模式。 多进程模式的缺点是创建进程的代价大，在Unix/Linux系统下，用fork调用还行，在Windows下创建进程开销巨大。另外，操作系统能同时运行的进程数也是有限的，在内存和CPU的限制下，如果有几千个进程同时运行，操作系统连调度都会成问题。 多线程模式通常比多进程快一点，但是也快不到哪去，而且，多线程模式致命的缺点就是任何一个线程挂掉都可能直接造成整个进程崩溃，因为所有线程共享进程的内存。在Windows上，如果一个线程执行的代码出了问题，你经常可以看到这样的提示：“该程序执行了非法操作，即将关闭”，其实往往是某个线程出了问题，但是操作系统会强制结束整个进程。 在Windows下，多线程的效率比多进程要高，所以微软的IIS服务器默认采用多线程模式。由于多线程存在稳定性的问题，IIS的稳定性就不如Apache。为了缓解这个问题，IIS和Apache现在又有多进程+多线程的混合模式，真是把问题越搞越复杂。 线程切换 无论是多进程还是多线程，只要数量一多，效率肯定上不去，为什么呢？ 我们打个比方，假设你不幸正在准备中考，每天晚上需要做语文、数学、英语、物理、化学这5科的作业，每项作业耗时1小时。 如果你先花1小时做语文作业，做完了，再花1小时做数学作业，这样，依次全部做完，一共花5小时，这种方式称为单任务模型，或者批处理任务模型。 假设你打算切换到多任务模型，可以先做1分钟语文，再切换到数学作业，做1分钟，再切换到英语，以此类推，只要切换速度足够快，这种方式就和单核CPU执行多任务是一样的了，以幼儿园小朋友的眼光来看，你就正在同时写5科作业。 但是，切换作业是有代价的，比如从语文切到数学，要先收拾桌子上的语文书本、钢笔（这叫保存现场），然后，打开数学课本、找出圆规直尺（这叫准备新环境），才能开始做数学作业。操作系统在切换进程或者线程时也是一样的，它需要先保存当前执行的现场环境（CPU寄存器状态、内存页等），然后，把新任务的执行环境准备好（恢复上次的寄存器状态，切换内存页等），才能开始执行。这个切换过程虽然很快，但是也需要耗费时间。如果有几千个任务同时进行，操作系统可能就主要忙着切换任务，根本没有多少时间去执行任务了，这种情况最常见的就是硬盘狂响，点窗口无反应，系统处于假死状态。 所以，多任务一旦多到一个限度，就会消耗掉系统所有的资源，结果效率急剧下降，所有任务都做不好。 计算密集型 vs IO密集型 是否采用多任务的第二个考虑是任务的类型。我们可以把任务分为计算密集型和IO密集型。 计算密集型任务的特点是要进行大量的计算，消耗CPU资源，比如计算圆周率、对视频进行高清解码等等，全靠CPU的运算能力。这种计算密集型任务虽然也可以用多任务完成，但是任务越多，花在任务切换的时间就越多，CPU执行任务的效率就越低，所以，要最高效地利用CPU，计算密集型任务同时进行的数量应当等于CPU的核心数。 计算密集型任务由于主要消耗CPU资源，因此，代码运行效率至关重要。Python这样的脚本语言运行效率很低，完全不适合计算密集型任务。对于计算密集型任务，最好用C语言编写。 第二种任务的类型是IO密集型，涉及到网络、磁盘IO的任务都是IO密集型任务，这类任务的特点是CPU消耗很少，任务的大部分时间都在等待IO操作完成（因为IO的速度远远低于CPU和内存的速度）。对于IO密集型任务，任务越多，CPU效率越高，但也有一个限度。常见的大部分任务都是IO密集型任务，比如Web应用。 IO密集型任务执行期间，99%的时间都花在IO上，花在CPU上的时间很少，因此，用运行速度极快的C语言替换用Python这样运行速度极低的脚本语言，完全无法提升运行效率。对于IO密集型任务，最合适的语言就是开发效率最高（代码量最少）的语言，脚本语言是首选，C语言最差。 异步IO 考虑到CPU和IO之间巨大的速度差异，一个任务在执行的过程中大部分时间都在等待IO操作，单进程单线程模型会导致别的任务无法并行执行，因此，我们才需要多进程模型或者多线程模型来支持多任务并发执行。 现代操作系统对IO操作已经做了巨大的改进，最大的特点就是支持异步IO。如果充分利用操作系统提供的异步IO支持，就可以用单进程单线程模型来执行多任务，这种全新的模型称为事件 驱动模型，Nginx就是支持异步IO的Web服务器，它在单核CPU上采用单进程模型就可以高效地支持多任务。在多核CPU上，可以运行多个进程（数量与CPU核心数相同），充分利用多核CPU。由于系统总的进程数量十分有限，因此操作系统调度非常高效。用异步IO编程模型来实现多任务是一个主要的趋势。 对应到Python语言，单线程的异步编程模型称为 协程，有了协程的支持，就可以基于事件驱动编写高效的多任务程序。我们会在后面讨论如何编写协程。 分布式进程在Thread和Process中，应当优选Process，因为Process更稳定，而且，Process可以分布到多台机器上，而Thread最多只能分布到同一台机器的多个CPU上。 Python的multiprocessing模块不但支持多进程，其中managers子模块还支持把多进程分布到多台机器上。一个服务进程可以作为调度者，将任务分布到其他多个进程中，依靠网络通信。由于managers模块封装很好，不必了解网络通信的细节，就可以很容易地编写分布式多进程程序。 举个例子：如果我们已经有一个通过Queue通信的多进程程序在同一台机器上运行，现在，由于处理任务的进程任务繁重，希望把发送任务的进程和处理任务的进程分布到两台机器上。怎么用分布式进程实现？ 原有的Queue可以继续使用，但是，通过managers模块把Queue通过网络暴露出去，就可以让其他机器的进程访问Queue了。 我们先看服务进程，服务进程负责启动Queue，把Queue注册到网络上，然后往Queue里面写入任务： 12345678910111213141516171819202122232425262728293031323334353637# task_master.pyimport random, time, queuefrom multiprocessing.managers import BaseManager# 发送任务的队列:task_queue = queue.Queue()# 接收结果的队列:result_queue = queue.Queue()# 从BaseManager继承的QueueManager:class QueueManager(BaseManager): pass# 把两个Queue都注册到网络上, callable参数关联了Queue对象:QueueManager.register('get_task_queue', callable=lambda: task_queue)QueueManager.register('get_result_queue', callable=lambda: result_queue)# 绑定端口5000, 设置验证码'abc':manager = QueueManager(address=('', 5000), authkey=b'abc')# 启动Queue:manager.start()# 获得通过网络访问的Queue对象:task = manager.get_task_queue()result = manager.get_result_queue()# 放几个任务进去:for i in range(10): n = random.randint(0, 10000) print('Put task %d...' % n) task.put(n)# 从result队列读取结果:print('Try get results...')for i in range(10): r = result.get(timeout=10) print('Result: %s' % r)# 关闭:manager.shutdown()print('master exit.') 请注意，当我们在一台机器上写多进程程序时，创建的Queue可以直接拿来用，但是，在分布式多进程环境下，添加任务到Queue不可以直接对原始的task_queue进行操作，那样就绕过了QueueManager的封装，必须通过manager.get_task_queue()获得的Queue接口添加。 然后，在另一台机器上启动任务进程（本机上启动也可以）： 1234567891011121314151617181920212223242526272829303132333435# task_worker.pyimport time, sys, queuefrom multiprocessing.managers import BaseManager# 创建类似的QueueManager:class QueueManager(BaseManager): pass# 由于这个QueueManager只从网络上获取Queue，所以注册时只提供名字:QueueManager.register('get_task_queue')QueueManager.register('get_result_queue')# 连接到服务器，也就是运行task_master.py的机器:server_addr = '127.0.0.1'print('Connect to server %s...' % server_addr)# 端口和验证码注意保持与task_master.py设置的完全一致:m = QueueManager(address=(server_addr, 5000), authkey=b'abc')# 从网络连接:m.connect()# 获取Queue的对象:task = m.get_task_queue()result = m.get_result_queue()# 从task队列取任务,并把结果写入result队列:for i in range(10): try: n = task.get(timeout=1) print('run task %d * %d...' % (n, n)) r = '%d * %d = %d' % (n, n, n*n) time.sleep(1) result.put(r) except Queue.Empty: print('task queue is empty.')# 处理结束:print('worker exit.') 任务进程要通过网络连接到服务进程，所以要指定服务进程的IP。现在，可以试试分布式进程的工作效果了。先启动task_master.py服务进程： 123456789101112$ python3 task_master.pyPut task 3411...Put task 1605...Put task 1398...Put task 4729...Put task 5300...Put task 7471...Put task 68...Put task 4219...Put task 339...Put task 7866...Try get results... task_master.py进程发送完任务后，开始等待result队列的结果。现在启动task_worker.py进程： 12345678910111213$ python3 task_worker.pyConnect to server 127.0.0.1...run task 3411 * 3411...run task 1605 * 1605...run task 1398 * 1398...run task 4729 * 4729...run task 5300 * 5300...run task 7471 * 7471...run task 68 * 68...run task 4219 * 4219...run task 339 * 339...run task 7866 * 7866...worker exit. task_worker.py进程结束，在task_master.py进程中会继续打印出结果： 12345678910Result: 3411 * 3411 = 11634921Result: 1605 * 1605 = 2576025Result: 1398 * 1398 = 1954404Result: 4729 * 4729 = 22363441Result: 5300 * 5300 = 28090000Result: 7471 * 7471 = 55815841Result: 68 * 68 = 4624Result: 4219 * 4219 = 17799961Result: 339 * 339 = 114921Result: 7866 * 7866 = 61873956 这个简单的Master/Worker模型有什么用？其实这就是一个简单但真正的分布式计算，把代码稍加改造，启动多个worker，就可以把任务分布到几台甚至几十台机器上，比如把计算n*n的代码换成发送邮件，就实现了邮件队列的异步发送。 Queue对象存储在哪？注意到task_worker.py中根本没有创建Queue的代码，所以，Queue对象存储在task_master.py进程中： 123456789101112131415161718┌─────────────────────────────────────────┐ ┌──────────────────────────────────────┐│task_master.py │ │ │task_worker.py ││ │ │ ││ task = manager.get_task_queue() │ │ │ task = manager.get_task_queue() ││ result = manager.get_result_queue() │ │ result = manager.get_result_queue() ││ │ │ │ │ │ ││ │ │ │ │ ││ ▼ │ │ │ │ ││ ┌─────────────────────────────────┐ │ │ │ ││ │QueueManager │ │ │ │ │ ││ │ ┌────────────┐ ┌──────────────┐ │ │ │ │ ││ │ │ task_queue │ │ result_queue │ │&lt;───┼──┼──┼──────────────┘ ││ │ └────────────┘ └──────────────┘ │ │ │ ││ └─────────────────────────────────┘ │ │ │ │└─────────────────────────────────────────┘ └──────────────────────────────────────┘ │ Network 而Queue之所以能通过网络访问，就是通过QueueManager实现的。由于QueueManager管理的不止一个Queue，所以，要给每个Queue的网络调用接口起个名字，比如get_task_queue。 authkey有什么用？这是为了保证两台机器正常通信，不被其他机器恶意干扰。如果task_worker.py的authkey和task_master.py的authkey不一致，肯定连接不上。 注意Queue的作用是用来传递任务和接收结果，每个任务的描述数据量要尽量小。比如发送一个处理日志文件的任务，就不要发送几百兆的日志文件本身，而是发送日志文件存放的完整路径，由Worker进程再去共享的磁盘上读取文件。 正则表达式正则表达式是一种用来匹配字符串的强有力的武器。它的设计思想是用一种描述性的语言来给字符串定义一个规则，凡是符合规则的字符串，我们就认为它“匹配”了，否则，该字符串就是不合法的。 要匹配变长的字符，在正则表达式中，用*表示任意个字符（包括0个），用+表示至少一个字符，用?表示0个或1个字符，用{n}表示n个字符，用{n,m}表示n-m个字符,来看一个复杂的例子： \\d{3}\\s+\\d{3,8} 。 从左到右解读一下： \\d{3}表示匹配3个数字，例如’010’； \\s可以匹配一个空格（也包括Tab等空白符），所以\\s+表示至少有一个空格，例如匹配&#39; &#39;等； \\d{3,8}表示3-8个数字，例如1234567。 综合起来，上面的正则表达式可以匹配以任意个空格隔开的带区号的电话号码。 如果要匹配’010-12345’这样的号码呢？由于’-‘是特殊字符，在正则表达式中，要用’\\’转义，所以，上面的正则是\\d{3}\\-\\d{3,8},但是，仍然无法匹配’010 - 12345’，因为带有空格。所以我们需要更复杂的匹配方式。 进阶 要做更精确地匹配，可以用[]表示范围，比如： [0-9a-zA-Z\\_]可以匹配一个数字、字母或者下划线； [0-9a-zA-Z\\_]+可以匹配至少由一个数字、字母或者下划线组成的字符串，比如’a100’，’0_Z’，’Py3000’等等； [a-zA-Z\\_][0-9a-zA-Z\\_]*可以匹配由字母或下划线开头，后接任意个由一个数字、字母或者下划线组成的字符串，也就是Python合法的变量； [a-zA-Z\\_][0-9a-zA-Z\\_]{0, 19}更精确地限制了变量的长度是1-20个字符（前面1个字符+后面最多19个字符）。 A|B可以匹配A或B，所以(P|p)ython可以匹配’Python’或者’python’。 ^表示行的开头，^\\d表示必须以数字开头。 $表示行的结束，\\d$表示必须以数字结束。 re模块 有了准备知识，我们就可以在Python中使用正则表达式了。Python提供re模块，包含所有正则表达式的功能。由于Python的字符串本身也用\\转义，所以要特别注意： 123s = 'ABC\\\\-001' # Python的字符串# 对应的正则表达式字符串变成：# 'ABC\\-001' 因此我们强烈建议使用Python的r前缀，就不用考虑转义的问题了： 123s = r'ABC\\-001' # Python的字符串# 对应的正则表达式字符串不变：# 'ABC\\-001' 先看看如何判断正则表达式是否匹配： 12345&gt;&gt;&gt; import re&gt;&gt;&gt; re.match(r'^\\d&#123;3&#125;\\-\\d&#123;3,8&#125;$', '010-12345')&lt;_sre.SRE_Match object; span=(0, 9), match='010-12345'&gt;&gt;&gt;&gt; re.match(r'^\\d&#123;3&#125;\\-\\d&#123;3,8&#125;$', '010 12345')&gt;&gt;&gt; match()方法判断是否匹配，如果匹配成功，返回一个Match对象，否则返回None。常见的判断方法就是： 12345test = '用户输入的字符串'if re.match(r'正则表达式', test): print('ok')else: print('failed') 切分字符串 用正则表达式切分字符串比用固定的字符更灵活，请看正常的切分代码： 12&gt;&gt;&gt; 'a b c'.split(' ')['a', 'b', '', '', 'c'] 嗯，无法识别连续的空格，用正则表达式试试： 12&gt;&gt;&gt; re.split(r'\\s+', 'a b c')['a', 'b', 'c'] 无论多少个空格都可以正常分割。加入,试试： 12&gt;&gt;&gt; re.split(r'[\\s\\,]+', 'a,b, c d')['a', 'b', 'c', 'd'] 再加入;试试： 12&gt;&gt;&gt; re.split(r'[\\s\\,\\;]+', 'a,b;; c d')['a', 'b', 'c', 'd'] 如果用户输入了一组标签，下次记得用正则表达式来把不规范的输入转化成正确的数组。 分组 除了简单地判断是否匹配之外，正则表达式还有提取子串的强大功能。用()表示的就是要提取的分组（Group）。比如： ^(\\d{3})-(\\d{3,8})$分别定义了两个组，可以直接从匹配的字符串中提取出区号和本地号码： 123456789&gt;&gt;&gt; m = re.match(r'^(\\d&#123;3&#125;)-(\\d&#123;3,8&#125;)$', '010-12345')&gt;&gt;&gt; m&lt;_sre.SRE_Match object; span=(0, 9), match='010-12345'&gt;&gt;&gt;&gt; m.group(0)'010-12345'&gt;&gt;&gt; m.group(1)'010'&gt;&gt;&gt; m.group(2)'12345' 如果正则表达式中定义了组，就可以在Match对象上用group()方法提取出子串来。注意到group(0)永远是原始字符串，group(1)、group(2)……表示第1、2、……个子串。 提取子串非常有用。来看一个更凶残的例子： 1234&gt;&gt;&gt; t = '19:05:30'&gt;&gt;&gt; m = re.match(r'^(0[0-9]|1[0-9]|2[0-3]|[0-9])\\:(0[0-9]|1[0-9]|2[0-9]|3[0-9]|4[0-9]|5[0-9]|[0-9])\\:(0[0-9]|1[0-9]|2[0-9]|3[0-9]|4[0-9]|5[0-9]|[0-9])$', t)&gt;&gt;&gt; m.groups()('19', '05', '30') 这个正则表达式可以直接识别合法的时间。但是有些时候，用正则表达式也无法做到完全验证，比如识别日期： 1'^(0[1-9]|1[0-2]|[0-9])-(0[1-9]|1[0-9]|2[0-9]|3[0-1]|[0-9])$' 对于&#39;2-30&#39;，&#39;4-31&#39;这样的非法日期，用正则还是识别不了，或者说写出来非常困难，这时就需要程序配合识别了。 贪婪匹配 最后需要特别指出的是，正则匹配默认是贪婪匹配，也就是匹配尽可能多的字符。举例如下，匹配出数字后面的0： 12&gt;&gt;&gt; re.match(r'^(\\d+)(0*)$', '102300').groups()('102300', '') 由于\\d+采用贪婪匹配，直接把后面的0全部匹配了，结果0*只能匹配空字符串了。 必须让\\d+采用非贪婪匹配（也就是尽可能少匹配），才能把后面的0匹配出来，加个?就可以让\\d+采用非贪婪匹配： 12&gt;&gt;&gt; re.match(r'^(\\d+?)(0*)$', '102300').groups()('1023', '00') 编译 当我们在Python中使用正则表达式时，re模块内部会干两件事情： 编译正则表达式，如果正则表达式的字符串本身不合法，会报错； 用编译后的正则表达式去匹配字符串。 如果一个正则表达式要重复使用几千次，出于效率的考虑，我们可以预编译该正则表达式，接下来重复使用时就不需要编译这个步骤了，直接匹配： 12345678&gt;&gt;&gt; import re# 编译:&gt;&gt;&gt; re_telephone = re.compile(r'^(\\d&#123;3&#125;)-(\\d&#123;3,8&#125;)$')# 使用：&gt;&gt;&gt; re_telephone.match('010-12345').groups()('010', '12345')&gt;&gt;&gt; re_telephone.match('010-8086').groups()('010', '8086') 编译后生成Regular Expression对象，由于该对象自己包含了正则表达式，所以调用对应的方法时不用给出正则字符串。 常用内建模块Python之所以自称batteries included，就是因为内置了许多非常有用的模块，无需额外安装和配置，即可直接使用。 datetimedatetime是Python处理日期和时间的标准库。 获取当前日期和时间 我们先看如何获取当前日期和时间： 123456&gt;&gt;&gt; from datetime import datetime&gt;&gt;&gt; now = datetime.now() # 获取当前datetime&gt;&gt;&gt; print(now)2020-05-18 16:28:07.198690&gt;&gt;&gt; print(type(now))&lt;class 'datetime.datetime'&gt; 注意到datetime是模块，datetime模块还包含一个datetime类，通过from datetime import datetime导入的才是datetime这个类。 如果仅导入import datetime，则必须引用全名datetime.datetime。 datetime.now()返回当前日期和时间，其类型是datetime。 获取指定日期和时间 要指定某个日期和时间，我们直接用参数构造一个datetime： 1234&gt;&gt;&gt; from datetime import datetime&gt;&gt;&gt; dt = datetime(2015, 4, 19, 12, 20) # 用指定日期时间创建datetime&gt;&gt;&gt; print(dt)2015-04-19 12:20:00 datetime转换为timestamp 在计算机中，时间实际上是用数字表示的。我们把1970年1月1日 00:00:00 UTC+00:00 时区的时刻称为epoch time，记为0（1970年以前的时间timestamp为负数），当前时间就是相对于epoch time的秒数，称为timestamp。 你可以认为： 1timestamp = 0 = 1970-1-1 00:00:00 UTC+0:00 对应的北京时间是： 1timestamp = 0 = 1970-1-1 08:00:00 UTC+8:00 可见timestamp的值与时区毫无关系，因为timestamp一旦确定，其UTC时间就确定了，转换到任意时区的时间也是完全确定的，这就是为什么计算机存储的当前时间是以timestamp表示的，因为全球各地的计算机在任意时刻的timestamp都是完全相同的（假定时间已校准）。 把一个datetime类型转换为timestamp只需要简单调用timestamp()方法： 1234&gt;&gt;&gt; from datetime import datetime&gt;&gt;&gt; dt = datetime(2015, 4, 19, 12, 20) # 用指定日期时间创建datetime&gt;&gt;&gt; dt.timestamp() # 把datetime转换为timestamp1429417200.0 注意Python的timestamp是一个浮点数。如果有小数位，小数位表示毫秒数。 某些编程语言（如Java和JavaScript）的timestamp使用整数表示毫秒数，这种情况下只需要把timestamp除以1000就得到Python的浮点表示方法。 timestamp转换为datetime 要把timestamp转换为datetime，使用datetime提供的fromtimestamp()方法： 1234&gt;&gt;&gt; from datetime import datetime&gt;&gt;&gt; t = 1429417200.0&gt;&gt;&gt; print(datetime.fromtimestamp(t))2015-04-19 12:20:00 注意到timestamp是一个浮点数，它没有时区的概念，而datetime是有时区的。上述转换是在timestamp和本地时间做转换。 timestamp也可以直接被转换到UTC标准时区的时间： 123456&gt;&gt;&gt; from datetime import datetime&gt;&gt;&gt; t = 1429417200.0&gt;&gt;&gt; print(datetime.fromtimestamp(t)) # 本地时间2015-04-19 12:20:00&gt;&gt;&gt; print(datetime.utcfromtimestamp(t)) # UTC时间2015-04-19 04:20:00 str转换为datetime 很多时候，用户输入的日期和时间是字符串，要处理日期和时间，首先必须把str转换为datetime。转换方法是通过datetime.strptime()实现，需要一个日期和时间的格式化字符串： 1234&gt;&gt;&gt; from datetime import datetime&gt;&gt;&gt; cday = datetime.strptime('2015-6-1 18:19:59', '%Y-%m-%d %H:%M:%S')&gt;&gt;&gt; print(cday)2015-06-01 18:19:59 字符串&#39;%Y-%m-%d %H:%M:%S&#39;规定了日期和时间部分的格式。详细的说明请参考Python文档。注意转换后的datetime是没有时区信息的。 datetime转换为str 如果已经有了datetime对象，要把它格式化为字符串显示给用户，就需要转换为str，转换方法是通过strftime()实现的，同样需要一个日期和时间的格式化字符串： 1234&gt;&gt;&gt; from datetime import datetime&gt;&gt;&gt; now = datetime.now()&gt;&gt;&gt; print(now.strftime('%a, %b %d %H:%M'))Mon, May 05 16:28 datetime加减 对日期和时间进行加减实际上就是把datetime往后或往前计算，得到新的datetime。加减可以直接用+和-运算符，不过需要导入timedelta这个类： 12345678910&gt;&gt;&gt; from datetime import datetime, timedelta&gt;&gt;&gt; now = datetime.now()&gt;&gt;&gt; nowdatetime.datetime(2015, 5, 18, 16, 57, 3, 540997)&gt;&gt;&gt; now + timedelta(hours=10)datetime.datetime(2015, 5, 19, 2, 57, 3, 540997)&gt;&gt;&gt; now - timedelta(days=1)datetime.datetime(2015, 5, 17, 16, 57, 3, 540997)&gt;&gt;&gt; now + timedelta(days=2, hours=12)datetime.datetime(2015, 5, 21, 4, 57, 3, 540997) 可见，使用timedelta你可以很容易地算出前几天和后几天的时刻。 本地时间转换为UTC时间 本地时间是指系统设定时区的时间，例如北京时间是UTC+8:00时区的时间，而UTC时间指UTC+0:00时区的时间。 一个datetime类型有一个时区属性tzinfo，但是默认为None，所以无法区分这个datetime到底是哪个时区，除非强行给datetime设置一个时区： 12345678&gt;&gt;&gt; from datetime import datetime, timedelta, timezone&gt;&gt;&gt; tz_utc_8 = timezone(timedelta(hours=8)) # 创建时区UTC+8:00&gt;&gt;&gt; now = datetime.now()&gt;&gt;&gt; nowdatetime.datetime(2015, 5, 18, 17, 2, 10, 871012)&gt;&gt;&gt; dt = now.replace(tzinfo=tz_utc_8) # 强制设置为UTC+8:00&gt;&gt;&gt; dtdatetime.datetime(2015, 5, 18, 17, 2, 10, 871012, tzinfo=datetime.timezone(datetime.timedelta(0, 28800))) 如果系统时区恰好是UTC+8:00，那么上述代码就是正确的，否则，不能强制设置为UTC+8:00时区。 时区转换 我们可以先通过utcnow()拿到当前的UTC时间，再转换为任意时区的时间： 12345678910111213141516# 拿到UTC时间，并强制设置时区为UTC+0:00:&gt;&gt;&gt; utc_dt = datetime.utcnow().replace(tzinfo=timezone.utc)&gt;&gt;&gt; print(utc_dt)2015-05-18 09:05:12.377316+00:00# astimezone()将转换时区为北京时间:&gt;&gt;&gt; bj_dt = utc_dt.astimezone(timezone(timedelta(hours=8)))&gt;&gt;&gt; print(bj_dt)2015-05-18 17:05:12.377316+08:00# astimezone()将转换时区为东京时间:&gt;&gt;&gt; tokyo_dt = utc_dt.astimezone(timezone(timedelta(hours=9)))&gt;&gt;&gt; print(tokyo_dt)2015-05-18 18:05:12.377316+09:00# astimezone()将bj_dt转换时区为东京时间:&gt;&gt;&gt; tokyo_dt2 = bj_dt.astimezone(timezone(timedelta(hours=9)))&gt;&gt;&gt; print(tokyo_dt2)2015-05-18 18:05:12.377316+09:00 时区转换的关键在于，拿到一个datetime时，要获知其正确的时区，然后强制设置时区，作为基准时间。 利用带时区的datetime，通过astimezone()方法，可以转换到任意时区。 注：不是必须从UTC+0:00时区转换到其他时区，任何带时区的datetime都可以正确转换，例如上述bj_dt到tokyo_dt的转换。 collectionscollections是Python内建的一个集合模块，提供了许多有用的集合类。 namedtuple 我们知道tuple可以表示不变集合，例如，一个点的二维坐标就可以表示成： 1&gt;&gt;&gt; p = (1, 2) 但是，看到(1, 2)，很难看出这个tuple是用来表示一个坐标的。 定义一个class又小题大做了，这时，namedtuple就派上了用场： 1234567&gt;&gt;&gt; from collections import namedtuple&gt;&gt;&gt; Point = namedtuple('Point', ['x', 'y'])&gt;&gt;&gt; p = Point(1, 2)&gt;&gt;&gt; p.x1&gt;&gt;&gt; p.y2 namedtuple是一个函数，它用来创建一个自定义的tuple对象，并且规定了tuple元素的个数，并可以用属性而不是索引来引用tuple的某个元素。这样一来，我们用namedtuple可以很方便地定义一种数据类型，它具备tuple的不变性，又可以根据属性来引用，使用十分方便。 可以验证创建的Point对象是tuple的一种子类： 1234&gt;&gt;&gt; isinstance(p, Point)True&gt;&gt;&gt; isinstance(p, tuple)True 类似的，如果要用坐标和半径表示一个圆，也可以用namedtuple定义： 12# namedtuple('名称', [属性list]):Circle = namedtuple('Circle', ['x', 'y', 'r']) deque 使用list存储数据时，按索引访问元素很快，但是插入和删除元素就很慢了，因为list是线性存储，数据量大的时候，插入和删除效率很低。deque是为了高效实现插入和删除操作的双向列表，适合用于队列和栈： 123456&gt;&gt;&gt; from collections import deque&gt;&gt;&gt; q = deque(['a', 'b', 'c'])&gt;&gt;&gt; q.append('x')&gt;&gt;&gt; q.appendleft('y')&gt;&gt;&gt; qdeque(['y', 'a', 'b', 'c', 'x']) deque除了实现list的append()和pop()外，还支持appendleft()和popleft()，这样就可以非常高效地往头部添加或删除元素。 defaultdict 使用dict时，如果引用的Key不存在，就会抛出KeyError。如果希望key不存在时，返回一个默认值，就可以用defaultdict： 1234567&gt;&gt;&gt; from collections import defaultdict&gt;&gt;&gt; dd = defaultdict(lambda: 'N/A')&gt;&gt;&gt; dd['key1'] = 'abc'&gt;&gt;&gt; dd['key1'] # key1存在'abc'&gt;&gt;&gt; dd['key2'] # key2不存在，返回默认值'N/A' 除了在Key不存在时返回默认值，defaultdict的其他行为跟dict是完全一样的。 OrderedDict 使用dict时，Key是无序的。在对dict做迭代时，我们无法确定Key的顺序。如果要保持Key的顺序，可以用OrderedDict： 1234567&gt;&gt;&gt; from collections import OrderedDict&gt;&gt;&gt; d = dict([('a', 1), ('b', 2), ('c', 3)])&gt;&gt;&gt; d # dict的Key是无序的&#123;'a': 1, 'c': 3, 'b': 2&#125;&gt;&gt;&gt; od = OrderedDict([('a', 1), ('b', 2), ('c', 3)])&gt;&gt;&gt; od # OrderedDict的Key是有序的OrderedDict([('a', 1), ('b', 2), ('c', 3)]) 注意，OrderedDict的Key会按照插入的顺序排列，不是Key本身排序： 123456&gt;&gt;&gt; od = OrderedDict()&gt;&gt;&gt; od['z'] = 1&gt;&gt;&gt; od['y'] = 2&gt;&gt;&gt; od['x'] = 3&gt;&gt;&gt; list(od.keys()) # 按照插入的Key的顺序返回['z', 'y', 'x'] OrderedDict可以实现一个FIFO（先进先出）的dict，当容量超出限制时，先删除最早添加的Key： 12345678910111213141516171819from collections import OrderedDictclass LastUpdatedOrderedDict(OrderedDict): def __init__(self, capacity): super(LastUpdatedOrderedDict, self).__init__() self._capacity = capacity def __setitem__(self, key, value): containsKey = 1 if key in self else 0 if len(self) - containsKey &gt;= self._capacity: last = self.popitem(last=False) print('remove:', last) if containsKey: del self[key] print('set:', (key, value)) else: print('add:', (key, value)) OrderedDict.__setitem__(self, key, value) Counter Counter是一个简单的计数器，例如，统计字符出现的个数： 1234567&gt;&gt;&gt; from collections import Counter&gt;&gt;&gt; c = Counter()&gt;&gt;&gt; for ch in 'programming':... c[ch] = c[ch] + 1...&gt;&gt;&gt; cCounter(&#123;'g': 2, 'm': 2, 'r': 2, 'a': 1, 'i': 1, 'o': 1, 'n': 1, 'p': 1&#125;) Counter实际上也是dict的一个子类，上面的结果可以看出，字符&#39;g&#39;、&#39;m&#39;、&#39;r&#39;各出现了两次，其他字符各出现了一次。 base64Base64是一种用64个字符来表示任意二进制数据的方法。 用记事本打开exe、jpg、pdf这些文件时，我们都会看到一大堆乱码，因为二进制文件包含很多无法显示和打印的字符，所以，如果要让记事本这样的文本处理软件能处理二进制数据，就需要一个二进制到字符串的转换方法。Base64是一种最常见的二进制编码方法。 Base64的原理很简单，首先，准备一个包含64个字符的数组： 1[&apos;A&apos;, &apos;B&apos;, &apos;C&apos;, ... &apos;a&apos;, &apos;b&apos;, &apos;c&apos;, ... &apos;0&apos;, &apos;1&apos;, ... &apos;+&apos;, &apos;/&apos;] 然后，对二进制数据进行处理，每3个字节一组，一共是3x8=24bit，划为4组，每组正好6个bit： 这样我们得到4个数字作为索引，然后查表，获得相应的4个字符，就是编码后的字符串。 所以，Base64编码会把3字节的二进制数据编码为4字节的文本数据，长度增加33%，好处是编码后的文本数据可以在邮件正文、网页等直接显示。 如果要编码的二进制数据不是3的倍数，最后会剩下1个或2个字节怎么办？Base64用\\x00字节在末尾补足后，再在编码的末尾加上1个或2个=号，表示补了多少字节，解码的时候，会自动去掉。 Python内置的base64可以直接进行base64的编解码： 12345&gt;&gt;&gt; import base64&gt;&gt;&gt; base64.b64encode(b'binary\\x00string')b'YmluYXJ5AHN0cmluZw=='&gt;&gt;&gt; base64.b64decode(b'YmluYXJ5AHN0cmluZw==')b'binary\\x00string' 由于标准的Base64编码后可能出现字符+和/，在URL中就不能直接作为参数，所以又有一种&quot;url safe&quot;的base64编码，其实就是把字符+和/分别变成-和_： 123456&gt;&gt;&gt; base64.b64encode(b'i\\xb7\\x1d\\xfb\\xef\\xff')b'abcd++//'&gt;&gt;&gt; base64.urlsafe_b64encode(b'i\\xb7\\x1d\\xfb\\xef\\xff')b'abcd--__'&gt;&gt;&gt; base64.urlsafe_b64decode('abcd--__')b'i\\xb7\\x1d\\xfb\\xef\\xff' 还可以自己定义64个字符的排列顺序，这样就可以自定义Base64编码，不过，通常情况下完全没有必要。Base64是一种通过查表的编码方法，不能用于加密，即使使用自定义的编码表也不行。 Base64适用于小段内容的编码，比如数字证书签名、Cookie的内容等。 由于=字符也可能出现在Base64编码中，但=用在URL、Cookie里面会造成歧义，所以，很多Base64编码后会把=去掉： 1234# 标准Base64:'abcd' -&gt; 'YWJjZA=='# 自动去掉=:'abcd' -&gt; 'YWJjZA' 去掉=后怎么解码呢？因为Base64是把3个字节变为4个字节，所以，Base64编码的长度永远是4的倍数，因此，需要加上=把Base64字符串的长度变为4的倍数，就可以正常解码了。 struct准确地讲，Python没有专门处理字节的数据类型。但由于b&#39;str&#39;可以表示字节，所以，字节数组 ＝ 二进制str。而在C语言中，我们可以很方便地用struct、union来处理字节，以及字节和int，float的转换。 在Python中，比方说要把一个32位无符号整数变成字节，也就是4个长度的bytes，你得配合位运算符这么写： 12345678&gt;&gt;&gt; n = 10240099&gt;&gt;&gt; b1 = (n &amp; 0xff000000) &gt;&gt; 24&gt;&gt;&gt; b2 = (n &amp; 0xff0000) &gt;&gt; 16&gt;&gt;&gt; b3 = (n &amp; 0xff00) &gt;&gt; 8&gt;&gt;&gt; b4 = n &amp; 0xff&gt;&gt;&gt; bs = bytes([b1, b2, b3, b4])&gt;&gt;&gt; bsb'\\x00\\x9c@c' 非常麻烦。如果换成浮点数就无能为力了。 好在Python提供了一个struct模块来解决bytes和其他二进制数据类型的转换。 struct的pack函数把任意数据类型变成bytes： 123&gt;&gt;&gt; import struct&gt;&gt;&gt; struct.pack('&gt;I', 10240099)b'\\x00\\x9c@c' pack的第一个参数是处理指令，&#39;&gt;I&#39;的意思是： &gt;表示字节顺序是big-endian，也就是网络序，I表示4字节无符号整数。 后面的参数个数要和处理指令一致。 unpack把bytes变成相应的数据类型： 12&gt;&gt;&gt; struct.unpack('&gt;IH', b'\\xf0\\xf0\\xf0\\xf0\\x80\\x80')(4042322160, 32896) 根据 &gt;IH 的说明，后面的bytes依次变为I：4字节无符号整数和 H：2 字节无符号整数。 所以，尽管Python不适合编写底层操作字节流的代码，但在对性能要求不高的地方，利用struct就方便多了。struct模块定义的数据类型可以参考Python官方文档 Windows的位图文件（.bmp）是一种非常简单的文件格式，我们来用struct分析一下。首先找一个bmp文件，没有的话用“画图”画一个。 读入前30个字节来分析： 1&gt;&gt;&gt; s = b'\\x42\\x4d\\x38\\x8c\\x0a\\x00\\x00\\x00\\x00\\x00\\x36\\x00\\x00\\x00\\x28\\x00\\x00\\x00\\x80\\x02\\x00\\x00\\x68\\x01\\x00\\x00\\x01\\x00\\x18\\x00' BMP格式采用小端方式存储数据，文件头的结构按顺序如下： 两个字节：&#39;BM&#39;表示Windows位图，&#39;BA&#39;表示OS/2位图；一个4字节整数：表示位图大小；一个4字节整数：保留位，始终为0；一个4字节整数：实际图像的偏移量；一个4字节整数：Header的字节数；一个4字节整数：图像宽度；一个4字节整数：图像高度；一个2字节整数：始终为1；一个2字节整数：颜色数。 所以，组合起来用unpack读取： 12&gt;&gt;&gt; struct.unpack('&lt;ccIIIIIIHH', s)(b'B', b'M', 691256, 0, 54, 40, 640, 360, 1, 24) 结果显示，b&#39;B&#39;、b&#39;M&#39;说明是Windows位图，位图大小为640x360，颜色数为24。 请编写一个bmpinfo.py，可以检查任意文件是否是位图文件，如果是，打印出图片大小和颜色数。 hashlib","tags":[{"name":"Python","slug":"Python","permalink":"http://wenbo.fun/tags/Python/"}]}]