[{"title":"最小二乘法","date":"2018-07-10T02:28:53.000Z","path":"2018/07/10/statistics-1/","text":"最小二乘法最小二乘法（又称最小平方法）通过最小化误差的平方和寻找数据的最佳函数匹配。用函数表示为： $$ \\min_{\\vec{b}}{\\sum _{{i=1}}^{{n}}(y_{m}-y_{i})^{2}}$$. 用欧几里得度量表达为： $$ \\min _{\\vec {b}}\\|{\\vec {y}}_{m}({\\vec {b}})-{\\vec {y}}\\|_{2}^{2} $$ 示例 wiki 某次实验得到了四个数据点 $(x, y)：(1,6)、 (2,5)、 (3,7)、(4,10)$。我们希望找出一条和这四个点最匹配的直线 $ y=\\beta _{1}+\\beta _{2}x $ ，即找出在某种“最佳情况”下能够大致符合如下超定线性方程组的$\\beta _{1}$ 和 $\\beta _{2}$：$$\\beta _{1}+1\\beta _{2}=6\\\\\\ \\beta _{1}+2\\beta _{2}=5\\\\\\\\beta _{1}+3\\beta _{2}=7\\\\\\ \\beta _{1}+4\\beta _{2}=10$$最小二乘法采用的手段是尽量使得等号两边的方差最小，也就是找出这个函数的最小值： $$S(\\beta _{1},\\beta _{2})=\\left[6-(\\beta _{1}+1\\beta _{2})\\right]^{2}+\\left[5-(\\beta _{1}+2\\beta _{2})\\right]^{2}+\\left[7-(\\beta _{1}+3\\beta _{2})\\right]^{2}+\\left[10-(\\beta _{1}+4\\beta _{2})\\right]^{2}$$ 最小值可以通过对 $S(\\beta _{1},\\beta _{2})$ 分别求 $\\beta _{1}$ 和 $\\beta _{2}$ 的偏导数，然后使它们等于零得到。 $${\\frac {\\partial S}{\\partial \\beta _{1}}}=0=8\\beta _{1}+20\\beta _{2}-56 \\\\\\{\\frac {\\partial S}{\\partial \\beta _{2}}}=0=20\\beta _{1}+60\\beta _{2}-154$$ 如此就得到了一个只有两个未知数的方程组，很容易就可以解出： $\\beta _{1}=3.5$$\\beta _{2}=1.4$ 也就是说直线 $y=3.5+1.4x$ 是最佳的。 线性函数模型一般线性情况 一般线线方程： $$y(t_{1},\\dots ,t_{q};b_{0},b_{1},\\dots ,b_{q})=b_{0}+b_{1}t_{1}+\\cdots +b_{q}t_{q} $$ 即线性方程组： $${\\begin{matrix}b_{0}+b_{1}t_{{11}}+\\cdots +b_{j}t_{{1j}}+\\cdots +b_{q}t_{{1q}}=y_{1}\\\\b_{0}+b_{1}t_{{21}}+\\cdots +b_{j}t_{{2j}}+\\cdots +b_{q}t_{{2q}}=y_{2}\\\\\\vdots \\\\b_{0}+b_{1}t_{{i1}}+\\cdots +b_{j}t_{{ij}}+\\cdots +b_{q}t_{{iq}}=y_{i}\\\\\\vdots \\\\b_{0}+b_{1}t_{{n1}}+\\cdots +b_{j}t_{{nj}}+\\cdots +b_{q}t_{{nq}}=y_{n}\\end{matrix}}$$ 通常人们将$t_{ij}$记作数据矩阵$A$，参数$b_j$记做参数向量$b$，观测值$y_i$记作$Y$，则线性方程组又可写成： $${\\displaystyle {\\begin{pmatrix}1&t_{11}&\\cdots &t_{1j}\\cdots &t_{1q}\\\\1&t_{21}&\\cdots &t_{2j}\\cdots &t_{2q}\\\\\\vdots \\\\1&t_{i1}&\\cdots &t_{ij}\\cdots &t_{iq}\\\\\\vdots \\\\1&t_{n1}&\\cdots &t_{nj}\\cdots &t_{nq}\\end{pmatrix}}\\cdot {\\begin{pmatrix}b_{0}\\\\b_{1}\\\\b_{2}\\\\\\vdots \\\\b_{j}\\\\\\vdots \\\\b_{q}\\end{pmatrix}}={\\begin{pmatrix}y_{1}\\\\y_{2}\\\\\\vdots \\\\y_{i}\\\\\\vdots \\\\y_{n}\\end{pmatrix}}} {\\begin{pmatrix}1&t_{{11}}&\\cdots &t_{{1j}}\\cdots &t_{{1q}}\\\\1&t_{{21}}&\\cdots &t_{{2j}}\\cdots &t_{{2q}}\\\\\\vdots \\\\1&t_{{i1}}&\\cdots &t_{{ij}}\\cdots &t_{{iq}}\\\\\\vdots \\\\1&t_{{n1}}&\\cdots &t_{{nj}}\\cdots &t_{{nq}}\\end{pmatrix}}\\cdot {\\begin{pmatrix}b_{0}\\\\b_{1}\\\\b_{2}\\\\\\vdots \\\\b_{j}\\\\\\vdots \\\\b_{q}\\end{pmatrix}}={\\begin{pmatrix}y_{1}\\\\y_{2}\\\\\\vdots \\\\y_{i}\\\\\\vdots \\\\y_{n}\\end{pmatrix}} 即 {\\displaystyle Ab=Y} Ab=Y$$ 上述方程运用最小二乘法导出为线性平方差计算的形式为： $$\\min _{b}|Ab-Y|_{2}$$ 最小二乘法的解 $$\\min _{b}\\left\\|{\\boldsymbol {Ab}}-{\\boldsymbol {Y}}\\right\\|_{2},{\\boldsymbol {A}}\\in {\\mathbf {C}}^{{n\\times m}},{\\boldsymbol {Y}}\\in {\\mathbf {C}}^{{n}}$$ 特解为$A$的广义逆矩阵(伪逆矩阵)与$Y$的乘积，这同时也是二范数极小的解.$$\\boldsymbol{b}=\\boldsymbol{A}^\\dagger \\boldsymbol{Y} $$其通解为特解加上$A$的零空间. $${\\boldsymbol {b}}={\\boldsymbol {A}}^{\\dagger }{\\boldsymbol {Y}}+\\left({\\boldsymbol {I}}-{\\boldsymbol {A}}^{\\dagger }{\\boldsymbol {A}}\\right){\\boldsymbol {h}}:{\\boldsymbol {h}}\\in {\\mathbf {C}}^{{n}}$$ 高斯－马尔可夫定理 wiki 高斯－马尔可夫定理(Gauss-Markov Theorem)陈述的是：在线性回归模型中，如果误差满足零均值、同方差且互不相关，则回归系数的最佳线性无偏估计(BLUE, Best Linear unbiased estimator)就是普通最小二乘法估计。 一元线性回归模型$$y_{i}=\\beta _{0}+\\beta _{1}x_{i}+\\varepsilon _{i};\\quad i=1,\\dots n$$ 其中 $\\beta_0$和 $\\beta _{1}$ 是非随机但不能观测到的参数， $x_{i}$是非随机且可观测到的一般变量， $\\varepsilon _{i}$是不可观测的随机变量，或称为随机误差或噪音，$y_{i}$是可观测的随机变量。 高斯－马尔可夫定理的假设条件是： ${\\rm {E}}\\left(\\varepsilon _{i}\\right)=0，\\forall i 零均值)$， ${\\rm {Var}}\\left(\\varepsilon _{i}\\right)=\\sigma ^{2}&lt;\\infty ，\\forall i(同方差)$， ${\\displaystyle {\\rm {Cov}}\\left(\\varepsilon _{i},\\varepsilon _{j}\\right)=0}， {\\displaystyle \\forall i\\not =j}(不相关)$。 则对 $\\beta_0$和 $\\beta _{1}$的最佳线性无偏估计为： $${\\displaystyle {\\hat {\\beta }}_{1}={\\frac {\\sum {x_{i}y_{i}}-{\\frac {1}{n}}\\sum {x_{i}}\\sum {y_{i}}}{\\sum {x_{i}^{2}}-{\\frac {1}{n}}(\\sum {x_{i}})^{2}}}={\\frac {\\operatorname {Cov} (x,y)}{\\sigma _{x}^{2}}},\\quad {\\hat {\\beta }}_{0}={\\overline {y}}-{\\hat {\\beta }}_{1}\\,{\\overline {x}}\\ .}$$ 多元线性回归模型对于多元线性回归模型， $$y_i=\\sum_{j=0}^p \\beta_j x_{ij}+\\varepsilon_i, x_{i0}=1; \\quad i = 1, \\dots n.$$ 使用矩阵形式，线性回归模型可简化记为${Y}={X}{\\beta}+{\\varepsilon}$，其中采用了以下记号： ${\\mathbf {Y}}=(y_{1},y_{2},\\dots ,y_{n})^{T}$ (观测值向量，Vector of Responses), ${\\mathbf {X}}=(x_{ij})={\\begin{bmatrix}1&amp;x_{11}&amp;x_{12}&amp;\\cdots &amp;x_{1p}\\\\1&amp;x_{21}&amp;x_{22}&amp;\\cdots &amp;x_{2p}\\\\\\vdots &amp;\\vdots &amp;\\vdots &amp;\\ddots &amp;\\vdots \\\\1&amp;x_{n1}&amp;x_{n2}&amp;\\cdots &amp;x_{np}\\end{bmatrix}}$(设计矩阵，Design Matrix), ${\\displaystyle {\\boldsymbol {\\beta }}=(\\beta _{0},\\beta _{1},\\dots ,\\beta _{p})^{T}}$ (参数向量，Vector of Parameters), ${\\displaystyle {\\boldsymbol {\\varepsilon }}=(\\varepsilon _{1},\\varepsilon _{2},\\dots ,\\varepsilon _{n})^{T}}$ (随机误差向量，Vectors of Error)。 高斯－马尔可夫定理的假设条件是： ${\\displaystyle {\\rm {E}}\\left({\\boldsymbol {\\varepsilon }}\\mid {\\mathbf {X}}\\right)=0} ， {\\displaystyle \\forall {\\mathbf {X}}}$（零均值）， ${\\displaystyle {\\rm {Var}}\\left({\\boldsymbol {\\varepsilon }}\\mid {\\mathbf {X}}\\right)={\\rm {E}}\\left({\\boldsymbol {\\varepsilon }}{\\boldsymbol {\\varepsilon }}^{T}\\mid {\\mathbf {X}}\\right)=\\sigma _{\\varepsilon }^{2}{\\mathbf {I_{n}}}}$，（同方差且不相关），其中 $ {\\displaystyle {\\mathbf {I_{n}}}}$为n阶单位矩阵(Identity Matrix)。 则对 ${\\beta}$ 的最佳线性无偏估计为 $${\\displaystyle {\\hat {\\boldsymbol {\\beta }}}=({\\mathbf {X}}^{T}{\\mathbf {X}})^{-1}{\\mathbf {X}}^{T}{\\mathbf {Y}}}$$","tags":[{"name":"统计学","slug":"统计学","permalink":"http://wenbo.fun/tags/统计学/"}]},{"title":"tracks","date":"2018-06-27T06:13:49.000Z","path":"2018/06/27/tracks/","text":"問題與技巧Anacondaanaconda 环境复制12source activate py36conda env export --file py36.yml 得到 *.yml。 1conda env create --file py36.yml 創建相似環境。 anaconda 中 jupyter notebook 切換成當前環境kernel 進入環境 1python -m ipykernel install --user --name 環境名稱 --display-name &quot;自定義kernel名稱&quot; juypter notebook 選擇切換。 Nvidia 顯卡驅動Ubuntu18.04深度学习GPU环境配置cudnn-install","tags":[{"name":"其他","slug":"其他","permalink":"http://wenbo.fun/tags/其他/"}]},{"title":"如何使用GitHub多人協作","date":"2018-06-03T10:08:21.000Z","path":"2018/06/03/github_t/","text":"什麼是GitHub不知道趕緊百度or谷歌 Github 安装首先肯定是註冊賬號不用多說。Github一般使用Git來管理。如果你喜歡圖形界面請直接跳到Github Desktop Git下載安裝Git Downloads 配置Git 右鍵 Git Bash，輸入: 1ssh-keygen -t rsa -C &quot;your_email@youremail.com&quot; 后面的your_email@youremail.com改为你在github上注册的邮箱，指定 RSA算法生成密钥，回车三次，密码不用输，生成两个隐藏文件id_rsa和id_rsa.pub，有顯示文件位置。 接下来就把公钥id_rsa.pub的内容拷贝到 GitHub需要的地方就可以了。如下： 點擊New SSH key, Title 隨便寫, ssh 就填id_rsa.pub的內容。 验证是否成功，在Git Bash中输入ssh -T git@github.com输入yes进行测试： 如图就是成功。 若第一次使用，最好设置邮箱和名字。 12git config --global user.name &quot;your name&quot;git config --global user.email &quot;your_email@youremail.com&quot; 多人協作(分支)現在有一個github倉庫，裡面有不同人的東西，比如每個人的作業。或同一個任務，不同idea，就可以使用分支區分開，方便管理。 首先建一個你中意的文件夾，git bash 輸入: 1git init 初始化。 添加遠程庫 以S-S_zoo為例: 1git remote add origin git@github.com:wenbo8585/S-S_zoo.git 此時，你就有一個遠程庫連接，她的名字為origin,你可以通過git remote來查看當前的遠程連接。 clone 执行如下命令將遠程仓库內容克隆到本地生成一個本地克隆庫： 1git clone git@github.com:wenbo8585/S-S_zoo.git clone下來默認分支是master，使用git branch -a查看分支，如果是多人協作，你需要另起以個分支branch，如圖: 創建并切換到分支wb_feature,名字自己取。 1git checkout -b wb_feature 現在你可以在當前目錄（本地倉庫）自由創作，你的本地仓库由 git 维护的三棵”树”组成。第一个是你的工作目录，它持有实际文件；第二个是 暂存区（Index），它像个缓存区域，临时保存你的改动；最后是 HEAD，它指向你最后一次提交的结果。 你可以修改文件把它们添加到暂存区，使用下面其中一個命令：12git add &lt;filename&gt;git add * # *指代目錄下所以文件 此時你的改動添加到了緩存區。但還沒有本地倉庫的實際改動使用如下命令以实际提交實際改動： 1git commit -m &quot;注釋信息&quot; 注釋信息相當於你每次改動的標籤。现在，你的改动已经提交到了HEAD，本地倉庫已經更新了改動，但是还没到提交到遠程倉庫。 如果你想提交修改到遠程，你可以選擇兩種方式： 提交feature_x分支： 1git push origin feature_x 這樣GitHub上就有feature_x分支。 合併merge到master 如果不想上傳feature_x分支，只使用master分支，你可以選擇合併。 切換回主分支： 1git chechout master 你可以選擇把新建的分支删掉： 1git branch -d feature_x merge: 1git merge feature_x 但兩個分支有衝突時（conflicts），比如 feature_x 分支修改了README，这时候就需要你手動合併這些衝突（conflicts）。改完之后，再進行add、commit： 12git add &lt;filename&gt;git commit -m &quot;注釋&quot; 此時再進行git push origin master就可以了。 注：在合并改动之前，你可以使用如下命令预览差异：1git diff &lt;source_branch&gt; &lt;target_branch&gt; 另外 如果GitHub上東西是新的，本地倉庫是舊的，你需要使用git pull命令,將遠程倉庫更新下載合併到當前本地倉庫分支。 1git pull &lt;远程主机名&gt; &lt;远程分支名&gt;:&lt;本地分支名&gt; 例： 1git pull origin next:master 取回origin主機的next分支，與本地的master分支合併。git pull 不加參數就是當前分支与远程库master進行同步更新。 远程分支(next)与當前分支合併，可簡寫為： 1git pull origin next 多人協作(參與項目)百度經驗 多人協作(權限管理)參考：http://www.cnblogs.com/zhaoyanjun/p/5882784.html 多人協作(邀請協作)參考：http://www.cnblogs.com/zhaoyanjun/p/5829142.html Github Desktop下載安裝Desktop。","tags":[{"name":"Git","slug":"Git","permalink":"http://wenbo.fun/tags/Git/"}]},{"title":"Matlab for Ubuntu","date":"2018-05-29T00:08:21.000Z","path":"2018/05/29/mat4u/","text":"安装下载地址: MATLAB R2017a Linux 新建一个matlab文件夹，拷贝MATLAB R2017a Linux到Ubuntu. cd到目录下，挂载第一个iso文件到matlab文件夹: 1sudo mount -o loop R2017a_glnxa64_dvd1.iso ~/matlab/ 挂载好直接安装: 1sudo ~/matlab/install 选第二个： 秘钥09806-07443-53955-64350-21751-41297，安装文件夹readme里有: 安装中途会提示换盘继续安装，另开一个终端： 12sudo umount ~/matlabsudo mount -o loop R2017a_glnxa64_dvd2.iso ~/matlab/ 安装结束后取消挂载： 1sudo umount ~/matlab 激活进行激活, 建一个licences文件夹: 1sudo mkdir /usr/local/MATLAB/R2017a/bin/licenses/ 到文件夹 Matlab 2017a Linux64 Crack 下拷贝一些东西: 1cd Matlab\\ 2017a\\ Linux64\\ Crack/ 拷贝license_standalone.lic: 1sudo cp license_standalone.lic /usr/local/MATLAB/R2017a/bin/licenses/ 拷贝libmwservices.so， libmwservices.so有空格: 1sudo cp MATLAB_Production_Server/R2017a/bin/glnxa64/libmwservices.so /usr/local/MATLAB/R2017a/bin/glnxa64/ 运行matlab: 1sudo /usr/local/MATLAB/R2017a/bin/matlab 选第二个: 选择刚才的license_standalone.lic文件: 激活完成 生成图标方便启动建一个desktop图标: 12cd ~/.local/share/applicationssudo gedit matlab.desktop 粘贴下面字眼，无需更改，直接用 12345678910[Desktop Entry]Type=ApplicationName=MatlabGenericName=Matlab 2017aComment=Matlab:The Language of Technical ComputingExec=sh /usr/local/MATLAB/R2017a/bin/matlab -desktopIcon=/usr/local/MATLAB/R2017a/toolbox/nnet/nnresource/icons/matlab.pngStartupNotify=trueTerminal=falseCategories=Development;Matlab; ok,软件列表搜索下matlab就能看到图标，你可以把它拖拽到Dock方便启动. 问题若点击图标出现错误: 修改下R2017a权限就可以了. 12cd ~/.matlabsudo chmod 777 R2017a Linux for Matlab中文注释乱码 中文注释乱码的原因是windows下的m文件采用的是gbk编码，只要将所有的m文件转成 utf8文件，显示就正常了。（在转成utf8前，别用matlab打开m文件，否则下面步骤完成后注释依旧有乱码，后果很严重。。） 首先安装enca：sudo apt-get install enca 进入m文件所在的文件夹，比如我的文件在/home/lx 里面：cd /home/lx 将所有m文件转成utf8：enca -x utf-8 * Enjoy.","tags":[]},{"title":"CS231n学习笔记 Lecture 8","date":"2018-05-09T11:00:08.000Z","path":"2018/05/09/L8/","text":"Lecture 8 | Software TensorFlow 使用Anaconda安装 Barebone TensorFlow一个简单的三层卷积网络: 123456789101112# Apply Convolution conv1 = tf.nn.conv2d(x, conv_w1, strides=[1,1,1,1], padding='SAME') conv2 = tf.nn.conv2d(conv1, conv_w2, strides=[1,1,1,1], padding='SAME') # Add bias conv1 = tf.nn.bias_add(conv1, conv_b1) conv2 = tf.nn.bias_add(conv2, conv_b2) # Apply activation function conv1 = tf.nn.relu(conv1) conv2 = tf.nn.relu(conv2) # Apply Full conv2_flat = flatten(conv2) scores = tf.matmul(conv2_flat, fc_w) + fc_b init: 123456789101112.....channel_1 = 32channel_2 = 64conv_w1 = tf.Variable(kaiming_normal((5, 5, 3, channel_1)))conv_b1 = tf.Variable(tf.zeros(channel_1,))conv_w2 = tf.Variable(kaiming_normal(( 3, 3, channel_1, channel_2)))conv_b2 = tf.Variable(tf.zeros(channel_2,))fc_w = tf.Variable(kaiming_normal((channel_2*32*32, 10)))fc_b = tf.Variable(tf.zeros(10,))params = [conv_w1, conv_b1, conv_w2, conv_b2, fc_w, fc_b] padding 用一个3x3的网格在一个28x28的图像上做切片并移动移动到边缘上的时候，如果不超出边缘，3x3的中心就到不了边界因此得到的内容就会缺乏边界的一圈像素点，只能得到26x26的结果 而可以越过边界的情况下，就可以让3x3的中心到达边界的像素点超出部分的矩阵补零 TensorFlow uses N x H x W x C but PyTorch uses N x C x H x W. Keras Model API1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556class ThreeLayerConvNet(tf.keras.Model): def __init__(self, channel_1, channel_2, num_classes): super().__init__() ######################################################################## # TODO: Implement the __init__ method for a three-layer ConvNet. You # # should instantiate layer objects to be used in the forward pass. # ######################################################################## #pass initializer = tf.variance_scaling_initializer(scale=2.0) self.conv1 = tf.layers.Conv2D(channel_1,kernel_size=5, strides=1, padding= \"SAME\", activation=tf.nn.relu, use_bias=\"TRUE\", bias_initializer=initializer, kernel_initializer=initializer) self.conv2 = tf.layers.Conv2D(channel_2,kernel_size=3, strides=1, padding= \"SAME\", activation=tf.nn.relu, use_bias=\"TRUE\", bias_initializer=initializer, kernel_initializer=initializer) self.fc = tf.layers.Dense(num_classes, use_bias=\"TRUE\", bias_initializer=initializer, kernel_initializer=initializer) ######################################################################## # END OF YOUR CODE # ######################################################################## def call(self, x, training=None): scores = None ######################################################################## # TODO: Implement the forward pass for a three-layer ConvNet. You # # should use the layer objects defined in the __init__ method. # ######################################################################## #pass x = self.conv1(x) x = self.conv2(x) x = tf.layers.flatten(x) scores = self.fc(x) ######################################################################## # END OF YOUR CODE # ######################################################################## return scores``` ## Keras Sequential API```pythonchannel_1, channel_2, num_classes= 16, 32, 10initializer = tf.variance_scaling_initializer(scale=2.0)layers = [tf.layers.Conv2D(channel_1,kernel_size=5, strides=1, padding= \"SAME\", activation=tf.nn.relu, use_bias=\"TRUE\", bias_initializer=initializer, kernel_initializer=initializer), tf.layers.Conv2D(channel_2,kernel_size=3, strides=1, padding= \"SAME\", activation=tf.nn.relu, use_bias=\"TRUE\", bias_initializer=initializer, kernel_initializer=initializer), tf.layers.Flatten(), tf.layers.Dense(num_classes, use_bias=\"TRUE\", bias_initializer=initializer, kernel_initializer=initializer) ]model = tf.keras.Sequential(layers) PytorchBarebones PyTorch: Three-Layer ConvNet123456789101112131415161718192021222324252627282930313233343536373839404142def three_layer_convnet(x, params): \"\"\" Performs the forward pass of a three-layer convolutional network with the architecture defined above. Inputs: - x: A PyTorch Tensor of shape (N, 3, H, W) giving a minibatch of images - params: A list of PyTorch Tensors giving the weights and biases for the network; should contain the following: - conv_w1: PyTorch Tensor of shape (channel_1, 3, KH1, KW1) giving weights for the first convolutional layer - conv_b1: PyTorch Tensor of shape (channel_1,) giving biases for the first convolutional layer - conv_w2: PyTorch Tensor of shape (channel_2, channel_1, KH2, KW2) giving weights for the second convolutional layer - conv_b2: PyTorch Tensor of shape (channel_2,) giving biases for the second convolutional layer - fc_w: PyTorch Tensor giving weights for the fully-connected layer. Can you figure out what the shape should be? - fc_b: PyTorch Tensor giving biases for the fully-connected layer. Can you figure out what the shape should be? Returns: - scores: PyTorch Tensor of shape (N, C) giving classification scores for x \"\"\" conv_w1, conv_b1, conv_w2, conv_b2, fc_w, fc_b = params scores = None ################################################################################ # TODO: Implement the forward pass for the three-layer ConvNet. # ################################################################################ conv1 = F.conv2d(x, weight=conv_w1, bias=conv_b1, padding=2) relu1 = F.relu(conv1) conv2 = F.conv2d(relu1, weight=conv_w2, bias=conv_b2, padding=1) relu2 = F.relu(conv2) relu2_flat = flatten(relu2) scores = relu2_flat.mm(fc_w) + fc_b ################################################################################ # END OF YOUR CODE # ################################################################################ return scores Module API: Three-Layer ConvNet12345678910111213141516171819202122232425262728293031323334353637383940class ThreeLayerConvNet(nn.Module): def __init__(self, in_channel, channel_1, channel_2, num_classes): super().__init__() ######################################################################## # TODO: Set up the layers you need for a three-layer ConvNet with the # # architecture defined above. # ######################################################################## self.conv1 = nn.Conv2d(in_channel, channel_1, kernel_size=5, padding=2, bias=True) nn.init.kaiming_normal_(self.conv1.weight) nn.init.constant_(self.conv1.bias, 0) self.conv2 = nn.Conv2d(channel_1, channel_2, kernel_size=3, padding=1, bias=True) nn.init.kaiming_normal_(self.conv2.weight) nn.init.constant_(self.conv2.bias, 0) self.fc = nn.Linear(channel_2*32*32, num_classes) nn.init.kaiming_normal_(self.fc.weight) nn.init.constant_(self.fc.bias, 0) ######################################################################## # END OF YOUR CODE # ######################################################################## def forward(self, x): scores = None ######################################################################## # TODO: Implement the forward function for a 3-layer ConvNet. you # # should use the layers you defined in __init__ and specify the # # connectivity of those layers in forward() # ######################################################################## relu1 = F.relu(self.conv1(x)) relu2 = F.relu(self.conv2(relu1)) scores = self.fc(flatten(relu2)) ######################################################################## # END OF YOUR CODE # ######################################################################## return scores Sequential API: Three-Layer ConvNet123456789101112131415161718192021222324252627282930313233343536373839channel_1 = 32channel_2 = 16learning_rate = 1e-2model = Noneoptimizer = None################################################################################# TODO: Rewrite the 2-layer ConvNet with bias from Part III with the ## Sequential API. #################################################################################model = nn.Sequential( nn.Conv2d(3, channel_1, kernel_size=5, padding=2), nn.ReLU(), nn.Conv2d(channel_1, channel_2, kernel_size=3, padding=1), nn.ReLU(), Flatten(), nn.Linear(channel_2*32*32, 10),)optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, nesterov=True)# Weight initialization# Ref: http://pytorch.org/docs/stable/nn.html#torch.nn.Module.applydef init_weights(m): #print(m) if type(m) == nn.Conv2d or type(m) == nn.Linear: random_weight(m.weight.size()) zero_weight(m.bias.size())model.apply(init_weights)################################################################################# END OF YOUR CODE ################################################################################train_part34(model, optimizer) Caffe2 + PyTorch = PyTorch 1.0趋势：PyTorch 1.0吸取了Caffe2和ONNX的模块化、面向生产的特点，并将它们与PyTorch现有的灵活、注重研究的设计结合起来，使得各项AI项目能从研究原型快速地无缝衔接到生产部署。 贾扬清: Caffe2 + PyTorch = PyTorch 1.0","tags":[{"name":"笔记","slug":"笔记","permalink":"http://wenbo.fun/tags/笔记/"},{"name":"CNN","slug":"CNN","permalink":"http://wenbo.fun/tags/CNN/"}]},{"title":"CS231n学习笔记 Lecture 7","date":"2018-05-08T13:08:08.000Z","path":"2018/05/08/L7/","text":"Lecture 7 | Training Neural Networks OptimizationSGD: local minima &amp; saddle point SGD + Momentum 超参数$\\rho$（摩擦系数）用来衰减速度。如图： Nesterov Momentum下半部分是等式变换，好计算。 计算顺序变了，look ahead。计算蓝框处的梯度。如图： AdaGrad &amp; RMSProp AdaGrad: 累加梯度平方，加速小梯度维度学习速度。 最终会越来越慢。RMSProp: 不累加梯度平方，让梯度平方按照一定比率下降(delay_rate)。 Adam 结合Momentum和AdaGrad/RMSProp的优点。 Model Ensembles概念：是对其他算法进行组合的一种形式。 通俗来说： 当做重要决定时，大家可能都会考虑吸取多个专家而不只是一个人的意见。 机器学习处理问题时又何尝不是如此？ 这就是集成方法背后的思想。 集成方法： 投票选举(bagging: 自举汇聚法 bootstrap aggregating): 是基于数据随机重抽样分类器构造的方法.bagging 方法最流行的版本是: 随机森林(random forest). 再学习(boosting): 是基于所有分类器的加权求和的方法.AdaBoost 集成方法 ensemble method TensorFlow Totorials Regularizationadd term to loss L2 Regularization Dropout随机激活函数置零。 more time. 一般在全连接层使用，卷积层往往是随机把整个特征映射为零。DropConnect 随机权重矩阵置零。一样效果。 Data Augmentation translation(平移)、 stretching(拉伸) shearing(裁剪) rotation(旋转)、 Color Jitter(色彩抖动?) lens distortions(扭曲)… More Fractional Max Pooling Stochastic Depth 训练用部分随机层，测试用全部网络。[new idea] Transfer Learning (Quick Look)You don’t need a huge amount of data.","tags":[{"name":"笔记","slug":"笔记","permalink":"http://wenbo.fun/tags/笔记/"},{"name":"CNN","slug":"CNN","permalink":"http://wenbo.fun/tags/CNN/"}]},{"title":"CS231n学习笔记 Lecture 6","date":"2018-05-02T11:00:08.000Z","path":"2018/05/02/L6/","text":"Lecture 6 | Training Neural Networks Activation Functions Sigmoid Leaky ReLU tanh Maxout ReLU ELU $$ \\begin{cases} x \\qquad(x \\geq 0) \\\\ \\alpha(e^x - 1) \\qquad(x &lt; 0) \\end{cases}$$ point: zero-centered Data Preprocessingfor images:center only Subtract the mean image e.g. AlexNet Subtract per-channel mean e.g. VGGNet Weight InitializationBatch NormalizationBatch normalization做什么:在训练过程计算一个minibatch的标准差和均值,测试过程用来对feature进行中心化和归一化.问题:可能会导致网络表征力下降.因为对于有一些层来说,非中心化,非归一化可能更好一些. 对卷积网络有点不同. 论文: Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift 一篇论文阅读解释 Usually inserted after FC or Convolutional layers, and before nonlinearity. 注：forward的时候bn层放在relu前面,bp的时候放在relu后面! Babysitting the Learning PreprocessDouble check loss loss not going down: learning rate too low; NaN: high learning rate. Hyperparameter Optimization Cross-validation strategy","tags":[{"name":"笔记","slug":"笔记","permalink":"http://wenbo.fun/tags/笔记/"},{"name":"CNN","slug":"CNN","permalink":"http://wenbo.fun/tags/CNN/"}]},{"title":"CS231n学习笔记 Module 2.1","date":"2018-04-27T11:00:08.000Z","path":"2018/04/27/cs231n_m2_1/","text":"Lecture 5 | Convolutional Neural Networks ArchitectureIt transforms an input 3D volume to an output 3D volume. 常见的模式 INPUT -&gt; [[CONV -&gt; RELU]*N -&gt; POOL?]*M -&gt; [FC -&gt; RELU]*K -&gt; softmax  POOL? 指的是一个可选的pooling layer N &gt;= 0 (and usually N &lt;= 5) M &gt;= 0 K &gt;= 0 (and usually K &lt; 3) Conv layer Convolutional Layer, Pooling Layer, Fully-Connected Layer 最简单的双层CNN(一个隐含层): [INPUT - CONV - RELU - POOL - FC] 总的特点: 每一层都是从3D volume到3D volume的转换. CONV/FC有参数学习(w,b), RELU/POOL没有. CONV/FC/POOL 有超参数学习(learning rate等等), RELU没有. 感受野(receptive field)每个神经元只与输入数据的一个局部区域连接。该连接的空间大小叫做神经元的感受野（receptive field） 3个超参数控制着输出数据体的尺寸 深度（depth）: = filters的个数. 步长（stride）: 1 or 2 usually 零填充（zero-padding）: 保持输入和输出的宽高都相等 the spatial size of the output volume (formula):$$(W-F +2P)/S+1$$ input volume size (W),the receptive field size (F)the stride (S)the amount of zero padding used (P) 要能够整除 权值共享权值共享指的是一层filter上(55*55)神经元,共享5*5*3的这个感受野的权值! 12个filters,就有12个[5*5]的感受野,12组权值.也就是(5*5*3+1)*12=76*12=912 BP的时候,把所有神经元的梯度累加,来更新这一个共享的filter的权重. 可以这么做的缘故是:图像具有结构上的平移不变性.但是在人脸中不太一样,有时候我们需要做局部连接. eg:W_row = [96 x 363]X_col = [363 x 3025] 计算整个卷积层的输出(一共96层filters) : np.dot(W_row, X_col) 最后把结果reshape成[96*55*55] 1*1的卷积 降维（ dimension reductionality ）。比如，一张500 x 500且厚度depth为100(一般是3 channels)的图片在20个filter上做1x1的卷积，那么结果的大小为500x500x20。 加入非线性。卷积层之后经过激励层，1x1的卷积在前一层的学习表示上添加了非线性激励（ non-linear activation ），提升网络的表达能力； Pooling Layer作用:降维,减少参数量,降低过拟合. Max polling的效果比average好 GAN等不太适于用pooling,可以用增大步长（stride）的方法降低数据量,未来可能会逐步discard pooling层. Normalization and Fully-connected Layer归一化层: 模仿生物神经网络中的抑制作用,in practice,贡献很小.全连接层: 和前一层全连接,乘积加偏置. FC-&gt;CNN:用一个和输入同等大小的卷积核即可实现.优点:可以让卷积网络在一张更大的输入图片上滑动，得到每个区域的输出（这样就突破了输入尺寸的限制）. 结构选择选择几个小感受野的卷积层,而不是一个大感受野的卷积层. 多个卷积层用非线性函数提取了更多图像的特征 减少了参数量 In practice:直接下一个最好的网络,然后fine tune它! 层的尺寸超参数调参卷积层: 输入数据的尺度最好可以被2除很多次.F小一些,3*3或者5*5常见,如果一定要用7*7这么大的,一般只用在第一层卷积层上. S=1.P=(F−1)/2, 这样可以使得输入输出数据尺度不变. Pooling层:F=2,一般不超过3S=2用max pooling 一些参数选择问题: 为什么用s=1?让卷积层只负责降低数据深度,而pooling层只负责降低数据长宽. 不使用padding可能使得图像的边缘信息迅速损失掉. 出于对GPU性能的考虑,应当做出一定的妥协,一般发生在第一层. 知名的CNN LeNet:最早对CNN进行应用(1990),主要用来识别数字和邮政编码 AlexNet:2012年ImageNet ILSVRC比赛冠军,更深,更大,第一次用多层卷积层来提取特征(以前都是Conv后面立刻跟一个pooling) ZF Net:2013年ILSVRC冠军,就是对AlexNet调参,调高中间卷积层的面积,第一层的stride和filter的size调低. GoogLeNet:2014年ILSVRC冠军.使用inception结构极大的降低了参数量.使用了平均pooling.(4M, compared to AlexNet with 60M) VGGNet:2014年ILSVRC亚军.网络的深度体现算法的优度.从头到尾全部使用的是3x3的卷积和2x2的pooling!缺点在于参数太多了.绝大多数都来源于第一层的全连接,后来发现把全连接去掉不会影响性能,这样参数就大大降低了. ResNet:2015年ILSVRC冠军.It features special skip connections and a heavy use of batch normalization,没有全连接层. Vgg:Vgg中,几乎全部的计算和内存消耗都发生在前面的卷积层,但是参数量最大的是后面的全连接层,这很常见. 计算内存占用时的trick 测试时抛弃前面层的激活值.(训练的时候不可以,要用来求梯度) 存储参数的内存往往要在总的参数量上x3(参数值,参数的梯度,有一些bp的过程比如动量法还要多存一个cache) 在2的基础上还有一些额外的空间 最后的计算结果应该以GB为单位.把所有的参数量*4(如果是double还要*8),除以1024几次去得到MB,GB级的结果.如果发现内存不够,可以用减少batch_size的方法,因为大多数内存消耗是在卷积层的激活值那里的.","tags":[{"name":"笔记","slug":"笔记","permalink":"http://wenbo.fun/tags/笔记/"},{"name":"CNN","slug":"CNN","permalink":"http://wenbo.fun/tags/CNN/"}]},{"title":"CS231n学习笔记 Module 1.4","date":"2018-04-18T11:00:08.000Z","path":"2018/04/18/cs231n_m1_4/","text":"Lecture 4 | Neural Networks Part 1: Setting up the Architecture a single neuron might look as follows: 1234567class Neuron(object): # ... def forward(self, inputs): \"\"\" assume inputs and weights are 1-D numpy arrays and bias is a number \"\"\" cell_body_sum = np.sum(inputs * self.weights) + self.bias firing_rate = 1.0 / (1.0 + math.exp(-cell_body_sum)) # sigmoid activation function return firing_rate Sigmoid (-) 在饱和的时候梯度很小,因为求gradient的时候总是需要用(local gradient) × (upstream gradient),如果local gradient很小那么网络再往前传播的时候很容易没有gradient了,网络也就不能学习. (-) 梯度下降权重更新时出现z字型的下降.? Tanh range [-1, 1]. $\\tanh(x) = 2 \\sigma(2x) -1$ ReLU$f(x) = \\max(0, x)$ (+) 更快使得梯度下降收敛 (+) 容易实现 (-) learning rate很高的时候会容易杀死很多neurons Leaky ReLU $f(x) = \\mathbb{1}(x &lt; 0) (\\alpha x) + \\mathbb{1}(x&gt;=0) (x)$ 推荐用ReLu,如果恐惧神经元死亡问题,可以转而用Leaky ReLU来避免这种情况. Neural Network architecturesSizing neural networks: The network has 4 + 2 = 6 neurons (not counting the inputs), [3 x 4] + [4 x 2] = 20 weights and 4 + 2 = 6 biases,for a total of 26 learnable parameters. Example feed-forward computation123456# 一个3层神经网络的前向传播:f = lambda x: 1.0/(1.0 + np.exp(-x)) # 激活函数(用的sigmoid)x = np.random.randn(3, 1) # 含3个数字的随机输入向量(3x1)h1 = f(np.dot(W1, x) + b1) # 计算第一个隐层的激活数据(4x1)h2 = f(np.dot(W2, h1) + b2) # 计算第二个隐层的激活数据(4x1)out = np.dot(W3, h2) + b3 # 神经元输出(1x1) Representational power 层数 两层全连接神经网络可以近似表达任何一个函数! 全连接网络中3层会比2层表达效果更好,但是4,5……之后的深度对能力的提升作用不大. 在CNN中,深度却是一个非常重要的影响表达能力的因素! 每层的个数 越多越好!1.越多越能拟合复杂的函数.好处是能拟合复杂的数据,坏处是容易过拟合.可以用其他方法防止过拟合。2.每层个数少的网络每次训练出来的结果经常是都不一样的,但是大一些的网络,每次的结果往往相似(loss差异不大). Part 2: Setting up the Data and the Loss数据预处理去均值$X -= np.mean(X, axis = 0)$ 归一化① 先零中心化,再对每一个维度除以标准差. $X /= np.std(X, axis = 0)$ ② 对每一个维度归一化,这样min和max分别是-1和1 对于图片而言不需要做归一化了,因为他们的数据范围都是在[0,255]之间的. Principal Component Analysis (PCA)协方差矩阵 首先给出 均值、方差、标准差 基本公式： 协方差 是仿照方差的定义, 来度量各个维度偏离其均值的程度： 对于多维，例如三维， 协方差矩阵 是计算不同维度之间的协方差： 协方差矩阵是一个对称和半正定的矩阵，而且对角线是各个维度的方差。 奇异值分解 SVD 特征值和特征向量 $$A\\nu = \\lambda\\nu $$ 方阵$A$ , 特征向量$\\nu$, 对应的特征值 $\\lambda$ 特征值分解是将一个矩阵分解成下面的形式： $$ A = Q\\Sigma Q^{-1} $$ $Q$是这个矩阵$A$的特征向量组成的矩阵，$\\Sigma$是一个对角阵，每一个对角线上的元素就是一个特征值。 注：变换的矩阵必须是方阵 奇异值分解 奇异值分解是能适用于任意的矩阵：$$A = U\\Sigma V^T$$ A是一个 N x M 的矩阵$U$ 是一个 N x N 的方阵（里面的向量是正交的，$U$里面的向量称为 左奇异向量）$\\Sigma$是一个 N x M 的矩阵（除了对角线的元素都是0，对角线上的元素称为 奇异值）$V^T$ 是一个 N x N 的矩阵，里面的向量也是正交的，称为 右奇异向量）: $A^TA$得到一个 M x M 方阵，用这个方阵求特征值.$$(A^TA)\\nu_i = \\lambda_i\\nu_i $$ $\\nu$就是我们上面的右奇异向量$$\\sigma_i = \\sqrt \\lambda_i$$$σ$就是上面的奇异值$$u_i = \\frac{1}{\\sigma_i} A\\nu_i$$$u$就是上面的左奇异向量 奇异值$σ$跟特征值类似，在矩阵$Σ$中也是从大到小排列，而且$σ$的减少特别的快，在很多情况下，前10%甚至1%的奇异值的和就占了全部的奇异值之和的99%以上了。也就是说，我们也可以用前$r$大的奇异值来近似描述矩阵： $$ A_{m\\times n} \\approx U_{m\\times r}\\Sigma_{r\\times r}V_{r\\times n}^T$$ r是一个远小于m、n的数，这样矩阵的乘法看起来像是下面的样子： 奇异值与主成分分析（PCA） PCA的问题其实是一个基的变换，使得变换后的数据有着最大的方差。以下面这张图为例子： 这个假设是一个摄像机采集一个物体运动得到的图片，上面的点表示物体运动的位置，假如我们想要用一条直线去拟合这些点，那我们会选择什么方向的线呢？当然是图上标有signal的那条线。如果我们把这些点单纯的投影到x轴或者y轴上，最后在x轴与y轴上得到的方差是相似的（因为这些点的趋势是在45度左右的方向，所以投影到x轴或者y轴上都是类似的），如果我们使用原来的xy坐标系去看这些点，容易看不出来这些点真正的方向是什么。但是如果我们进行坐标系的变化，横轴变成了signal的方向，纵轴变成了noise的方向，则就很容易发现什么方向的方差大，什么方向的方差小了。 一般来说，方差大的方向是信号的方向，方差小的方向是噪声的方向，我们在数据挖掘中或者数字信号处理中，往往要提高信号与噪声的比例，也就是信噪比。对上图来说，如果我们只保留signal方向的数据，也可以对原数据进行不错的近似了。 PCA的全部工作简单点说，就是对原始的空间中顺序地找一组相互正交的坐标轴，第一个轴是使得方差最大的，第二个轴是在与第一个轴正交的平面中使得方差最大的，第三个轴是在与第1、2个轴正交的平面中方差最大的，这样假设在N维空间中，我们可以找到N个这样的坐标轴，我们取前r个去近似这个空间，这样就从一个N维的空间压缩到r维的空间了，但是我们选择的r个坐标轴能够使得空间的压缩使得数据的损失最小。 还是假设我们矩阵每一行表示一个样本，每一列表示一个feature，用矩阵的语言来表示，将一个m x n的矩阵A的进行坐标轴的变化，P就是一个变换的矩阵从一个N维的空间变换到另一个N维的空间，在空间中就会进行一些类似于旋转、拉伸的变化。$$A_{m\\times n}P_{n\\times n} = \\sim A_{m\\times n}$$ SVD得出的奇异向量也是从奇异值由大到小排列的，按PCA的观点来看，就是方差最大的坐标轴就是第一个奇异向量，方差次大的坐标轴就是第二个奇异向量…我们回忆一下之前得到的SVD式子：$$A_{m\\times n} \\approx U_{m\\times r}\\Sigma_{r\\times r}V_{r\\times n}^T$$ 在矩阵的两边同时乘上一个矩阵$V$，由于$V$是一个正交的矩阵，所以$V$转置乘以$V$得到单位阵$I$，所以可以化成后面的式子: $$\\begin{cases}A_{m\\times n}V_{r\\times n} \\approx U_{m\\times r}\\Sigma_{r\\times r}V_{r\\times n}^TV_{r\\times n}\\\\ A_{m\\times n}V_{r\\times n} \\approx U_{m\\times r}\\Sigma_{r\\times r}\\end{cases} $$将后面的式子与A x P那个m x n的矩阵变换为m x r的矩阵的式子对照看看，在这里，其实$V$就是$P$，这里是将一个m x n 的矩阵压缩到一个m x r的矩阵，也就是对列进行压缩，如果我们想对行进行压缩（在PCA的观点下，对行进行压缩可以理解为，将一些相似的sample合并在一起，或者将一些没有太大价值的sample去掉）怎么办呢？同样我们写出一个通用的行压缩例子： $$ U^T_{r\\times m}A_{m\\times n}= \\Sigma_{r\\times r}V_{r\\times n}^T$$ 可以看出，其实PCA几乎可以说是对SVD的一个包装，如果我们实现了SVD，那也就实现了PCA了。 先对数据进行零中心化处理，然后计算协方差矩阵. 123# 假设输入数据矩阵X的尺寸为[N x D]X -= np.mean(X, axis = 0) # 对数据进行零中心化(重要)cov = np.dot(X.T, X) / X.shape[0] # 得到数据的协方差矩阵 对数据协方差矩阵进行SVD（奇异值分解）运算。 1U,S,V = np.linalg.svd(cov) S中元素是特征值的平方np.linalg.svd的一个良好性质是在它的返回值U中，特征向量是按照特征值的大小排列的。 为了去除数据相关性： 1Xrot = np.dot(X,U) # 对数据去相关性 将原始的数据集的大小由[N x D]降到了[N x 100] 1Xrot_reduced = np.dot(X, U[:,:100]) # Xrot_reduced 变成 [N x 100] Whitening(白化)白化是把每一个维度除以它的特征值,使得这个维度归一化.几何解释：if the input data is a multivariable gaussian, then the whitened data will be a gaussian with zero mean and identity covariance matrix. 注意:应该先把数据集划分成training/validation/test,然后计算训练集的均值,每个训练集都要减去这个均值,而不是各自的均值或所有数据的均值. Weight Initialization 全是0是错误的 Small random numbers W = 0.01* np.random.randn(D,H) Ps:并不是越小的权值就越好,它可能导致bp中返回很小的梯度,这在深度网络中并不是一件好事. 用n的平方根校准数据. W = np.random.randn(n) / sqrt(n) 经验上,使用ReLU激活函数，并且使用 w = np.random.randn(n) * sqrt(2.0/n)来进行权重初始化 Initializing the biases: it is more common to simply use 0 bias initialization. 批量归一化（Batch Normalization） 在全连接层和非线性操作之间添加一个归一化层. 批量归一化可以理解为在网络的每一层之前都做预处理，只是这种操作以另一种方式与网络集成在了一起。 Regularizationseveral ways to prevent overfitting: 在输入激励函数的值后面加上L1/L2: $\\lambda_1 \\mid w \\mid + \\lambda_2 w^2$ L2倾向于选择使得网络能够应用到全部输入数据而不是其中某一部分的权值(权值更弥散分布). 如果没有明确的特征选择倾向,L2 outperform than L1. 最大范式约束(max norm constraint),防止learning rate过高时网络爆炸. Dropout: 如上图,只激活一部分神经元而不是整个网络. 测试过程并不进行dropout. 实现 use inverted dropout： train：12345678910111213141516171819202122\"\"\"反向随机失活: 推荐实现方式.在训练的时候drop和调整数值范围，测试时不做任何事.\"\"\"p = 0.5 # 激活神经元的概率. p值更高 = 随机失活更弱def train_step(X): # 3层neural network的前向传播 H1 = np.maximum(0, np.dot(W1, X) + b1) U1 = (np.random.rand(*H1.shape) &lt; p) / p # first dropout mask. Notice /p! H1 *= U1 # drop! .....``` predict:```pythondef predict(X): # 前向传播时模型集成 H1 = np.maximum(0, np.dot(W1, X) + b1) # 不用数值范围调整了 ..... The value of $p=0.5$ is a reasonable default, but this can be tuned on validation data. Loss functions分类问题 类别数目巨大,当标签集非常庞大，就需要使用分层Softmax ,用树形结构表达classifier,一条路径就是一个class. 每个样本都有一个唯一的正确标签（是固定分类标签之一）。 在这类问题中，一个最常见的损失函数就是SVM. 属性（Attribute）分类:一张图片上可能有多个标签 ①对于每个attribute创建一个独立的二分类的分类器: 1选,0不选②对于每个attribute训练一个独立的逻辑回归分类器: &gt;0.5选,otherwise不选 回归问题是预测实数的值的问题，比如预测房价，预测图片中某个东西的长度等。对于这种问题，通常是计算预测值和真实值之间的损失。然后用L2平方范式或L1范式度量差异。 当面对一个回归任务，首先考虑是不是必须这样。一般而言，尽量把你的输出变成二分类，然后对它们进行分类，从而变成一个分类问题。 尽量用分类,分类相对于回归更能给出一个结果的分布,而不仅仅是一个精确值 结构化预测 指的是预测结果较为复杂,不仅仅是一个label,可能包含树形结构等等. Part 3: Learning and Evaluation梯度检查(1) Use the centered formula:$$\\frac{df(x)}{dx} = \\frac{f(x + h) - f(x)}{h} \\hspace{0.1in} \\text{(bad, do not use)}$$ (2) 使用相对误差而不是绝对误差:$$\\frac{\\mid f’_a - f’_n \\mid}{\\max(\\mid f’_a \\mid, \\mid f’_n \\mid)}$$ 在实践中： 相对误差&gt;1e-2：通常就意味着梯度可能出错。 1e-2&gt;相对误差&gt;1e-4：要对这个值感到不舒服才行。 1e-4&gt;相对误差：这个值的相对误差对于有不可导点的目标函数是OK的。但如果目标函数中没有kink（使用tanh和softmax），那么相对误差值还是太高。 1e-7或者更小：好结果，可以高兴一把了。 (3)使用双精度,保持在浮点数的有效范围 (4)不可导点:有时候f(x+h)和f(x-h)这一步可能越过不可导点.通过跟踪max()函数中的赢家什么时候变化了可以认为越过了不可导点. (5)使用少量数据做梯度检查.可以避免越过不可导点并且可以以偏概全,更快效率更高. (6)步长并不是越小越好,有时候h过小会导致问题(3),一般1e-4~1e-6很合适. (7)在网络预热一段时间,开始梯度下降之后再进行梯度检查. (8)预防正则项overwhelm数据.可以先去掉正则项或者直接加强正则项. (9)记得关闭随机失活（dropout）和数据扩张（augmentation）。在进行梯度检查时，记得关闭网络中任何不确定的效果的操作，比如随机失活，随机数据扩展等。不然它们会在计算数值梯度的时候导致巨大误差。 (10)检查少量的维度 Before learning(1)检查特定情况的loss.(2)随着正则化项加强,loss应当增加.(3)先过拟合小数据集. 学习过程 loss train/validate accuracy相差过大说明过拟合,如影随形说明模型参数太少. Ratio of weights:updates一个经验性的结论是这个比例应该在1e-3左右。如果更低，说明学习率可能太小，如果更高，说明学习率可能太高。 每层的激活值画出柱状图,例如对于tanh,应该在[-1,1]之间. First-layer Visualizations第一层特征可视化会有帮助 Parameter updates 普通更新 动量更新 梯度影响速度,速度影响位置. Nesterov动量 学习率退火(Annealing the learning rate) 二阶方法 逐参数适应学习率方法(Per-parameter adaptive learning rate methods) (1) Adagrad (2) RMSprop (3) Adam Hyperparameter optimization训练一个神经网络会遇到很多超参数设置。神经网络最常用的设置有： 初始学习率。 学习率衰减方式（例如一个衰减常量）。 正则化强度（L2惩罚，随机失活强度）。 实现问题worker用来记录checkpoint(validation的表达准确率),一个master用来调控workers. 用一个验证集而不是交叉验证 超参数范围(不懂) learning_rate = 10 ** uniform(-6, 1)但是有一些参数（比如随机失活）还是在原始尺度上进行搜索（例如：dropout=uniform(0,1)） 随机搜索比网格搜索结果更好 边界最优值万一最优参数出现在边界上,要小心是不是错过了其他更好的参数,这一般发生在初始范围设定的不太好的前提之下. 从粗糙到细致地搜索 贝叶斯超参数最优化 Evaluation模型集成 提升神经网络表达效果的一个可靠方式就是计算好几个独立模型的结果,然后求均值,模型数量越多,准确度越高,当然准确度提升的越慢. 用最好的一组参数训练不同初始化权重的模型. 用最好的几组参数训练几个模型.交叉验证之后就不需要再训练模型了. 就用训练超参数过程中的几个checkpoint对应的模型直接集成.很省事. 一旦这一次损失值相对于上一次出现指数下降,就记录下来权重,对这几个模型集成.效果总是很好. 模型集成的劣势是消耗时间,参考hinton的论文,从一个好的集成中抽一个单独的模型出来. 推荐的两个更新方法是SGD+Nesterov动量方法，或者Adam方法。","tags":[{"name":"笔记","slug":"笔记","permalink":"http://wenbo.fun/tags/笔记/"},{"name":"CNN","slug":"CNN","permalink":"http://wenbo.fun/tags/CNN/"}]},{"title":"CS231n学习笔记 Module 1.3","date":"2018-04-17T02:00:08.000Z","path":"2018/04/17/cs231n_m1_3/","text":"Lecture 3 | BackpropSimple expressions and interpretation of the gradientmax operation:$$f(x,y) = \\max(x, y) \\hspace{0.5in} \\rightarrow \\hspace{0.5in} \\frac{\\partial f}{\\partial x} = \\mathbb{1}(x &gt;= y) \\hspace{0.5in} \\frac{\\partial f}{\\partial y} = \\mathbb{1}(y &gt;= x)$$ Compound expressions with chain ruleFor example:$\\frac{\\partial f}{\\partial x} = \\frac{\\partial f}{\\partial q} \\frac{\\partial q}{\\partial x}$ . eg. $f(x,y,z) = (x + y) z$ 求梯度。1234567891011x = -2; y = 5; z = -4q = x + yf = q * z# first backprop through f = q * zdfdz = q # df/dz = q = 3dfdq = z # -4# backprop though q = x + ydfdx = 1.0 * dfdq # dq/dx = 1, chain rule!dfdy = 1.0 * dfdq 因为输入梯度为负，想要增大输出值，可以减小输入值。 梯度大小可以看做贡献。 Modularity: Sigmoid example$$f(x) = \\frac{1}{1 + e^{-x}}$$ $$f’(x) = f(x)(1-f(x)) $$ 可以将多个门结合成一个计算梯度，Lets look at another expression：$$f(w,x) = \\frac{1}{1+e^{-(w_0x_0 + w_1x_1 + w_2)}}$$ 这是一个输入为 x，权重为 w 的 2-D 神经元. 该神经元反向传播的代码实现如下： 1234567891011w = [2,-3,-3] # 假设一些随机数据和权重x = [-1, -2]# 前向传播dot = w[0]*x[0] + w[1]*x[1] + w[2]f = 1.0 / (1 + math.exp(-dot)) # sigmoid函数# 对神经元反向传播ddot = (1 - f) * f # 点积变量的梯度, 使用sigmoid函数求导dx = [w[0] * ddot, w[1] * ddot] # 回传到xdw = [x[0] * ddot, x[1] * ddot, 1.0 * ddot] # 回传到w $$f(x) = w_1x_1 + w_2x_2+w_3$$$$g(x) = { 1 \\over {1 + e^{-f(x)}}}$$$${dg(x) \\over dx }= {dg(x)\\over df(x)}{df(x) \\over dx}$$ As shown above , staged backpropagation is always helpful. (f(x) &amp; g(x)) Staged computationeg:$$\\sigma(x) = \\frac{1}{1+e^{-x}} $$$$f(x,y) = \\frac{x + \\sigma(y)}{\\sigma(x) + (x+y)^2}$$ x = 3, y = -4 123456789101112x = 3 # example valuesy = -4# forward passsigy = 1.0 / (1 + math.exp(-y)) # sigmoid in numerator #(1)num = x + sigy # numerator #(2)sigx = 1.0 / (1 + math.exp(-x)) # sigmoid in denominator #(3)xpy = x + y #(4)xpysqr = xpy**2 #(5)den = sigx + xpysqr # denominator #(6)invden = 1.0 / den #(7)f = num * invden # done! #(8) 需要注意的一些东西： 对前向传播变量进行缓存：在计算反向传播时，前向传播过程中得到的一些中间变量非常有用。在实际操作中，最好代码实现对于这些中间变量的缓存，这样在反向传播的时候也能用上它们。如果这样做过于困难，也可以（但是浪费计算资源）重新计算它们。 123456789101112131415161718192021# backprop f = num * invdendnum = invden # gradient on numerator #(8)dinvden = num #(8)# backprop invden = 1.0 / dendden = (-1.0 / (den**2)) * dinvden #(7)# backprop den = sigx + xpysqrdsigx = (1) * dden #(6)dxpysqr = (1) * dden #(6)# backprop xpysqr = xpy**2dxpy = (2 * xpy) * dxpysqr #(5)# backprop xpy = x + ydx = (1) * dxpy #(4)dy = (1) * dxpy #(4)# backprop sigx = 1.0 / (1 + math.exp(-x))dx += ((1 - sigx) * sigx) * dsigx # Notice += !! See notes below #(3)# backprop num = x + sigydx += (1) * dnum #(2)dsigy = (1) * dnum #(2)# backprop sigy = 1.0 / (1 + math.exp(-y))dy += ((1 - sigy) * sigy) * dsigy #(1)# done! phew 在不同分支的梯度要相加：如果变量x，y在前向传播的表达式中出现多次，那么进行反向传播的时候就要非常小心，使用+=而不是=来累计这些变量的梯度（不然就会造成覆写）。这是遵循了在微积分中的多元链式法则，该法则指出如果变量在线路中分支走向不同的部分，那么梯度在回传的时候，就应该进行累加。 Patterns in backward flow add gate: 梯度等分分给输入 max gate： 传给最大值 multiply gate： 相互交换相乘 Gradients for vectorized operationsMatrix-Matrix multiply gradient： 在操作的时候要注意关注维度和转置操作. 123456789# 前向传播W = np.random.randn(5, 10)X = np.random.randn(10, 3)D = W.dot(X)# 假设我们得到了D的梯度dD = np.random.randn(*D.shape) # 和D一样的尺寸dW = dD.dot(X.T) #.T就是对矩阵进行转置dX = W.T.dot(dD) concise 简明的be aware of 清楚知道","tags":[{"name":"笔记","slug":"笔记","permalink":"http://wenbo.fun/tags/笔记/"},{"name":"CNN","slug":"CNN","permalink":"http://wenbo.fun/tags/CNN/"}]},{"title":"CS231n学习笔记 Module 1.1","date":"2018-04-11T09:15:17.000Z","path":"2018/04/11/cs231n_m1_1/","text":"Abstract Assignment Git:https://github.com/CS231n-zju/CS231n 视频地址：https://www.youtube.com/watch?v=vT1JzLTH4G4&amp;index=2&amp;list=PLe7764SJVnV10-Nr7e0sBlC9J0LRf4sQo 课程作业：http://cs231n.github.io/ Syllabus:http://cs231n.stanford.edu/syllabus.html Note翻译:http://www.52ml.net/17723.html Lecture 1Introduction to Convolutional Neural Networks for Visual Recognition NOTE 2012 CNN ImageNet Image ClassificationChallenges Viewpoint variation Scale variation Deformation Occlusion Illumination conditions Background clutter Intra-class variation： The classes of interest can often be relatively broad, such as chair. There are many different types of these objects, each with their own appearance.(一类物体的个体之间的外形差异很大，比如椅子。这一类物体有许多不同的对象，每个都有自己的外形。) The image classification pipeline Input: training set. Learning: training a classifier, or learning a model. Evaluation: In the end, we evaluate the quality of the classifier by asking it to predict labels for a new set of images that it has never seen before. Nearest Neighbor ClassifierHow we compare two images? L1 distance: $$d(I_1,I_2)=\\sum|I_1^p - I_2^p|$$ As an evaluation criterion, it is common to use the accuracy. 123456789101112131415161718192021222324252627import numpy as npclass NearestNeighbor(object): def __init__(self): pass def train(self, X, y): \"\"\" X is N x D where each row is an example. Y is 1-dimension of size N \"\"\" # the nearest neighbor classifier simply remembers all the training data self.Xtr = X self.ytr = y def predict(self, X): \"\"\" X is N x D where each row is an example we wish to predict label for \"\"\" num_test = X.shape[0] # lets make sure that the output type matches the input type Ypred = np.zeros(num_test, dtype = self.ytr.dtype) # loop over all test rows for i in xrange(num_test): # find the nearest training image to the i'th test image # using the L1 distance (sum of absolute value differences) distances = np.sum(np.abs(self.Xtr - X[i,:]), axis = 1) min_index = np.argmin(distances) # get the index with smallest distance Ypred[i] = self.ytr[min_index] # predict the label of the nearest example return Ypred L2 distance(欧氏距离): $$d(I_1,I_2)=\\sqrt{\\sum_p{|I_1^p - I_2^p|}^2}$$ 1distances = np.sqrt(np.sum(np.square(self.Xtr - X[i,:]), axis = 1)) the L2 distance is much more unforgiving than the L1 distance. k - Nearest Neighbor Classifier找最相似的k个图片的标签，然后让他们针对测试图片进行投票，最后把票数最高的标签作为对测试图片的预测。k的值？ Validation sets for Hyperparameter tuning (用于超参数调优的验证集) we cannot use the test set for the purpose of tweaking hyperparameters, overfit. Evaluate on the test set only a single time, at the very end. to split our training set in two: a slightly smaller training set, and what we call a validation set. 这里将5万张train中1千张作为验证集，来寻找最优的k。 1234567891011121314151617181920# assume we have Xtr_rows, Ytr, Xte_rows, Yte as before# recall Xtr_rows is 50,000 x 3072 matrixXval_rows = Xtr_rows[:1000, :] # take first 1000 for validationYval = Ytr[:1000]Xtr_rows = Xtr_rows[1000:, :] # keep last 49,000 for trainYtr = Ytr[1000:]# find hyperparameters that work best on the validation setvalidation_accuracies = []for k in [1, 3, 5, 10, 20, 50, 100]:# use a particular value of k and evaluation on validation data nn = NearestNeighbor() nn.train(Xtr_rows, Ytr) # here we assume a modified NearestNeighbor class that can take a k as input Yval_predict = nn.predict(Xval_rows, k = k) acc = np.mean(Yval_predict == Yval) print 'accuracy: %f' % (acc,)# keep track of what works on the validation set validation_accuracies.append((k, acc)) N-fold cross-validation5份交叉验证对k值调优的例子。将训练集平均分成5份，循环着取其中4份来训练，份来验证针,对每个k值，得到5个准确率结果，取其平均值。 Pros and Cons of Nearest Neighbor classifier Approximate Nearest Neighbor (ANN) algorithms and libraries exist that can accelerate the nearest neighbor lookup in a dataset (e.g. FLANN) 低维适用(图片一般高维，高维度向量之间的距离通常是反直觉的) 实际应用K-NN: Preprocess your data：对你数据中的特征进行归一化(normalize) Dimensionality reduction technique, such as PCA … Linear ClassificationkNN , disadvantages: The classifier must remember all of the training data and store it for future comparisons with test data. space inefficient Classifying a test image is expensive Parameterized mapping from images to label scoresCIFAR-10: N = 50,000 images, D = 32 x 32 x 3 pixels, K = 10 classess. score function: $f:R^D\\mapsto R^K$ Linear classifier A linear mapping: $f(x_i,W,b) = Wx_i + b$ $x_i$: 输入图像,flattened out to a single column vector of shape [D x 1].Matrix $W$(weight) of size [K x D], the vector $b$(bias vector) of size [K x 1]. NOTE: each Classifier is a row of $W$ Interpreting a linear classifierAnalogy of images as high-dimensional points 将图像看作高维空间的点 Since the images are stretched into high-dimensional column vectors, we can interpret each image as a single point in this space we defined the score of each class as a weighted sum of all image pixels, each class score is a linear function over this space. $W$ 的每一行都是一个分类类别的分类器。 Change one of the rows of $W$, the corresponding line in the pixel space will rotate in different directions.如果改变其中一行的数字，会看见分类器在空间中对应的直线开始向着不同方向旋转。 而偏差$b$，则允许分类器对应的translate the lines直线平移。如果没有偏差，所有分类器的线都不得不穿过原点。 Interpretation of linear classifiers as template matching Another interpretation for the weights $W$: each row of $W$ corresponds to a template (or sometimes also called a prototype) for one of the classes. using an inner product (or dot product) one by one to find the one that fits best. 可以认为还是在高效地使用 k-NN，不同的是我们没有使用所有的训练集的图像来比较，而是每个类别只用了一张图片（这张图片是我们学习到的，而不是训练集中的某一张），而且我们会使用（负）内积来计算向量间的距离，而不是使用L1或者L2距离。 Bias trick 合并 W 和 b，在 $x_i$ 末尾加 1 $$f(x_i,W,b) = Wx_i + b \\longrightarrow f(x_i,W) = Wx_i$$ image data preprocessing normalization range from [-1,1],zero mean centering Loss functionsometimes also referred to as cos function or the objective(目标函数) Multiclass Support Vector Machine lossThe Multiclass SVM loss for the i-th example is then formalized as follows:$$L_i = \\sum_{j\\neq y_i} max(0, s_j - s_{y_i} + \\Delta)$$ Example: a scores $s=[13,-7,11]$ and that the first class is the true, assume that $\\Delta$ is 10. The expression above sums over all incorrect classes ($ j\\neq y_j$), so we get two terms:$$L_i = max(0,-7-13+10) + max(0,11-13+10)$$ hinge loss: $max(0,-)$ ,sometimes use $max(0,-)^2$将更强烈（平方地而不是线性地）地惩罚过界的边界值。 Regularization: bug -&gt; loss 为0 ，M就可有无数个。为了避免这种情况，在loss中增加 regularization penalty,常用 L2 norm 来 discourages large weights.$$R(W) = \\sum_k\\sum_lW^2_{k,l}$$ the full Multiclass SVM loss becomes:$$L = \\underbrace{ \\frac{1}{N} \\sum_i L_i }_\\text{data loss} + \\underbrace{ \\lambda R(W) }_\\text{regularization loss} $$ Where $N$ is the number of training examples. this effect can improve the generalization performance of the classifiers on test images and lead to less overfitting L2惩罚倾向于更小更分散的W权重向量。 正则化的意义 不添加正则项, loss 会使 w 更拟合 training data，添加后， 更加拟合未知数据。 使拟合函数曲线更加趋向于低次表达， 越简单越正确。 程序解释 作业要求用微分分析方法（而不是数值分析法）来计算梯度，svm_loss_naive中允许使用循环 svm_loss_naive： 12345678910111213141516....num_classes = W.shape[1] num_train = X.shape[0] loss = 0.0 for i in xrange(num_train): scores = X[i].dot(W) correct_class_score = scores[y[i]] for j in xrange(num_classes): if j == y[i]: continue margin = scores[j] - correct_class_score + 1 # note delta = 1 if margin &gt; 0: loss += margin dW[:,j] += X[i].T dW[:,y[i]] += -X[i].T.... 其中，scores = X[i].dot(W)就是下面计算loss 中的 f 函数。$$L = \\frac{1}{N} \\sum_i \\sum_{j\\neq y_i} \\left[ \\max(0, f(x_i; W)_j - f(x_i; W)_{y_i} + \\Delta) \\right] + \\lambda \\sum_k\\sum_l W_{k,l}^2$$ 这样得出的scores是X[i]关于权重W（含偏置）的各个分类器的得分，是一个C维向量（假设共有C个分类）。 最后两行： 在j不等于y_i的时候，求偏导就是 d scores，为 X[i].T, 最后总式子的累加符号中可以看出j不等于y_i，在j等于y_i的时候,累加中只有减号后才有y_i项，求偏导时前面都是0，减号后项保留，得到结果 -X[i].T 。 既然W_yi的作用是要使得损失函数 L_i最小，那么就给 W_yi 加上若干个 Xi（这里为负号，但在SGD计算时采用负梯度方向，所以实际效果为相加），使得 Wyi的权重值变大，结果是新一轮迭代时损失函数应该变小。 svm_loss_vectorized: 123456789....num_train = X.shape[0] num_classes = W.shape[1] scores = X.dot(W) correct_class_scores = scores[range(num_train), list(y)].reshape(-1,1) #(N, 1) margins = np.maximum(0, scores - correct_class_scores +1) margins[range(num_train), list(y)] = 0 loss = np.sum(margins) / num_train + 0.5 * reg * np.sum(W * W).... 参照公式一目了然。 后面 0.5 有的说通常加入0.5是为了计算regulation loss的梯度时系数变为1。这个应该是看如何定义的。 Setting Delta can safely be set to $Δ=1.0$ in all cases. 它影响的是 w 的大小而不是 loss.the only real tradeoff is how large we allow the weights to grow (through the regularization strength $λ$). Softmax classifier$f(x_i,W) = Wx_i$ unchanged,interpret these scores as the unnormalized log probabilities(未归一化的对数概率) for each class and replace the hinge loss with a cross-entropy loss(交叉熵损失) that has the form:$$L_i = -\\log( {e^{f_{y_i}} \\over \\sum_je^{f_j} }) \\ \\ \\ \\ or \\ \\ \\ L_i = -f_{y_i} + \\log\\sum_je^{f_j}$$ $f_j$ to mean the j-th element of the vector of class scores $f$. $f_{y_i}$ to mean 把 i 分类为 $y_i$ 的 score. (j can = $y_j$ ? ) $L_i$ 控制在[0,1] Information theory view: The cross-entropy between a “true” distribution $p$ and an estimated distribution(预测分布) $q$ is defined as: $$H(p,q) = -\\sum{p(x) \\log q(x)}$$ 希望真实分布于预测分布的交叉熵最小，即差异最小。 Probabilistic interpretation:To see $$P(y_i | x_i; W) = { e^{f_{y_i}} \\over \\sum_j e^{f_j} }$$给定图像数据$x_i$，以$W$为参数，分配给正确分类标签$y_i$的 normalized probability (归一化概率)。 Practical issues: Numeric stability: 存在指数函数，所以数值可能非常大。除以大数值可能导致数值计算的不稳定。Notice that if we multiply the top and bottom of the fraction by a constant $C$ (常数C) ，通常取: $ \\log C = max_jf_j$ $$\\frac{e^{f_{y_i}}}{\\sum_j e^{f_j}}= \\frac{Ce^{f_{y_i}}}{C\\sum_j e^{f_j}}= \\frac{e^{f_{y_i} + \\log C}}{\\sum_j e^{f_j + \\log C}}$$ 这样指数就 &lt;= 0,分子分母都小于1。 名词误解: SVM分类器使用的是折叶损失（hinge loss），有时候又被称为最大边界损失（max-margin loss）。Softmax分类器使用的是交叉熵损失（corss-entropy loss）。Softmax分类器的命名是从softmax函数那里得来的，softmax函数将原始分类评分变成正的归一化数值，所有数值和为1，这样处理后交叉熵损失才能应用。注意从技术上说“softmax损失（softmax loss）”是没有意义的，因为softmax只是一个压缩数值的函数。但是在这个说法常常被用来做简称。 代码解释 softmax_loss_naive(W, X, y, reg): 12345678910111213141516...num_classes = W.shape[1]num_train = X.shape[0]for i in range(num_train): scores = X[i].dot(W) shift_scores = scores - max(scores) loss_i = - shift_scores[y[i]] + np.log(sum(np.exp(shift_scores))) loss += loss_i for j in range(num_classes): softmax_output = np.exp(shift_scores[j])/sum(np.exp(shift_scores)) if j == y[i]: dW[:,j] += (-1 + softmax_output) *X[i] else: dW[:,j] += softmax_output *X[i]... shift_scores 就是上面指出的 Numeric stability。 loss_i 是上面损失函数第二个式子。 softmax_output：链式法则$$\\frac{dloss} {dw} = \\frac{dloss}{df_j}.\\frac{df_j}{dw}$$后面一项为 X[i]，主要计算前面。$j = y_j$对分子求偏导:$$\\frac{\\partial loss}{\\partial f_j} = (-\\log( {e^{f_{y_i}} \\over \\sum_je^{f_j} }))’={e^{f_{y_i}} \\over \\sum_je^{f_j} }-1$$ $j \\neq y_j$对分母求偏导:$$\\frac{\\partial loss}{\\partial f_j} = (-\\log( {e^{f_{y_i}} \\over \\sum_je^{f_j} }))’={e^{f_{y_i}} \\over \\sum_je^{f_j} }$$ softmax_loss_vectorized(W, X, y, reg): 12345678...scores = X.dot(W)shift_scores = scores - np.max(scores, axis = 1).reshape(-1,1)softmax_output = np.exp(shift_scores)/np.sum(np.exp(shift_scores), axis = 1).reshape(-1,1)loss = -np.sum(np.log(softmax_output[range(num_train), list(y)]))loss /= num_trainloss += 0.5* reg * np.sum(W * W)... 参考loss function前面一个式子。 SVM vs. Softmax softmax分类器对于分数是永远不会满意的：正确分类总能得到更高的可能性，错误分类总能得到更低的可能性，损失值总是能够更小。但是，SVM只要边界值被满足了就满意了。 eg. SVM对于数字个体的细节是不关心的：如果分数是[10, -100, -100]或者[10, 9, 9]，对于SVM来说没设么不同，只要满足超过边界值等于1，那么损失值就等于0。 对于softmax分类器，情况则不同。对于[10, 9, 9]来说，计算出的损失值就远远高于[10, -100, -100]的。 Interactive web demo Unknown words simplicity 简单 nseemingly 貌似 advdistinct 清楚的 adjquarter 四分之一trivial 简单，不重要的 adj.instance 实例be oriented inexhibit variation 表现出差异extreme 极端的rigid 刚性，僵硬的occlude 挡住a portion of 一部分clutter 杂乱blend into 融入 annotate 注释 vt. catalog 登记成册 vtinterdisciplinary 跨学科 adj.constellation 一群 n.neuroscience 神经科学overlap 重叠部分 concurrently 同时 adv.encompassing 包含seminar 研讨会syllabus 教学大纲tag team 两人一组take over 接管 agenda 议程chill 寒冷 onset 开始proactive 积极主动 go after 追逐predators went after prey 捕食者 / 被捕食者intelligent animal 智慧动物 manipulate 操纵Renaissance period of time 文艺复兴期间obscuration 昏暗pinhole 小孔in the mean time 同时inspired 受到启发electrode 电极primary visual cortex 初级视觉皮层 by and large 大体上说 thesis 论文geometric 几何linear algebra 线性代数pay tribute to sb. 赞扬sb. holistic 全面的 adj. idealized 理想化的 adj. intuitive 直观的 adj.intuition 直觉deconstruct 解构 seminal 意义深远的 adj. cylinder 圆柱 generalized 广义的 adj. elastic 可伸缩的 adj.razor 剃须刀 frankly 坦白地 adv.audacious 大胆地 ambitious 雄心勃勃的 made some headway 有一些进展gain momentum 见效boosting 助推 There is a lot to admire this work. ?near-real-time 近实时的 roll out 推出occlusion 遮挡 intrinsic 本质的 adj.diagnostic 诊断的 pyramid 金字塔 clue 线索 vt/nhighway 公路 compose 组成histogram of gradient 梯度直方图 deformable 可变形的 adj. all along 自始至终 bottleneck 瓶颈 benchmarking 基准学习 gigantic 巨大的stringent 严格的 on par with 达到相当的水平 hover （数值）徘徊在，原义为翱翔deep dive 深入学习have a deep dive into convolutional neural networktremendous capacity 巨大的能力 academia 学术界 calory 卡路里image captioning 给图像加解说 tweaking 调整 collaborator 合作者deploy 部署 coarse 粗糙的transistor 晶体管orders magnitude 数量级crunching 运算 capacity 容量 fancy 设想pop up 突然出现,弹出 tackle 解决,处理 semantic 语义的perceptional 知觉的augmented reality 增强现实intricacy 错综复杂的事物 exposure 暴露 n. holy grail 圣杯 exemplify 举例说明 vt. conscious 有意识的respectable 可敬的compatriot 同胞 by no means 决不hallucinate 使产生幻觉psychedelic 迷幻的 render 渲染,给予extenuating 情有可原的 circumstance 环境 prerequisite 先决条件 calculus 微积分 derivative 导数reintroduce 恢复 discard 丢弃 penalty 惩罚","tags":[{"name":"笔记","slug":"笔记","permalink":"http://wenbo.fun/tags/笔记/"},{"name":"CNN","slug":"CNN","permalink":"http://wenbo.fun/tags/CNN/"}]},{"title":"Mean-shift clustering algorithm","date":"2018-03-30T08:49:18.000Z","path":"2018/03/30/Mean-shift/","text":"Mean-Shift算法Mean Shift向量的基本形式定义为: $$M_h = \\frac{1}{K} \\sum_{x_i \\in S_k}(x_i - x) $$$S_k$是一个半径为h的高维球区域,K表示在这n个样本点$x_i$中,有k个点落入$S_k$区域中. Meanshift向量是选取一个点为圆心，以 h 为半径做一个高维球，里面所有点与圆心为起点形成的向量相加的结果就是Meanshift向量 如图所以。其中黄色箭头就是 $M_h$（meanshift向量）。 再以meanshift向量的终点为圆心，再做一个高维的球。如下图所以，重复以上步骤，就可得到一个meanshift向量。如此重复下去，meanshift算法可以收敛到概率密度最大得地方。也就是最稠密的地方。 结果： Mean-Shift 聚类对于集合中的每一个元素，对它执行下面的操作：把该元素移动到它邻域中所有元素的特征值的均值的位置，不断重复直到收敛。准确的说，不是真正移动元素，而是把该元素与它的收敛位置的元素标记为同一类。 Mean-shift 流程Kernel density estimation is the most popular density estimation method. 在基本的mean shift向量加入核函数， 前面部分用核G计算x处的密度估计值为： 后面就是 mean shift 向量： 要使等式等于 0 ，那么 $m_{h,G}(x) = 0$，得到更新的圆心点： 具体流程： 选择空间中x为圆心，以h为半径为半径，做一个高维球。 对落在所有球内的所有点$x_i$,计算 $m_{h,G}(x)$，如果 $m_{h,G}(x)&lt;ε$ (人工设定)，退出程序。如果 &gt;ε, 则利用上式计算x，返回1. Meanshift的这种思想可以应用于目标跟踪、图像平滑、边缘检测、聚类等，是一种适应性很好的算法，缺点是速度非常慢. Generate isotropic Gaussian blobs for clustering 生成随机数据 12345import numpy as npfrom sklearn.datasets.samples_generator import make_blobscenters = [[2,1],[1,0],[-1,-1],[0,1]]X, y = make_blobs(n_samples = 3000, n_features = 2, centers = centers, cluster_std = 0.6, random_state = 0)print(X.shape) (3000, 2) 12345678910import matplotlib.pyplot as pltfrom itertools import cycleplt.figure(1)plt.clf()for i in range(X.shape[0]): plt.plot(X[i, 0], X[i, 1], 'bo')plt.title('Generate Data')plt.show() png 设置带宽 算法自动设定聚类的数目，取代依赖参数,聚类数目跟 bandwidth（带宽）有关,带宽是决定搜索区域的size的参数。 这个参数可以手动设置，可以使用评估函数 estimate_bandwidth 计算。 123456789101112131415161718192021from sklearn.utils import check_random_state, gen_batchesfrom sklearn.neighbors import NearestNeighborsdef estimate_bandwidth(X, n_samples, quantile = 0.5, n_jobs = 1, random_state = 0): random_state = check_random_state(random_state) if n_samples is not None: #permutation将序列打乱 并取n_samples个数的样本 idx = random_state.permutation(X.shape[0])[:n_samples] X = X[idx] nbrs = NearestNeighbors(n_neighbors = int(X.shape[0] * quantile), n_jobs = n_jobs) nbrs.fit(X) bandwidth = 0. #gen_batches(n,batch_size) 根据batch_size的大小生成0~n的切片 for batch in gen_batches(len(X), 500): #kneighbors返回batch里面每个点的n_sample个邻居的距离（不包括自己） #n_sample要是没有定义那就和NearestNeighbors里面的n_neighbors相等 #还有个返回值是下标，不过用不到就拿_忽略了 d, _ = nbrs.kneighbors(X[batch, :], return_distance=True) #将每个点的最近的n_neighbors个邻居中最远的距离加起来 bandwidth += np.max(d, axis=1).sum() return bandwidth/X.shape[0] 12bandwidth = estimate_bandwidth(X,quantile = 0.1, n_samples=500)bandwidth 0.76320309185897317 Mean-shift 迭代算法: L2 距离 1234567891011121314151617181920def _mean_shift_single_seed(my_mean, X, nbrs, max_iter): # For each seed, climb gradient until convergence or max_iter bandwidth = nbrs.get_params()['radius'] stop_thresh = 1e-3 * bandwidth # when mean has converged completed_iterations = 0 while True: # Find mean of points within bandwidth #radius_neighbors寻找my_mean周围的邻居 i_nbrs = nbrs.radius_neighbors([my_mean], bandwidth, return_distance=False)[0] points_within = X[i_nbrs] if len(points_within) == 0: break # Depending on seeding strategy this condition may occur my_old_mean = my_mean # save the old mean my_mean = np.mean(points_within, axis=0) # If converged or at max_iter, adds the cluster if (np.linalg.norm(my_mean - my_old_mean) &lt; stop_thresh or completed_iterations == max_iter): return tuple(my_mean), len(points_within) completed_iterations += 1 主程序 没有使用核函数seeds = X 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869from sklearn.externals.joblib import Parallel, delayeddef mean_shift(X, bandwidth=None, bin_seeding=False, min_bin_freq=1, cluster_all=True, max_iter=300, n_jobs= 1): if bandwidth is None: bandwidth = estimate_bandwidth(X, n_jobs=n_jobs) elif bandwidth &lt;= 0: raise ValueError(\"bandwidth needs to be greater than zero or None,\\ got %f\" % bandwidth) seeds = X n_samples,n_features = X.shape center_intensity_dict = &#123;&#125; nbrs = NearestNeighbors(radius=bandwidth, n_jobs= -1).fit(X) #all_res为所有种子的迭代完的中心以及周围的邻居数 all_res = Parallel(n_jobs=n_jobs)(delayed(_mean_shift_single_seed) (seed, X, nbrs, max_iter) for seed in seeds) # copy results in a dictionary for i in range(len(seeds)): if all_res[i] is not None: # 一个中心点为以周围邻居数为强度 center_intensity_dict[all_res[i][0]] = all_res[i][1] if not center_intensity_dict: # nothing near seeds raise ValueError(\"No point was within bandwidth=%f of any seed.\" \" Try a different seeding strategy \\ or increase the bandwidth.\" % bandwidth) #按照强度来排序 sorted_by_intensity = sorted(center_intensity_dict.items(), key=lambda tup: tup[1], reverse=True) #单独把排好序的点分出来 sorted_centers = np.array([tup[0] for tup in sorted_by_intensity]) unique = np.ones(len(sorted_centers), dtype=np.bool) #在这些点里再来一次找邻居 nbrs = NearestNeighbors(radius=bandwidth, n_jobs=n_jobs).fit(sorted_centers) for i, center in enumerate(sorted_centers): if unique[i]: neighbor_idxs = nbrs.radius_neighbors([center], return_distance=False)[0] unique[neighbor_idxs] = 0 unique[i] = 1 # leave the current point as unique cluster_centers = sorted_centers[unique] print(cluster_centers.shape) print(cluster_centers[0]) # ASSIGN LABELS: a point belongs to the cluster that it is closest to nbrs = NearestNeighbors(n_neighbors=1, n_jobs=n_jobs).fit(cluster_centers) labels = np.zeros(n_samples, dtype=np.int) distances, idxs = nbrs.kneighbors(X) if cluster_all: labels = idxs.flatten() else: labels.fill(-1) #距离小于bandwidth才能参与聚类 bool_selector = distances.flatten() &lt;= bandwidth labels[bool_selector] = idxs.flatten()[bool_selector] return cluster_centers, labels 1cluster_centers,labels = mean_shift(X,bandwidth = bandwidth) (3, 2) [ 1.25421279 0.42251172] 12345# 计算类数labels_unique = np.unique(labels)n_clusters = len(labels_unique)print('number of estimated clusters : %d' % n_clusters) number of estimated clusters : 3 聚类结果 1234567891011121314plt.figure(1)plt.clf()#cycle把一个序列无限重复下去colors = cycle('bgrcmykbgrcmykbgrcmykbgrcmyk')for k, col in zip(range(n_clusters), colors): my_members = labels == k cluster_center = cluster_centers[k] plt.plot(X[my_members, 0], X[my_members, 1], col + '.') plt.plot(cluster_center[0], cluster_center[1], 'o', markerfacecolor=col, markeredgecolor='k', markersize=14)plt.title('Estimated number of clusters: %d' % n_clusters)plt.show() png 该算法在执行算法期间需要执行多个最近邻搜索。该算法保证收敛，但是当质心的变化较小时，算法将停止迭代。","tags":[{"name":"算法","slug":"算法","permalink":"http://wenbo.fun/tags/算法/"},{"name":"聚类","slug":"聚类","permalink":"http://wenbo.fun/tags/聚类/"}]},{"title":"CS231n学习笔记 Module 1.2","date":"2018-03-22T09:15:17.000Z","path":"2018/03/22/cs231n_m1_2/","text":"Abstract Assignment1 Q2 SVM:http://7xsg2l.com1.z0.glb.clouddn.com/CS/svm.html 视频地址：https://www.youtube.com/watch?v=vT1JzLTH4G4&amp;index=2&amp;list=PLe7764SJVnV10-Nr7e0sBlC9J0LRf4sQo 课程作业：http://cs231n.github.io/ Syllabus:http://cs231n.stanford.edu/syllabus.html Note翻译:http://www.52ml.net/17723.html Module 1.2Lecture 2 | OptimizationOptimization is the process of finding the set of parameters W that minimize the loss function. Visualizing the loss function注意到loss function是不可微分的,由于max函数,导致函数中存在一定的不可导点(kinks).However, the subgradient still exists and is commonly used instead. OptimizationStrategy #1: A first very bad idea solution: Random search随机选择weights,然后看哪个loss低.Strategy #2: Random Local Search随机选择一个方向,loss下降,才继续往下走.Strategy #3: Following the Gradient梯度 1-D function $$\\frac{df(x)}{dx} = \\lim_{h\\ \\to 0} \\frac{f(x + h) - f(x)}{h}$$ 多维偏导数 partial derivatives Computing the gradient numerical gradient: slow, approximate but easy analytic gradient: fast, exact but more error-prone(容易出错) way that requires calculus Computing the gradient numerically with finite differences(有限差值)123456789101112131415161718192021222324252627def eval_numerical_gradient(f, x): \"\"\" 一个f在x的数值梯度简单实现 - f should be a function that takes a single argument - x is the point (numpy array) to evaluate the gradient at \"\"\" fx = f(x) # evaluate function value at original point grad = np.zeros(x.shape) h = 0.00001 # np.nditer原来是numpy array自带的迭代器 #flags=['multi_index']表示对a进行多重索引. #op_flags=['readwrite']表示不仅可以对a进行read（读取）,还可以write（写入）.即相当于在创建这个迭代器的时候，我们就规定好了有哪些权限。 it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite']) while not it.finished:# evaluate function at x+h#it.multi_index表示元素的索引 ix = it.multi_index old_value = x[ix] x[ix] = old_value + h # increment by h fxh = f(x) # evalute f(x + h) x[ix] = old_value # restore to previous value (very important!) # compute the partial derivative grad[ix] = (fxh - fx) / h # the slope #表示进入下一次迭代 it.iternext() # step to next dimension return grad h在数学意义上趋向于0,但是这里用一个极小值1e-5代替就可以了.better to compute the numeric gradient using the centered difference formula:$[f(x+h) - f(x-h)] / 2 h$ Update in negative gradient direction since we wish our loss function to decrease, not increase. Effect of step size The step size (or as we will later call it - the learning rate) 效率低 Computing the gradient analytically with Calculusgradient check：it is very common to compute the analytic gradient and compare it to the numerical gradient to check the correctness of your implementation. SVM loss function: $$L_i = \\sum_{j\\neq y_i} \\left[ \\max(0, w_j^Tx_i - w_{y_i}^Tx_i + \\Delta) \\right]$$ 对$w_{y_j}$微分,正确分类那一行:$$\\nabla_{w_{y_i}} L_i = - \\left( \\sum_{j\\neq y_i} \\mathbb{1}(w_j^Tx_i - w_{y_i}^Tx_i + \\Delta &gt; 0) \\right) x_i$$ ps： where $1$ is the indicator function: 括号内为真，值为1，假，0. other rows where $j \\neq y_i$ the gradient is: $$\\nabla_{w_j} L_i = \\mathbb{1}(w_j^Tx_i - w_{y_i}^Tx_i + \\Delta &gt; 0) x_i$$ Gradient Descentcore idea Mini-batch gradient descent 小批量容易收敛 12345# Vanilla Minibatch Gradient Descentwhile True: data_batch = sample_training_data(data, 256) # sample 256 examples weights_grad = evaluate_gradient(loss_fun, data_batch, weights) weights += - step_size * weights_grad # perform parameter update The extreme case batch_size=1 , 称为随机梯度下降(on-line gradient descent)or(Stochastic Gradient Descent (SGD)). Unknown wordsrecursive 递归的 extreme 极端的 duplicate 复制identical 相同的Throughout 在…期间，自始至终straight-forward 顺利的，直接的scary 可怕的differentiate 微分downside 负面，下降趋势shuffle 拖着脚步走increment 增加 calculus 微积分exact 精确的partial derivatives 偏导数instantaneous 瞬间 slope 斜率perturbation 扰乱iterative refinement 迭代优化eventually 最后quantify the quality of reiterate 重申interchangeably 可交替地terrain 地形 literature 文献convex function 凸函数clamp atfor instance 例如explicit 清楚的Concretely 具体地consistent with 符合，与…一致relatively 相对地intuition 直觉 plane 平面procedure 步骤piecewise-linear 分段线性的","tags":[{"name":"笔记","slug":"笔记","permalink":"http://wenbo.fun/tags/笔记/"},{"name":"CNN","slug":"CNN","permalink":"http://wenbo.fun/tags/CNN/"}]},{"title":"Python教程笔记(3/4)","date":"2018-03-09T15:24:08.000Z","path":"2018/03/09/myPython_b/","text":"IO编程本章的IO编程都是同步模式 文件读写Python内置了读写文件的函数，用法和C是兼容的。 读文件 要以读文件的模式打开一个文件对象，使用Python内置的open()函数，传入文件名和标示符： 1&gt;&gt;&gt; f = open('/Users/michael/test.txt', 'r') 标示符’r’表示读，这样，我们就成功地打开了一个文件。 如果文件不存在，open()函数就会抛出一个IOError的错误，并且给出错误码和详细的信息告诉你文件不存在： 1234&gt;&gt;&gt; f=open('/Users/michael/notfound.txt', 'r')Traceback (most recent call last): File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;FileNotFoundError: [Errno 2] No such file or directory: '/Users/michael/notfound.txt' 如果文件打开成功，接下来，调用read()方法可以一次读取文件的全部内容，Python把内容读到内存，用一个str对象表示： 12&gt;&gt;&gt; f.read()'Hello, world!' 最后一步是调用close()方法关闭文件。文件使用完毕后必须关闭，因为文件对象会占用操作系统的资源，并且操作系统同一时间能打开的文件数量也是有限的： 1&gt;&gt;&gt; f.close() 由于文件读写时都有可能产生IOError，一旦出错，后面的f.close()就不会调用。所以，为了保证无论是否出错都能正确地关闭文件，我们可以使用try ... finally来实现： 12345678910111213try: f = open('/path/to/file', 'r') print(f.read())finally: if f: f.close()``` 但是每次都这么写实在太繁琐，所以，Python引入了`with`语句来自动帮我们调用`close()`方法：```Pythonwith open('/path/to/file', 'r') as f: print(f.read()) 这和前面的try ... finally是一样的，但是代码更佳简洁，并且不必调用f.close()方法。 调用read()会一次性读取文件的全部内容，如果文件有10G，内存就爆了，所以，要保险起见，可以反复调用read(size)方法，每次最多读取size个字节的内容。另外，调用readline()可以每次读取一行内容，调用readlines()一次读取所有内容并按行返回list。因此，要根据需要决定怎么调用。 如果文件很小，read()一次性读取最方便；如果不能确定文件大小，反复调用read(size)比较保险；如果是配置文件，调用readlines()最方便： 12for line in f.readlines(): print(line.strip()) # 把末尾的'\\n'删掉 file-like Object像open()函数返回的这种有个read()方法的对象，在Python中统称为file-like Object。除了file外，还可以是内存的字节流，网络流，自定义流等等。file-like Object不要求从特定类继承，只要写个read()方法就行。 StringIO就是在内存中创建的file-like Object，常用作临时缓冲。 二进制文件 前面讲的默认都是读取文本文件，并且是UTF-8编码的文本文件。要读取二进制文件，比如图片、视频等等，用&#39;rb&#39;模式打开文件即可： 123&gt;&gt;&gt; f = open('/Users/michael/test.jpg', 'rb')&gt;&gt;&gt; f.read()b'\\xff\\xd8\\xff\\xe1\\x00\\x18Exif\\x00\\x00...' # 十六进制表示的字节 写文件 写文件和读文件是一样的，唯一区别是调用open()函数时，传入标识符&#39;w&#39;或者&#39;wb&#39;表示写文本文件或写二进制文件： 123&gt;&gt;&gt; f = open('/Users/michael/test.txt', 'w')&gt;&gt;&gt; f.write('Hello, world!')&gt;&gt;&gt; f.close() 你可以反复调用write()来写入文件，但是务必要调用f.close()来关闭文件。当我们写文件时，操作系统往往不会立刻把数据写入磁盘，而是放到内存缓存起来，空闲的时候再慢慢写入。只有调用close()方法时，操作系统才保证把没有写入的数据全部写入磁盘。忘记调用close()的后果是数据可能只写了一部分到磁盘，剩下的丢失了。所以，还是用with语句来得保险： 12with open('/Users/michael/test.txt', 'w') as f: f.write('Hello, world!') 要写入特定编码的文本文件，请给open()函数传入encoding参数，将字符串自动转换成指定编码。 细心的童鞋会发现，以&#39;w&#39;模式写入文件时，如果文件已存在，会直接覆盖（相当于删掉后新写入一个文件）。如果我们希望追加到文件末尾怎么办？可以传入&#39;a&#39;以追加（append）模式写入。 所有模式的定义及含义可以参考Python的官方文档。 StringIO和BytesIOStringIO 很多时候，数据读写不一定是文件，也可以在内存中读写。 StringIO顾名思义就是在内存中读写str。 要把str写入StringIO，我们需要先创建一个StringIO，然后，像文件一样写入即可： 12345678910&gt;&gt;&gt; from io import StringIO&gt;&gt;&gt; f = StringIO()&gt;&gt;&gt; f.write('hello')5&gt;&gt;&gt; f.write(' ')1&gt;&gt;&gt; f.write('world!')6&gt;&gt;&gt; print(f.getvalue())hello world! getvalue()方法用于获得写入后的str。 要读取StringIO，可以用一个str初始化StringIO，然后，像读文件一样读取： 1234567891011&gt;&gt;&gt; from io import StringIO&gt;&gt;&gt; f = StringIO('Hello!\\nHi!\\nGoodbye!')&gt;&gt;&gt; while True:... s = f.readline()... if s == '':... break... print(s.strip())...Hello!Hi!Goodbye! BytesIO StringIO操作的只能是str，如果要操作二进制数据，就需要使用BytesIO。BytesIO实现了在内存中读写bytes，我们创建一个BytesIO，然后写入一些bytes： 123456&gt;&gt;&gt; from io import BytesIO&gt;&gt;&gt; f = BytesIO()&gt;&gt;&gt; f.write('中文'.encode('utf-8'))6&gt;&gt;&gt; print(f.getvalue())b'\\xe4\\xb8\\xad\\xe6\\x96\\x87' 请注意，写入的不是str，而是经过UTF-8编码的bytes。 和StringIO类似，可以用一个bytes初始化BytesIO，然后，像读文件一样读取： 1234&gt;&gt;&gt; from io import BytesIO&gt;&gt;&gt; f = BytesIO(b'\\xe4\\xb8\\xad\\xe6\\x96\\x87')&gt;&gt;&gt; f.read()b'\\xe4\\xb8\\xad\\xe6\\x96\\x87' 操作文件和目录Python内置的os模块也可以直接调用操作系统提供的接口函数。 打开Python交互式命令行，我们来看看如何使用os模块的基本功能： 123&gt;&gt;&gt; import os&gt;&gt;&gt; os.name # 操作系统类型'nt' 如果是posix，说明系统是Linux、Unix或Mac OS X，如果是nt，就是Windows系统。 要获取详细的系统信息，可以调用uname()函数： 12&gt;&gt;&gt; os.uname()posix.uname_result(sysname='Darwin', nodename='MichaelMacPro.local', release='14.3.0', version='Darwin Kernel Version 14.3.0: Mon Mar 23 11:59:05 PDT 2015; root:xnu-2782.20.48~5/RELEASE_X86_64', machine='x86_64') 注意uname()函数在Windows上不提供，也就是说，os模块的某些函数是跟操作系统相关的。 环境变量 在操作系统中定义的环境变量，全部保存在os.environ这个变量中，可以直接查看： 12&gt;&gt;&gt; os.environenviron(&#123;'VERSIONER_PYTHON_PREFER_32_BIT': 'no', 'TERM_PROGRAM_VERSION': '326', 'LOGNAME': 'michael', 'USER': 'michael', 'PATH': '/usr/bin:/bin:/usr/sbin:/sbin:/usr/local/bin:/opt/X11/bin:/usr/local/mysql/bin', ...&#125;) 要获取某个环境变量的值，可以调用os.environ.get(&#39;key&#39;)： 1234&gt;&gt;&gt; os.environ.get('PATH')'/usr/bin:/bin:/usr/sbin:/sbin:/usr/local/bin:/opt/X11/bin:/usr/local/mysql/bin'&gt;&gt;&gt; os.environ.get('x', 'default')'default' 第二句是什么意思？ 操作文件和目录 操作文件和目录的函数一部分放在os模块中，一部分放在os.path模块中，这一点要注意一下。查看、创建和删除目录可以这么调用： 12345678910# 查看当前目录的绝对路径MAC:&gt;&gt;&gt; os.path.abspath('.')'/Users/michael'# 在某个目录下创建一个新目录，首先把新目录的完整路径表示出来:&gt;&gt;&gt; os.path.join('/Users/michael', 'testdir')'/Users/michael/testdir'# 创建一个目录:&gt;&gt;&gt; os.mkdir('/Users/michael/testdir')# 删掉一个目录:&gt;&gt;&gt; os.rmdir('/Users/michael/testdir') 把两个路径合成一个时，不要直接拼字符串，而要通过os.path.join()函数，这样可以正确处理不同操作系统的路径分隔符。在Linux/Unix/Mac下，os.path.join()返回这样的字符串： 1part-1/part-2 而Windows下会返回这样的字符串： 1part-1\\part-2 同样的道理，要拆分路径时，也不要直接去拆字符串，而要通过os.path.split()函数，这样可以把一个路径拆分为两部分，后一部分总是最后级别的目录或文件名： 12&gt;&gt;&gt; os.path.split('/Users/michael/testdir/file.txt')('/Users/michael/testdir', 'file.txt') os.path.splitext()可以直接让你得到文件扩展名，很多时候非常方便： 12&gt;&gt;&gt; os.path.splitext('/path/to/file.txt')('/path/to/file', '.txt') 这些合并、拆分路径的函数并不要求目录和文件要真实存在，它们只对字符串进行操作。 文件操作使用下面的函数。假定当前目录下有一个test.txt文件： 1234# 对文件重命名:&gt;&gt;&gt; os.rename('test.txt', 'test.py')# 删掉文件:&gt;&gt;&gt; os.remove('test.py') 但是复制文件的函数居然在os模块中不存在！原因是复制文件并非由操作系统提供的系统调用。理论上讲，我们通过上一节的读写文件可以完成文件复制，只不过要多写很多代码。 幸运的是shutil模块提供了copyfile()的函数，你还可以在shutil模块中找到很多实用函数，它们可以看做是os模块的补充。 最后看看如何利用Python的特性来过滤文件。比如我们要列出当前目录下的所有目录，只需要一行代码： 12&gt;&gt;&gt; [x for x in os.listdir('.') if os.path.isdir(x)]['.lein', '.local', '.m2', '.npm', '.ssh', '.Trash', '.vim', 'Applications', 'Desktop', ...] 要列出所有的.py文件，也只需一行代码： 123&gt;&gt;&gt; [x for x in os.listdir('.') if os.path.isfile(x) and os.path.splitext(x)[1]=='.py']['apis.py', 'config.py', 'models.py', 'pymonitor.py', 'test_db.py', 'urls.py', 'wsgiapp.py'] Python的os模块封装了操作系统的目录和文件操作，要注意这些函数有的在os模块中，有的在os.path模块中。 练习编写一个程序，能在当前目录以及当前目录的所有子目录下查找文件名包含指定字符串的文件，并打印出相对路径。 1234567891011121314151617def findFile(str, path='.'): for f in os.listdir(path): fPath = os.path.join(path, f) if os.path.isfile(fPath) and str in f: print(fPath) if os.path.isdir(fPath): findFile(str, fPath)``` ---## 序列化在程序运行的过程中，所有的变量都是在内存中，比如，定义一个`dict`：```pythond = dict(name='Bob', age=20, score=88) 可以随时修改变量，比如把name改成&#39;Bill&#39;，但是一旦程序结束，变量所占用的内存就被操作系统全部回收。如果没有把修改后的&#39;Bill&#39;存储到磁盘上，下次重新运行程序，变量又被初始化为&#39;Bob&#39;。 我们把变量从内存中变成可存储或传输的过程称之为 序列化，在Python中叫pickling，在其他语言中也被称之为serialization，marshalling，flattening等等，都是一个意思。 序列化之后，就可以把序列化后的内容写入磁盘，或者通过网络传输到别的机器上。反过来，把变量内容从序列化的对象重新读到内存里称之为反序列化，即unpickling。 Python提供了pickle模块来实现序列化。 首先，我们尝试把一个对象序列化并写入文件： 1234&gt;&gt;&gt; import pickle&gt;&gt;&gt; d = dict(name='Bob', age=20, score=88)&gt;&gt;&gt; pickle.dumps(d)b'\\x80\\x03&#125;q\\x00(X\\x03\\x00\\x00\\x00ageq\\x01K\\x14X\\x05\\x00\\x00\\x00scoreq\\x02KXX\\x04\\x00\\x00\\x00nameq\\x03X\\x03\\x00\\x00\\x00Bobq\\x04u.' pickle.dumps()方法把任意对象序列化成一个bytes，然后，就可以把这个bytes写入文件。或者用另一个方法pickle.dump()直接把对象序列化后写入一个file-like Object： 123&gt;&gt;&gt; f = open('dump.txt', 'wb')&gt;&gt;&gt; pickle.dump(d, f)&gt;&gt;&gt; f.close() 看看写入的dump.txt文件，一堆乱七八糟的内容，这些都是Python保存的对象内部信息。 当我们要把对象从磁盘读到内存时，可以先把内容读到一个bytes，然后用pickle.loads()方法反序列化出对象，也可以直接用pickle.load()方法从一个file-like Object中直接反序列化出对象。我们打开另一个Python命令行来反序列化刚才保存的对象： 12345&gt;&gt;&gt; f = open('dump.txt', 'rb')&gt;&gt;&gt; d = pickle.load(f)&gt;&gt;&gt; f.close()&gt;&gt;&gt; d&#123;'age': 20, 'score': 88, 'name': 'Bob'&#125; 变量的内容又回来了！ 当然，这个变量和原来的变量是完全不相干的对象，它们只是内容相同而已。 Pickle的问题和所有其他编程语言特有的序列化问题一样，就是它只能用于Python，并且可能不同版本的Python彼此都不兼容，因此，只能用Pickle保存那些不重要的数据，不能成功地反序列化也没关系。 JSON如果我们要在不同的编程语言之间传递对象，就必须把对象序列化为标准格式，比如XML，但更好的方法是序列化为JSON，因为JSON表示出来就是一个字符串，可以被所有语言读取，也可以方便地存储到磁盘或者通过网络传输。JSON不仅是标准格式，并且比XML更快，而且可以直接在Web页面中读取，非常方便。 JSON表示的对象就是标准的JavaScript语言的对象，JSON和Python内置的数据类型对应如下： JSON类型 Python类型 {} dict [] list “string” str 1234.56 int或float true/false True/False null None Python内置的json模块提供了非常完善的Python对象到JSON格式的转换。我们先看看如何把Python对象变成一个JSON： 1234&gt;&gt;&gt; import json&gt;&gt;&gt; d = dict(name='Bob', age=20, score=88)&gt;&gt;&gt; json.dumps(d)'&#123;\"age\": 20, \"score\": 88, \"name\": \"Bob\"&#125;' dumps()方法返回一个str，内容就是标准的JSON。类似的，dump()方法可以直接把JSON写入一个file-like Object。 要把JSON反序列化为Python对象，用loads()或者对应的load()方法，前者把JSON的字符串反序列化，后者从file-like Object中读取字符串并反序列化： 123&gt;&gt;&gt; json_str = '&#123;\"age\": 20, \"score\": 88, \"name\": \"Bob\"&#125;'&gt;&gt;&gt; json.loads(json_str)&#123;'age': 20, 'score': 88, 'name': 'Bob'&#125; 由于JSON标准规定JSON编码是UTF-8，所以我们总是能正确地在Python的str与JSON的字符串之间转换。 JSON进阶 Python的dict对象可以直接序列化为JSON的{}，不过，很多时候，我们更喜欢用class表示对象，比如定义Student类，然后序列化： 12345678910import jsonclass Student(object): def __init__(self, name, age, score): self.name = name self.age = age self.score = scores = Student('Bob', 20, 88)print(json.dumps(s)) 运行代码，毫不留情地得到一个TypeError： 123Traceback (most recent call last): ...TypeError: &lt;__main__.Student object at 0x10603cc50&gt; is not JSON serializable 错误的原因是Student对象不是一个可序列化为JSON的对象。如果连class的实例对象都无法序列化为JSON，这肯定不合理！ 别急，我们仔细看看dumps()方法的参数列表，可以发现，除了第一个必须的obj参数外，dumps()方法还提供了一大堆的可选参数： https://docs.python.org/3/library/json.html#json.dumps 这些可选参数就是让我们来定制JSON序列化。前面的代码之所以无法把Student类实例序列化为JSON，是因为默认情况下，dumps()方法不知道如何将Student实例变为一个JSON的{}对象。 可选参数default就是把任意一个对象变成一个可序列为JSON的对象，我们只需要为Student专门写一个转换函数，再把函数传进去即可： 123456def student2dict(std): return &#123; 'name': std.name, 'age': std.age, 'score': std.score &#125; 这样，Student实例首先被student2dict()函数转换成dict，然后再被顺利序列化为JSON： 12&gt;&gt;&gt; print(json.dumps(s, default=student2dict))&#123;\"age\": 20, \"name\": \"Bob\", \"score\": 88&#125; 不过，下次如果遇到一个Teacher类的实例，照样无法序列化为JSON。我们可以偷个懒，把任意class的实例变为dict： 1print(json.dumps(s, default=lambda obj: obj.__dict__)) 因为通常class的实例都有一个__dict__属性，它就是一个dict，用来存储实例变量。也有少数例外，比如定义了__slots__的class。 同样的道理，如果我们要把JSON反序列化为一个Student对象实例，loads()方法首先转换出一个dict对象，然后，我们传入的object_hook函数负责把dict转换为Student实例： 12def dict2student(d): return Student(d['name'], d['age'], d['score']) 运行结果如下： 123&gt;&gt;&gt; json_str = '&#123;\"age\": 20, \"score\": 88, \"name\": \"Bob\"&#125;'&gt;&gt;&gt; print(json.loads(json_str, object_hook=dict2student))&lt;__main__.Student object at 0x10cd3c190&gt; 打印出的是反序列化的Student实例对象。 进程和线程对于操作系统来说，一个任务就是一个进程（Process），比如打开一个浏览器就是启动一个浏览器进程，打开一个记事本就启动了一个记事本进程，打开两个记事本就启动了两个记事本进程，打开一个Word就启动了一个Word进程。 有些进程还不止同时干一件事，比如Word，它可以同时进行打字、拼写检查、打印等事情。在一个进程内部，要同时干多件事，就需要同时运行多个“子任务”，我们把进程内的这些“子任务”称为线程（Thread）。 多进程要让Python程序实现多进程（multiprocessing），我们先了解操作系统的相关知识。 Unix/Linux操作系统提供了一个fork()系统调用，它非常特殊。普通的函数调用，调用一次，返回一次，但是fork()调用一次，返回两次，因为操作系统自动把当前进程（称为父进程）复制了一份（称为子进程），然后，分别在父进程和子进程内返回。 子进程永远返回0，而父进程返回子进程的ID。这样做的理由是，一个父进程可以fork出很多子进程，所以，父进程要记下每个子进程的ID，而子进程只需要调用getppid()就可以拿到父进程的ID。 Python的os模块封装了常见的系统调用，其中就包括fork，可以在Python程序中轻松创建子进程： 123456789import osprint('Process (%s) start...' % os.getpid())# Only works on Unix/Linux/Mac:pid = os.fork()if pid == 0: print('I am child process (%s) and my parent is %s.' % (os.getpid(), os.getppid()))else: print('I (%s) just created a child process (%s).' % (os.getpid(), pid)) 运行结果如下： 123Process (876) start...I (876) just created a child process (877).I am child process (877) and my parent is 876. 由于Windows没有fork调用，上面的代码在Windows上无法运行。由于Mac系统是基于BSD（Unix的一种）内核，所以，在Mac下运行是没有问题的。 有了fork调用，一个进程在接到新任务时就可以复制出一个子进程来处理新任务，常见的Apache服务器就是由父进程监听端口，每当有新的http请求时，就fork出子进程来处理新的http请求。 multiprocessing 如果你打算编写多进程的服务程序，Unix/Linux无疑是正确的选择。由于Windows没有fork调用，难道在Windows上无法用Python编写多进程的程序？ 由于Python是跨平台的，自然也应该提供一个跨平台的多进程支持。multiprocessing模块就是跨平台版本的多进程模块。multiprocessing模块提供了一个Process类来代表一个进程对象，下面的例子演示了启动一个子进程并等待其结束： 1234567891011121314from multiprocessing import Processimport os# 子进程要执行的代码def run_proc(name): print('Run child process %s (%s)...' % (name, os.getpid()))if __name__=='__main__': print('Parent process %s.' % os.getpid()) p = Process(target=run_proc, args=('test',)) print('Child process will start.') p.start() p.join() print('Child process end.') 执行结果如下： 1234Parent process 928.Process will start.Run child process test (929)...Process end. 创建子进程时，只需要传入一个执行函数和函数的参数，创建一个Process实例， 用start()方法启动，这样创建进程比fork()还要简单。join()方法可以等待子进程结束后再继续往下运行，通常用于进程间的同步。 Pool 如果要启动大量的子进程，可以用进程池的方式批量创建子进程： 12345678910111213141516171819from multiprocessing import Poolimport os, time, randomdef long_time_task(name): print('Run task %s (%s)...' % (name, os.getpid())) start = time.time() time.sleep(random.random() * 3) end = time.time() print('Task %s runs %0.2f seconds.' % (name, (end - start)))if __name__=='__main__': print('Parent process %s.' % os.getpid()) p = Pool(4) for i in range(5): p.apply_async(long_time_task, args=(i,)) print('Waiting for all subprocesses done...') p.close() p.join() print('All subprocesses done.') 执行结果如下： 12345678910111213Parent process 669.Waiting for all subprocesses done...Run task 0 (671)...Run task 1 (672)...Run task 2 (673)...Run task 3 (674)...Task 2 runs 0.14 seconds.Run task 4 (673)...Task 1 runs 0.27 seconds.Task 3 runs 0.86 seconds.Task 0 runs 1.41 seconds.Task 4 runs 1.91 seconds.All subprocesses done. 代码解读： 对Pool对象调用join()方法会等待所有子进程执行完毕，调用join()之前必须先调用close()，调用close()之后就不能继续添加新的Process了。 请注意输出的结果，task 0，1，2，3是立刻执行的，而task 4要等待前面某个task完成后才执行，这是因为Pool的默认大小在我的电脑上是4，因此，最多同时执行4个进程。这是Pool有意设计的限制，并不是操作系统的限制。如果改成： 1p = Pool(5) 就可以同时跑5个进程。 由于Pool的默认大小是CPU的核数，如果你不幸拥有8核CPU，你要提交至少9个子进程才能看到上面的等待效果。 子进程很多时候，子进程并不是自身，而是一个外部进程。我们创建了子进程后，还需要控制子进程的输入和输出。 subprocess模块可以让我们非常方便地启动一个子进程，然后控制其输入和输出。 下面的例子演示了如何在Python代码中运行命令nslookup www.python.org，这和命令行直接运行的效果是一样的： 12345import subprocessprint('$ nslookup www.python.org')r = subprocess.call(['nslookup', 'www.python.org'])print('Exit code:', r) 运行结果： 12345678910$ nslookup www.python.orgServer: 192.168.19.4Address: 192.168.19.4#53Non-authoritative answer:www.python.org canonical name = python.map.fastly.net.Name: python.map.fastly.netAddress: 199.27.79.223Exit code: 0 如果子进程还需要输入，则可以通过communicate()方法输入： 1234567import subprocessprint('$ nslookup')p = subprocess.Popen(['nslookup'], stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE)output, err = p.communicate(b'set q=mx\\npython.org\\nexit\\n')print(output.decode('utf-8'))print('Exit code:', p.returncode) 上面的代码相当于在命令行执行命令nslookup，然后手动输入： 123set q=mxpython.orgexit 运行结果如下： 123456789101112$ nslookupServer: 192.168.19.4Address: 192.168.19.4#53Non-authoritative answer:python.org mail exchanger = 50 mail.python.org.Authoritative answers can be found from:mail.python.org internet address = 82.94.164.166mail.python.org has AAAA address 2001:888:2000:d::a6Exit code: 0 进程间通信Process之间肯定是需要通信的，操作系统提供了很多机制来实现进程间的通信。Python的multiprocessing模块包装了底层的机制，提供了Queue、Pipes等多种方式来交换数据。 我们以Queue为例，在父进程中创建两个子进程，一个往Queue里写数据，一个从Queue里读数据： 12345678910111213141516171819202122232425262728293031from multiprocessing import Process, Queueimport os, time, random# 写数据进程执行的代码:def write(q): print('Process to write: %s' % os.getpid()) for value in ['A', 'B', 'C']: print('Put %s to queue...' % value) q.put(value) time.sleep(random.random())# 读数据进程执行的代码:def read(q): print('Process to read: %s' % os.getpid()) while True: value = q.get(True) print('Get %s from queue.' % value)if __name__=='__main__': # 父进程创建Queue，并传给各个子进程： q = Queue() pw = Process(target=write, args=(q,)) pr = Process(target=read, args=(q,)) # 启动子进程pw，写入: pw.start() # 启动子进程pr，读取: pr.start() # 等待pw结束: pw.join() # pr进程里是死循环，无法等待其结束，只能强行终止: pr.terminate() 运行结果如下： 12345678Process to write: 50563Put A to queue...Process to read: 50564Get A from queue.Put B to queue...Get B from queue.Put C to queue...Get C from queue. 由于Windows没有fork调用，因此，multiprocessing需要“模拟”出fork的效果，父进程所有Python对象都必须通过pickle序列化再传到子进程去，所有，如果multiprocessing在Windows下调用失败了，要先考虑是不是pickle失败了。 多线程多任务可以由多进程完成，也可以由一个进程内的多线程完成。 由于线程是操作系统直接支持的执行单元，因此，高级语言通常都内置多线程的支持，Python也不例外，并且，Python的线程是真正的Posix Thread，而不是模拟出来的线程。 Python的标准库提供了两个模块：_thread和threading，_thread是低级模块，threading是高级模块，对_thread进行了封装。绝大多数情况下，我们只需要使用threading这个高级模块。 启动一个线程就是把一个函数传入并创建Thread实例，然后调用start()开始执行： 1234567891011121314151617import time, threading# 新线程执行的代码:def loop(): print('thread %s is running...' % threading.current_thread().name) n = 0 while n &lt; 5: n = n + 1 print('thread %s &gt;&gt;&gt; %s' % (threading.current_thread().name, n)) time.sleep(1) print('thread %s ended.' % threading.current_thread().name)print('thread %s is running...' % threading.current_thread().name)t = threading.Thread(target=loop, name='LoopThread')t.start()t.join()print('thread %s ended.' % threading.current_thread().name) 执行结果如下： 123456789thread MainThread is running...thread LoopThread is running...thread LoopThread &gt;&gt;&gt; 1thread LoopThread &gt;&gt;&gt; 2thread LoopThread &gt;&gt;&gt; 3thread LoopThread &gt;&gt;&gt; 4thread LoopThread &gt;&gt;&gt; 5thread LoopThread ended.thread MainThread ended. 由于任何进程默认就会启动一个线程，我们把该线程称为主线程，主线程又可以启动新的线程，Python的threading模块有个current_thread()函数，它永远返回当前线程的实例。主线程实例的名字叫MainThread，子线程的名字在创建时指定，我们用LoopThread命名子线程。名字仅仅在打印时用来显示，完全没有其他意义，如果不起名字Python就自动给线程命名为Thread-1，Thread-2…… Lock 多线程和多进程最大的不同在于，多进程中，同一个变量，各自有一份拷贝存在于每个进程中，互不影响，而多线程中，所有变量都由所有线程共享，所以，任何一个变量都可以被任何一个线程修改，因此，线程之间共享数据最大的危险在于多个线程同时改一个变量，把内容给改乱了。 来看看多个线程同时操作一个变量怎么把内容给改乱了： 12345678910111213141516171819202122import time, threading# 假定这是你的银行存款:balance = 0def change_it(n): # 先存后取，结果应该为0: global balance balance = balance + n balance = balance - ndef run_thread(n): for i in range(100000): change_it(n)t1 = threading.Thread(target=run_thread, args=(5,))t2 = threading.Thread(target=run_thread, args=(8,))t1.start()t2.start()t1.join()t2.join()print(balance) 我们定义了一个共享变量balance，初始值为0，并且启动两个线程，先存后取，理论上结果应该为0，但是，由于线程的调度是由操作系统决定的，当t1、t2交替执行时，只要循环次数足够多，balance的结果就不一定是0了。 原因是因为高级语言的一条语句在CPU执行时是若干条语句，即使一个简单的计算： 1balance = balance + n 也分两步： 计算balance + n，存入临时变量中； 将临时变量的值赋给balance。 也就是可以看成： 12x = balance + nbalance = x 由于x是局部变量，两个线程各自都有自己的x，当代码正常执行时： 12345678910111213初始值 balance = 0t1: x1 = balance + 5 # x1 = 0 + 5 = 5t1: balance = x1 # balance = 5t1: x1 = balance - 5 # x1 = 5 - 5 = 0t1: balance = x1 # balance = 0t2: x2 = balance + 8 # x2 = 0 + 8 = 8t2: balance = x2 # balance = 8t2: x2 = balance - 8 # x2 = 8 - 8 = 0t2: balance = x2 # balance = 0结果 balance = 0 但是t1和t2是交替运行的，如果操作系统以下面的顺序执行t1、t2： 123456789101112131415初始值 balance = 0t1: x1 = balance + 5 # x1 = 0 + 5 = 5t2: x2 = balance + 8 # x2 = 0 + 8 = 8t2: balance = x2 # balance = 8t1: balance = x1 # balance = 5t1: x1 = balance - 5 # x1 = 5 - 5 = 0t1: balance = x1 # balance = 0t2: x2 = balance - 8 # x2 = 0 - 8 = -8t2: balance = x2 # balance = -8结果 balance = -8 究其原因，是因为修改balance需要多条语句，而执行这几条语句时，线程可能中断，从而导致多个线程把同一个对象的内容改乱了。 如果我们要确保balance计算正确，就要给change_it()上一把锁，当某个线程开始执行change_it()时，我们说，该线程因为获得了锁，因此其他线程不能同时执行change_it()，只能等待，直到锁被释放后，获得该锁以后才能改。由于锁只有一个，无论多少线程，同一时刻最多只有一个线程持有该锁，所以，不会造成修改的冲突。创建一个锁就是通过threading.Lock()来实现： 12345678910111213balance = 0lock = threading.Lock()def run_thread(n): for i in range(100000): # 先要获取锁: lock.acquire() try: # 放心地改吧: change_it(n) finally: # 改完了一定要释放锁: lock.release() 当多个线程同时执行lock.acquire()时，只有一个线程能成功地获取锁，然后继续执行代码，其他线程就继续等待直到获得锁为止。 获得锁的线程用完后一定要释放锁，否则那些苦苦等待锁的线程将永远等待下去，成为死线程。所以我们用try...finally来确保锁一定会被释放。 锁的好处就是确保了某段关键代码只能由一个线程从头到尾完整地执行，坏处当然也很多，首先是阻止了多线程并发执行，包含锁的某段代码实际上只能以单线程模式执行，效率就大大地下降了。其次，由于可以存在多个锁，不同的线程持有不同的锁，并试图获取对方持有的锁时，可能会造成死锁，导致多个线程全部挂起，既不能执行，也无法结束，只能靠操作系统强制终止。 多核CPU 如果你不幸拥有一个多核CPU，你肯定在想，多核应该可以同时执行多个线程。如果写一个死循环的话，会出现什么情况呢？ 打开Mac OS X的Activity Monitor，或者Windows的Task Manager，都可以监控某个进程的CPU使用率。 我们可以监控到一个死循环线程会100%占用一个CPU。 如果有两个死循环线程，在多核CPU中，可以监控到会占用200%的CPU，也就是占用两个CPU核心。 要想把N核CPU的核心全部跑满，就必须启动N个死循环线程。 试试用Python写个死循环： 12345678910import threading, multiprocessingdef loop(): x = 0 while True: x = x ^ 1for i in range(multiprocessing.cpu_count()): t = threading.Thread(target=loop) t.start() 启动与CPU核心数量相同的N个线程，在4核CPU上可以监控到CPU占用率仅有102%，也就是仅使用了一核。 但是用C、C++或Java来改写相同的死循环，直接可以把全部核心跑满，4核就跑到400%，8核就跑到800%，为什么Python不行呢？ 因为Python的线程虽然是真正的线程，但解释器执行代码时，有一个GIL锁：Global Interpreter Lock，任何Python线程执行前，必须先获得GIL锁，然后，每执行100条字节码，解释器就自动释放GIL锁，让别的线程有机会执行。这个GIL全局锁实际上把所有线程的执行代码都给上了锁，所以，多线程在Python中只能交替执行，即使100个线程跑在100核CPU上，也只能用到1个核。 GIL是Python解释器设计的历史遗留问题，通常我们用的解释器是官方实现的CPython，要真正利用多核，除非重写一个不带GIL的解释器。 所以，在Python中，可以使用多线程，但不要指望能有效利用多核。如果一定要通过多线程利用多核，那只能通过C扩展来实现，不过这样就失去了Python简单易用的特点。 不过，也不用过于担心，Python虽然不能利用多线程实现多核任务，但可以通过多进程实现多核任务。多个Python进程有各自独立的GIL锁，互不影响。 ThreadLocal在多线程环境下，每个线程都有自己的数据。一个线程使用自己的局部变量比使用全局变量好，因为局部变量只有线程自己能看见，不会影响其他线程，而全局变量的修改必须加锁。 但是局部变量也有问题，就是在函数调用的时候，传递起来很麻烦： 12345678910111213def process_student(name): std = Student(name) # std是局部变量，但是每个函数都要用它，因此必须传进去： do_task_1(std) do_task_2(std)def do_task_1(std): do_subtask_1(std) do_subtask_2(std)def do_task_2(std): do_subtask_2(std) do_subtask_2(std) 每个函数一层一层调用都这么传参数那还得了？用全局变量？也不行，因为每个线程处理不同的Student对象，不能共享。 如果用一个全局dict存放所有的Student对象，然后以thread自身作为key获得线程对应的Student对象如何？ 123456789101112131415161718global_dict = &#123;&#125;def std_thread(name): std = Student(name) # 把std放到全局变量global_dict中： global_dict[threading.current_thread()] = std do_task_1() do_task_2()def do_task_1(): # 不传入std，而是根据当前线程查找： std = global_dict[threading.current_thread()] ...def do_task_2(): # 任何函数都可以查找出当前线程的std变量： std = global_dict[threading.current_thread()] ... 这种方式理论上是可行的，它最大的优点是消除了std对象在每层函数中的传递问题，但是，每个函数获取std的代码有点丑。 有没有更简单的方式？ ThreadLocal应运而生，不用查找dict，ThreadLocal帮你自动做这件事： 123456789101112131415161718192021import threading# 创建全局ThreadLocal对象:local_school = threading.local()def process_student(): # 获取当前线程关联的student: std = local_school.student print('Hello, %s (in %s)' % (std, threading.current_thread().name))def process_thread(name): # 绑定ThreadLocal的student: local_school.student = name process_student()t1 = threading.Thread(target= process_thread, args=('Alice',), name='Thread-A')t2 = threading.Thread(target= process_thread, args=('Bob',), name='Thread-B')t1.start()t2.start()t1.join()t2.join() 执行结果： 12Hello, Alice (in Thread-A)Hello, Bob (in Thread-B) 全局变量local_school就是一个ThreadLocal对象，每个Thread对它都可以读写student属性，但互不影响。你可以把local_school看成全局变量，但每个属性如local_school.student都是线程的局部变量，可以任意读写而互不干扰，也不用管理锁的问题，ThreadLocal内部会处理。可以理解为全局变量local_school是一个dict，不但可以用local_school.student，还可以绑定其他变量，如local_school.teacher等等。 ThreadLocal最常用的地方就是为每个线程绑定一个数据库连接，HTTP请求，用户身份信息等，这样一个线程的所有调用到的处理函数都可以非常方便地访问这些资源。 一个ThreadLocal变量虽然是全局变量，但每个线程都只能读写自己线程的独立副本，互不干扰。ThreadLocal解决了参数在一个线程中各个函数之间互相传递的问题。 进程 vs 线程我们介绍了多进程和多线程，这是实现多任务最常用的两种方式。现在，我们来讨论一下这两种方式的优缺点。 首先，要实现多任务，通常我们会设计Master-Worker模式，Master负责分配任务，Worker负责执行任务，因此，多任务环境下，通常是一个Master，多个Worker。 如果用多进程实现Master-Worker，主进程就是Master，其他进程就是Worker。如果用多线程实现Master-Worker，主线程就是Master，其他线程就是Worker。 多进程模式最大的优点就是稳定性高，因为一个子进程崩溃了，不会影响主进程和其他子进程。（当然主进程挂了所有进程就全挂了，但是Master进程只负责分配任务，挂掉的概率低）著名的Apache最早就是采用多进程模式。 多进程模式的缺点是创建进程的代价大，在Unix/Linux系统下，用fork调用还行，在Windows下创建进程开销巨大。另外，操作系统能同时运行的进程数也是有限的，在内存和CPU的限制下，如果有几千个进程同时运行，操作系统连调度都会成问题。 多线程模式通常比多进程快一点，但是也快不到哪去，而且，多线程模式致命的缺点就是任何一个线程挂掉都可能直接造成整个进程崩溃，因为所有线程共享进程的内存。在Windows上，如果一个线程执行的代码出了问题，你经常可以看到这样的提示：“该程序执行了非法操作，即将关闭”，其实往往是某个线程出了问题，但是操作系统会强制结束整个进程。 在Windows下，多线程的效率比多进程要高，所以微软的IIS服务器默认采用多线程模式。由于多线程存在稳定性的问题，IIS的稳定性就不如Apache。为了缓解这个问题，IIS和Apache现在又有多进程+多线程的混合模式，真是把问题越搞越复杂。 线程切换 无论是多进程还是多线程，只要数量一多，效率肯定上不去，为什么呢？ 我们打个比方，假设你不幸正在准备中考，每天晚上需要做语文、数学、英语、物理、化学这5科的作业，每项作业耗时1小时。 如果你先花1小时做语文作业，做完了，再花1小时做数学作业，这样，依次全部做完，一共花5小时，这种方式称为单任务模型，或者批处理任务模型。 假设你打算切换到多任务模型，可以先做1分钟语文，再切换到数学作业，做1分钟，再切换到英语，以此类推，只要切换速度足够快，这种方式就和单核CPU执行多任务是一样的了，以幼儿园小朋友的眼光来看，你就正在同时写5科作业。 但是，切换作业是有代价的，比如从语文切到数学，要先收拾桌子上的语文书本、钢笔（这叫保存现场），然后，打开数学课本、找出圆规直尺（这叫准备新环境），才能开始做数学作业。操作系统在切换进程或者线程时也是一样的，它需要先保存当前执行的现场环境（CPU寄存器状态、内存页等），然后，把新任务的执行环境准备好（恢复上次的寄存器状态，切换内存页等），才能开始执行。这个切换过程虽然很快，但是也需要耗费时间。如果有几千个任务同时进行，操作系统可能就主要忙着切换任务，根本没有多少时间去执行任务了，这种情况最常见的就是硬盘狂响，点窗口无反应，系统处于假死状态。 所以，多任务一旦多到一个限度，就会消耗掉系统所有的资源，结果效率急剧下降，所有任务都做不好。 计算密集型 vs IO密集型 是否采用多任务的第二个考虑是任务的类型。我们可以把任务分为计算密集型和IO密集型。 计算密集型任务的特点是要进行大量的计算，消耗CPU资源，比如计算圆周率、对视频进行高清解码等等，全靠CPU的运算能力。这种计算密集型任务虽然也可以用多任务完成，但是任务越多，花在任务切换的时间就越多，CPU执行任务的效率就越低，所以，要最高效地利用CPU，计算密集型任务同时进行的数量应当等于CPU的核心数。 计算密集型任务由于主要消耗CPU资源，因此，代码运行效率至关重要。Python这样的脚本语言运行效率很低，完全不适合计算密集型任务。对于计算密集型任务，最好用C语言编写。 第二种任务的类型是IO密集型，涉及到网络、磁盘IO的任务都是IO密集型任务，这类任务的特点是CPU消耗很少，任务的大部分时间都在等待IO操作完成（因为IO的速度远远低于CPU和内存的速度）。对于IO密集型任务，任务越多，CPU效率越高，但也有一个限度。常见的大部分任务都是IO密集型任务，比如Web应用。 IO密集型任务执行期间，99%的时间都花在IO上，花在CPU上的时间很少，因此，用运行速度极快的C语言替换用Python这样运行速度极低的脚本语言，完全无法提升运行效率。对于IO密集型任务，最合适的语言就是开发效率最高（代码量最少）的语言，脚本语言是首选，C语言最差。 异步IO 考虑到CPU和IO之间巨大的速度差异，一个任务在执行的过程中大部分时间都在等待IO操作，单进程单线程模型会导致别的任务无法并行执行，因此，我们才需要多进程模型或者多线程模型来支持多任务并发执行。 现代操作系统对IO操作已经做了巨大的改进，最大的特点就是支持异步IO。如果充分利用操作系统提供的异步IO支持，就可以用单进程单线程模型来执行多任务，这种全新的模型称为事件 驱动模型，Nginx就是支持异步IO的Web服务器，它在单核CPU上采用单进程模型就可以高效地支持多任务。在多核CPU上，可以运行多个进程（数量与CPU核心数相同），充分利用多核CPU。由于系统总的进程数量十分有限，因此操作系统调度非常高效。用异步IO编程模型来实现多任务是一个主要的趋势。 对应到Python语言，单线程的异步编程模型称为 协程，有了协程的支持，就可以基于事件驱动编写高效的多任务程序。我们会在后面讨论如何编写协程。 分布式进程在Thread和Process中，应当优选Process，因为Process更稳定，而且，Process可以分布到多台机器上，而Thread最多只能分布到同一台机器的多个CPU上。 Python的multiprocessing模块不但支持多进程，其中managers子模块还支持把多进程分布到多台机器上。一个服务进程可以作为调度者，将任务分布到其他多个进程中，依靠网络通信。由于managers模块封装很好，不必了解网络通信的细节，就可以很容易地编写分布式多进程程序。 举个例子：如果我们已经有一个通过Queue通信的多进程程序在同一台机器上运行，现在，由于处理任务的进程任务繁重，希望把发送任务的进程和处理任务的进程分布到两台机器上。怎么用分布式进程实现？ 原有的Queue可以继续使用，但是，通过managers模块把Queue通过网络暴露出去，就可以让其他机器的进程访问Queue了。 我们先看服务进程，服务进程负责启动Queue，把Queue注册到网络上，然后往Queue里面写入任务： 12345678910111213141516171819202122232425262728293031323334353637# task_master.pyimport random, time, queuefrom multiprocessing.managers import BaseManager# 发送任务的队列:task_queue = queue.Queue()# 接收结果的队列:result_queue = queue.Queue()# 从BaseManager继承的QueueManager:class QueueManager(BaseManager): pass# 把两个Queue都注册到网络上, callable参数关联了Queue对象:QueueManager.register('get_task_queue', callable=lambda: task_queue)QueueManager.register('get_result_queue', callable=lambda: result_queue)# 绑定端口5000, 设置验证码'abc':manager = QueueManager(address=('', 5000), authkey=b'abc')# 启动Queue:manager.start()# 获得通过网络访问的Queue对象:task = manager.get_task_queue()result = manager.get_result_queue()# 放几个任务进去:for i in range(10): n = random.randint(0, 10000) print('Put task %d...' % n) task.put(n)# 从result队列读取结果:print('Try get results...')for i in range(10): r = result.get(timeout=10) print('Result: %s' % r)# 关闭:manager.shutdown()print('master exit.') 请注意，当我们在一台机器上写多进程程序时，创建的Queue可以直接拿来用，但是，在分布式多进程环境下，添加任务到Queue不可以直接对原始的task_queue进行操作，那样就绕过了QueueManager的封装，必须通过manager.get_task_queue()获得的Queue接口添加。 然后，在另一台机器上启动任务进程（本机上启动也可以）： 1234567891011121314151617181920212223242526272829303132333435# task_worker.pyimport time, sys, queuefrom multiprocessing.managers import BaseManager# 创建类似的QueueManager:class QueueManager(BaseManager): pass# 由于这个QueueManager只从网络上获取Queue，所以注册时只提供名字:QueueManager.register('get_task_queue')QueueManager.register('get_result_queue')# 连接到服务器，也就是运行task_master.py的机器:server_addr = '127.0.0.1'print('Connect to server %s...' % server_addr)# 端口和验证码注意保持与task_master.py设置的完全一致:m = QueueManager(address=(server_addr, 5000), authkey=b'abc')# 从网络连接:m.connect()# 获取Queue的对象:task = m.get_task_queue()result = m.get_result_queue()# 从task队列取任务,并把结果写入result队列:for i in range(10): try: n = task.get(timeout=1) print('run task %d * %d...' % (n, n)) r = '%d * %d = %d' % (n, n, n*n) time.sleep(1) result.put(r) except Queue.Empty: print('task queue is empty.')# 处理结束:print('worker exit.') 任务进程要通过网络连接到服务进程，所以要指定服务进程的IP。现在，可以试试分布式进程的工作效果了。先启动task_master.py服务进程： 123456789101112$ python3 task_master.pyPut task 3411...Put task 1605...Put task 1398...Put task 4729...Put task 5300...Put task 7471...Put task 68...Put task 4219...Put task 339...Put task 7866...Try get results... task_master.py进程发送完任务后，开始等待result队列的结果。现在启动task_worker.py进程： 12345678910111213$ python3 task_worker.pyConnect to server 127.0.0.1...run task 3411 * 3411...run task 1605 * 1605...run task 1398 * 1398...run task 4729 * 4729...run task 5300 * 5300...run task 7471 * 7471...run task 68 * 68...run task 4219 * 4219...run task 339 * 339...run task 7866 * 7866...worker exit. task_worker.py进程结束，在task_master.py进程中会继续打印出结果： 12345678910Result: 3411 * 3411 = 11634921Result: 1605 * 1605 = 2576025Result: 1398 * 1398 = 1954404Result: 4729 * 4729 = 22363441Result: 5300 * 5300 = 28090000Result: 7471 * 7471 = 55815841Result: 68 * 68 = 4624Result: 4219 * 4219 = 17799961Result: 339 * 339 = 114921Result: 7866 * 7866 = 61873956 这个简单的Master/Worker模型有什么用？其实这就是一个简单但真正的分布式计算，把代码稍加改造，启动多个worker，就可以把任务分布到几台甚至几十台机器上，比如把计算n*n的代码换成发送邮件，就实现了邮件队列的异步发送。 Queue对象存储在哪？注意到task_worker.py中根本没有创建Queue的代码，所以，Queue对象存储在task_master.py进程中： 123456789101112131415161718┌─────────────────────────────────────────┐ ┌──────────────────────────────────────┐│task_master.py │ │ │task_worker.py ││ │ │ ││ task = manager.get_task_queue() │ │ │ task = manager.get_task_queue() ││ result = manager.get_result_queue() │ │ result = manager.get_result_queue() ││ │ │ │ │ │ ││ │ │ │ │ ││ ▼ │ │ │ │ ││ ┌─────────────────────────────────┐ │ │ │ ││ │QueueManager │ │ │ │ │ ││ │ ┌────────────┐ ┌──────────────┐ │ │ │ │ ││ │ │ task_queue │ │ result_queue │ │&lt;───┼──┼──┼──────────────┘ ││ │ └────────────┘ └──────────────┘ │ │ │ ││ └─────────────────────────────────┘ │ │ │ │└─────────────────────────────────────────┘ └──────────────────────────────────────┘ │ Network 而Queue之所以能通过网络访问，就是通过QueueManager实现的。由于QueueManager管理的不止一个Queue，所以，要给每个Queue的网络调用接口起个名字，比如get_task_queue。 authkey有什么用？这是为了保证两台机器正常通信，不被其他机器恶意干扰。如果task_worker.py的authkey和task_master.py的authkey不一致，肯定连接不上。 注意Queue的作用是用来传递任务和接收结果，每个任务的描述数据量要尽量小。比如发送一个处理日志文件的任务，就不要发送几百兆的日志文件本身，而是发送日志文件存放的完整路径，由Worker进程再去共享的磁盘上读取文件。 正则表达式正则表达式是一种用来匹配字符串的强有力的武器。它的设计思想是用一种描述性的语言来给字符串定义一个规则，凡是符合规则的字符串，我们就认为它“匹配”了，否则，该字符串就是不合法的。 要匹配变长的字符，在正则表达式中，用*表示任意个字符（包括0个），用+表示至少一个字符，用?表示0个或1个字符，用{n}表示n个字符，用{n,m}表示n-m个字符,来看一个复杂的例子： \\d{3}\\s+\\d{3,8} 。 从左到右解读一下： \\d{3}表示匹配3个数字，例如’010’； \\s可以匹配一个空格（也包括Tab等空白符），所以\\s+表示至少有一个空格，例如匹配&#39; &#39;等； \\d{3,8}表示3-8个数字，例如1234567。 综合起来，上面的正则表达式可以匹配以任意个空格隔开的带区号的电话号码。 如果要匹配’010-12345’这样的号码呢？由于’-‘是特殊字符，在正则表达式中，要用’\\’转义，所以，上面的正则是\\d{3}\\-\\d{3,8},但是，仍然无法匹配’010 - 12345’，因为带有空格。所以我们需要更复杂的匹配方式。 进阶 要做更精确地匹配，可以用[]表示范围，比如： [0-9a-zA-Z\\_]可以匹配一个数字、字母或者下划线； [0-9a-zA-Z\\_]+可以匹配至少由一个数字、字母或者下划线组成的字符串，比如’a100’，’0_Z’，’Py3000’等等； [a-zA-Z\\_][0-9a-zA-Z\\_]*可以匹配由字母或下划线开头，后接任意个由一个数字、字母或者下划线组成的字符串，也就是Python合法的变量； [a-zA-Z\\_][0-9a-zA-Z\\_]{0, 19}更精确地限制了变量的长度是1-20个字符（前面1个字符+后面最多19个字符）。 A|B可以匹配A或B，所以(P|p)ython可以匹配’Python’或者’python’。 ^表示行的开头，^\\d表示必须以数字开头。 $表示行的结束，\\d$表示必须以数字结束。 re模块 有了准备知识，我们就可以在Python中使用正则表达式了。Python提供re模块，包含所有正则表达式的功能。由于Python的字符串本身也用\\转义，所以要特别注意： 123s = 'ABC\\\\-001' # Python的字符串# 对应的正则表达式字符串变成：# 'ABC\\-001' 因此我们强烈建议使用Python的r前缀，就不用考虑转义的问题了： 123s = r'ABC\\-001' # Python的字符串# 对应的正则表达式字符串不变：# 'ABC\\-001' 先看看如何判断正则表达式是否匹配： 12345&gt;&gt;&gt; import re&gt;&gt;&gt; re.match(r'^\\d&#123;3&#125;\\-\\d&#123;3,8&#125;$', '010-12345')&lt;_sre.SRE_Match object; span=(0, 9), match='010-12345'&gt;&gt;&gt;&gt; re.match(r'^\\d&#123;3&#125;\\-\\d&#123;3,8&#125;$', '010 12345')&gt;&gt;&gt; match()方法判断是否匹配，如果匹配成功，返回一个Match对象，否则返回None。常见的判断方法就是： 12345test = '用户输入的字符串'if re.match(r'正则表达式', test): print('ok')else: print('failed') 切分字符串 用正则表达式切分字符串比用固定的字符更灵活，请看正常的切分代码： 12&gt;&gt;&gt; 'a b c'.split(' ')['a', 'b', '', '', 'c'] 嗯，无法识别连续的空格，用正则表达式试试： 12&gt;&gt;&gt; re.split(r'\\s+', 'a b c')['a', 'b', 'c'] 无论多少个空格都可以正常分割。加入,试试： 12&gt;&gt;&gt; re.split(r'[\\s\\,]+', 'a,b, c d')['a', 'b', 'c', 'd'] 再加入;试试： 12&gt;&gt;&gt; re.split(r'[\\s\\,\\;]+', 'a,b;; c d')['a', 'b', 'c', 'd'] 如果用户输入了一组标签，下次记得用正则表达式来把不规范的输入转化成正确的数组。 分组 除了简单地判断是否匹配之外，正则表达式还有提取子串的强大功能。用()表示的就是要提取的分组（Group）。比如： ^(\\d{3})-(\\d{3,8})$分别定义了两个组，可以直接从匹配的字符串中提取出区号和本地号码： 123456789&gt;&gt;&gt; m = re.match(r'^(\\d&#123;3&#125;)-(\\d&#123;3,8&#125;)$', '010-12345')&gt;&gt;&gt; m&lt;_sre.SRE_Match object; span=(0, 9), match='010-12345'&gt;&gt;&gt;&gt; m.group(0)'010-12345'&gt;&gt;&gt; m.group(1)'010'&gt;&gt;&gt; m.group(2)'12345' 如果正则表达式中定义了组，就可以在Match对象上用group()方法提取出子串来。注意到group(0)永远是原始字符串，group(1)、group(2)……表示第1、2、……个子串。 提取子串非常有用。来看一个更凶残的例子： 1234&gt;&gt;&gt; t = '19:05:30'&gt;&gt;&gt; m = re.match(r'^(0[0-9]|1[0-9]|2[0-3]|[0-9])\\:(0[0-9]|1[0-9]|2[0-9]|3[0-9]|4[0-9]|5[0-9]|[0-9])\\:(0[0-9]|1[0-9]|2[0-9]|3[0-9]|4[0-9]|5[0-9]|[0-9])$', t)&gt;&gt;&gt; m.groups()('19', '05', '30') 这个正则表达式可以直接识别合法的时间。但是有些时候，用正则表达式也无法做到完全验证，比如识别日期： 1'^(0[1-9]|1[0-2]|[0-9])-(0[1-9]|1[0-9]|2[0-9]|3[0-1]|[0-9])$' 对于&#39;2-30&#39;，&#39;4-31&#39;这样的非法日期，用正则还是识别不了，或者说写出来非常困难，这时就需要程序配合识别了。 贪婪匹配 最后需要特别指出的是，正则匹配默认是贪婪匹配，也就是匹配尽可能多的字符。举例如下，匹配出数字后面的0： 12&gt;&gt;&gt; re.match(r'^(\\d+)(0*)$', '102300').groups()('102300', '') 由于\\d+采用贪婪匹配，直接把后面的0全部匹配了，结果0*只能匹配空字符串了。 必须让\\d+采用非贪婪匹配（也就是尽可能少匹配），才能把后面的0匹配出来，加个?就可以让\\d+采用非贪婪匹配： 12&gt;&gt;&gt; re.match(r'^(\\d+?)(0*)$', '102300').groups()('1023', '00') 编译 当我们在Python中使用正则表达式时，re模块内部会干两件事情： 编译正则表达式，如果正则表达式的字符串本身不合法，会报错； 用编译后的正则表达式去匹配字符串。 如果一个正则表达式要重复使用几千次，出于效率的考虑，我们可以预编译该正则表达式，接下来重复使用时就不需要编译这个步骤了，直接匹配： 12345678&gt;&gt;&gt; import re# 编译:&gt;&gt;&gt; re_telephone = re.compile(r'^(\\d&#123;3&#125;)-(\\d&#123;3,8&#125;)$')# 使用：&gt;&gt;&gt; re_telephone.match('010-12345').groups()('010', '12345')&gt;&gt;&gt; re_telephone.match('010-8086').groups()('010', '8086') 编译后生成Regular Expression对象，由于该对象自己包含了正则表达式，所以调用对应的方法时不用给出正则字符串。 常用内建模块Python之所以自称batteries included，就是因为内置了许多非常有用的模块，无需额外安装和配置，即可直接使用。 datetimedatetime是Python处理日期和时间的标准库。 获取当前日期和时间 我们先看如何获取当前日期和时间： 123456&gt;&gt;&gt; from datetime import datetime&gt;&gt;&gt; now = datetime.now() # 获取当前datetime&gt;&gt;&gt; print(now)2020-05-18 16:28:07.198690&gt;&gt;&gt; print(type(now))&lt;class 'datetime.datetime'&gt; 注意到datetime是模块，datetime模块还包含一个datetime类，通过from datetime import datetime导入的才是datetime这个类。 如果仅导入import datetime，则必须引用全名datetime.datetime。 datetime.now()返回当前日期和时间，其类型是datetime。 获取指定日期和时间 要指定某个日期和时间，我们直接用参数构造一个datetime： 1234&gt;&gt;&gt; from datetime import datetime&gt;&gt;&gt; dt = datetime(2015, 4, 19, 12, 20) # 用指定日期时间创建datetime&gt;&gt;&gt; print(dt)2015-04-19 12:20:00 datetime转换为timestamp 在计算机中，时间实际上是用数字表示的。我们把1970年1月1日 00:00:00 UTC+00:00 时区的时刻称为epoch time，记为0（1970年以前的时间timestamp为负数），当前时间就是相对于epoch time的秒数，称为timestamp。 你可以认为： 1timestamp = 0 = 1970-1-1 00:00:00 UTC+0:00 对应的北京时间是： 1timestamp = 0 = 1970-1-1 08:00:00 UTC+8:00 可见timestamp的值与时区毫无关系，因为timestamp一旦确定，其UTC时间就确定了，转换到任意时区的时间也是完全确定的，这就是为什么计算机存储的当前时间是以timestamp表示的，因为全球各地的计算机在任意时刻的timestamp都是完全相同的（假定时间已校准）。 把一个datetime类型转换为timestamp只需要简单调用timestamp()方法： 1234&gt;&gt;&gt; from datetime import datetime&gt;&gt;&gt; dt = datetime(2015, 4, 19, 12, 20) # 用指定日期时间创建datetime&gt;&gt;&gt; dt.timestamp() # 把datetime转换为timestamp1429417200.0 注意Python的timestamp是一个浮点数。如果有小数位，小数位表示毫秒数。 某些编程语言（如Java和JavaScript）的timestamp使用整数表示毫秒数，这种情况下只需要把timestamp除以1000就得到Python的浮点表示方法。 timestamp转换为datetime 要把timestamp转换为datetime，使用datetime提供的fromtimestamp()方法： 1234&gt;&gt;&gt; from datetime import datetime&gt;&gt;&gt; t = 1429417200.0&gt;&gt;&gt; print(datetime.fromtimestamp(t))2015-04-19 12:20:00 注意到timestamp是一个浮点数，它没有时区的概念，而datetime是有时区的。上述转换是在timestamp和本地时间做转换。 timestamp也可以直接被转换到UTC标准时区的时间： 123456&gt;&gt;&gt; from datetime import datetime&gt;&gt;&gt; t = 1429417200.0&gt;&gt;&gt; print(datetime.fromtimestamp(t)) # 本地时间2015-04-19 12:20:00&gt;&gt;&gt; print(datetime.utcfromtimestamp(t)) # UTC时间2015-04-19 04:20:00 str转换为datetime 很多时候，用户输入的日期和时间是字符串，要处理日期和时间，首先必须把str转换为datetime。转换方法是通过datetime.strptime()实现，需要一个日期和时间的格式化字符串： 1234&gt;&gt;&gt; from datetime import datetime&gt;&gt;&gt; cday = datetime.strptime('2015-6-1 18:19:59', '%Y-%m-%d %H:%M:%S')&gt;&gt;&gt; print(cday)2015-06-01 18:19:59 字符串&#39;%Y-%m-%d %H:%M:%S&#39;规定了日期和时间部分的格式。详细的说明请参考Python文档。注意转换后的datetime是没有时区信息的。 datetime转换为str 如果已经有了datetime对象，要把它格式化为字符串显示给用户，就需要转换为str，转换方法是通过strftime()实现的，同样需要一个日期和时间的格式化字符串： 1234&gt;&gt;&gt; from datetime import datetime&gt;&gt;&gt; now = datetime.now()&gt;&gt;&gt; print(now.strftime('%a, %b %d %H:%M'))Mon, May 05 16:28 datetime加减 对日期和时间进行加减实际上就是把datetime往后或往前计算，得到新的datetime。加减可以直接用+和-运算符，不过需要导入timedelta这个类： 12345678910&gt;&gt;&gt; from datetime import datetime, timedelta&gt;&gt;&gt; now = datetime.now()&gt;&gt;&gt; nowdatetime.datetime(2015, 5, 18, 16, 57, 3, 540997)&gt;&gt;&gt; now + timedelta(hours=10)datetime.datetime(2015, 5, 19, 2, 57, 3, 540997)&gt;&gt;&gt; now - timedelta(days=1)datetime.datetime(2015, 5, 17, 16, 57, 3, 540997)&gt;&gt;&gt; now + timedelta(days=2, hours=12)datetime.datetime(2015, 5, 21, 4, 57, 3, 540997) 可见，使用timedelta你可以很容易地算出前几天和后几天的时刻。 本地时间转换为UTC时间 本地时间是指系统设定时区的时间，例如北京时间是UTC+8:00时区的时间，而UTC时间指UTC+0:00时区的时间。 一个datetime类型有一个时区属性tzinfo，但是默认为None，所以无法区分这个datetime到底是哪个时区，除非强行给datetime设置一个时区： 12345678&gt;&gt;&gt; from datetime import datetime, timedelta, timezone&gt;&gt;&gt; tz_utc_8 = timezone(timedelta(hours=8)) # 创建时区UTC+8:00&gt;&gt;&gt; now = datetime.now()&gt;&gt;&gt; nowdatetime.datetime(2015, 5, 18, 17, 2, 10, 871012)&gt;&gt;&gt; dt = now.replace(tzinfo=tz_utc_8) # 强制设置为UTC+8:00&gt;&gt;&gt; dtdatetime.datetime(2015, 5, 18, 17, 2, 10, 871012, tzinfo=datetime.timezone(datetime.timedelta(0, 28800))) 如果系统时区恰好是UTC+8:00，那么上述代码就是正确的，否则，不能强制设置为UTC+8:00时区。 时区转换 我们可以先通过utcnow()拿到当前的UTC时间，再转换为任意时区的时间： 12345678910111213141516# 拿到UTC时间，并强制设置时区为UTC+0:00:&gt;&gt;&gt; utc_dt = datetime.utcnow().replace(tzinfo=timezone.utc)&gt;&gt;&gt; print(utc_dt)2015-05-18 09:05:12.377316+00:00# astimezone()将转换时区为北京时间:&gt;&gt;&gt; bj_dt = utc_dt.astimezone(timezone(timedelta(hours=8)))&gt;&gt;&gt; print(bj_dt)2015-05-18 17:05:12.377316+08:00# astimezone()将转换时区为东京时间:&gt;&gt;&gt; tokyo_dt = utc_dt.astimezone(timezone(timedelta(hours=9)))&gt;&gt;&gt; print(tokyo_dt)2015-05-18 18:05:12.377316+09:00# astimezone()将bj_dt转换时区为东京时间:&gt;&gt;&gt; tokyo_dt2 = bj_dt.astimezone(timezone(timedelta(hours=9)))&gt;&gt;&gt; print(tokyo_dt2)2015-05-18 18:05:12.377316+09:00 时区转换的关键在于，拿到一个datetime时，要获知其正确的时区，然后强制设置时区，作为基准时间。 利用带时区的datetime，通过astimezone()方法，可以转换到任意时区。 注：不是必须从UTC+0:00时区转换到其他时区，任何带时区的datetime都可以正确转换，例如上述bj_dt到tokyo_dt的转换。 collectionscollections是Python内建的一个集合模块，提供了许多有用的集合类。 namedtuple 我们知道tuple可以表示不变集合，例如，一个点的二维坐标就可以表示成： 1&gt;&gt;&gt; p = (1, 2) 但是，看到(1, 2)，很难看出这个tuple是用来表示一个坐标的。 定义一个class又小题大做了，这时，namedtuple就派上了用场： 1234567&gt;&gt;&gt; from collections import namedtuple&gt;&gt;&gt; Point = namedtuple('Point', ['x', 'y'])&gt;&gt;&gt; p = Point(1, 2)&gt;&gt;&gt; p.x1&gt;&gt;&gt; p.y2 namedtuple是一个函数，它用来创建一个自定义的tuple对象，并且规定了tuple元素的个数，并可以用属性而不是索引来引用tuple的某个元素。这样一来，我们用namedtuple可以很方便地定义一种数据类型，它具备tuple的不变性，又可以根据属性来引用，使用十分方便。 可以验证创建的Point对象是tuple的一种子类： 1234&gt;&gt;&gt; isinstance(p, Point)True&gt;&gt;&gt; isinstance(p, tuple)True 类似的，如果要用坐标和半径表示一个圆，也可以用namedtuple定义： 12# namedtuple('名称', [属性list]):Circle = namedtuple('Circle', ['x', 'y', 'r']) deque 使用list存储数据时，按索引访问元素很快，但是插入和删除元素就很慢了，因为list是线性存储，数据量大的时候，插入和删除效率很低。deque是为了高效实现插入和删除操作的双向列表，适合用于队列和栈： 123456&gt;&gt;&gt; from collections import deque&gt;&gt;&gt; q = deque(['a', 'b', 'c'])&gt;&gt;&gt; q.append('x')&gt;&gt;&gt; q.appendleft('y')&gt;&gt;&gt; qdeque(['y', 'a', 'b', 'c', 'x']) deque除了实现list的append()和pop()外，还支持appendleft()和popleft()，这样就可以非常高效地往头部添加或删除元素。 defaultdict 使用dict时，如果引用的Key不存在，就会抛出KeyError。如果希望key不存在时，返回一个默认值，就可以用defaultdict： 1234567&gt;&gt;&gt; from collections import defaultdict&gt;&gt;&gt; dd = defaultdict(lambda: 'N/A')&gt;&gt;&gt; dd['key1'] = 'abc'&gt;&gt;&gt; dd['key1'] # key1存在'abc'&gt;&gt;&gt; dd['key2'] # key2不存在，返回默认值'N/A' 除了在Key不存在时返回默认值，defaultdict的其他行为跟dict是完全一样的。 OrderedDict 使用dict时，Key是无序的。在对dict做迭代时，我们无法确定Key的顺序。如果要保持Key的顺序，可以用OrderedDict： 1234567&gt;&gt;&gt; from collections import OrderedDict&gt;&gt;&gt; d = dict([('a', 1), ('b', 2), ('c', 3)])&gt;&gt;&gt; d # dict的Key是无序的&#123;'a': 1, 'c': 3, 'b': 2&#125;&gt;&gt;&gt; od = OrderedDict([('a', 1), ('b', 2), ('c', 3)])&gt;&gt;&gt; od # OrderedDict的Key是有序的OrderedDict([('a', 1), ('b', 2), ('c', 3)]) 注意，OrderedDict的Key会按照插入的顺序排列，不是Key本身排序： 123456&gt;&gt;&gt; od = OrderedDict()&gt;&gt;&gt; od['z'] = 1&gt;&gt;&gt; od['y'] = 2&gt;&gt;&gt; od['x'] = 3&gt;&gt;&gt; list(od.keys()) # 按照插入的Key的顺序返回['z', 'y', 'x'] OrderedDict可以实现一个FIFO（先进先出）的dict，当容量超出限制时，先删除最早添加的Key： 12345678910111213141516171819from collections import OrderedDictclass LastUpdatedOrderedDict(OrderedDict): def __init__(self, capacity): super(LastUpdatedOrderedDict, self).__init__() self._capacity = capacity def __setitem__(self, key, value): containsKey = 1 if key in self else 0 if len(self) - containsKey &gt;= self._capacity: last = self.popitem(last=False) print('remove:', last) if containsKey: del self[key] print('set:', (key, value)) else: print('add:', (key, value)) OrderedDict.__setitem__(self, key, value) Counter Counter是一个简单的计数器，例如，统计字符出现的个数： 1234567&gt;&gt;&gt; from collections import Counter&gt;&gt;&gt; c = Counter()&gt;&gt;&gt; for ch in 'programming':... c[ch] = c[ch] + 1...&gt;&gt;&gt; cCounter(&#123;'g': 2, 'm': 2, 'r': 2, 'a': 1, 'i': 1, 'o': 1, 'n': 1, 'p': 1&#125;) Counter实际上也是dict的一个子类，上面的结果可以看出，字符&#39;g&#39;、&#39;m&#39;、&#39;r&#39;各出现了两次，其他字符各出现了一次。 base64Base64是一种用64个字符来表示任意二进制数据的方法。 用记事本打开exe、jpg、pdf这些文件时，我们都会看到一大堆乱码，因为二进制文件包含很多无法显示和打印的字符，所以，如果要让记事本这样的文本处理软件能处理二进制数据，就需要一个二进制到字符串的转换方法。Base64是一种最常见的二进制编码方法。 Base64的原理很简单，首先，准备一个包含64个字符的数组： 1[&apos;A&apos;, &apos;B&apos;, &apos;C&apos;, ... &apos;a&apos;, &apos;b&apos;, &apos;c&apos;, ... &apos;0&apos;, &apos;1&apos;, ... &apos;+&apos;, &apos;/&apos;] 然后，对二进制数据进行处理，每3个字节一组，一共是3x8=24bit，划为4组，每组正好6个bit： 这样我们得到4个数字作为索引，然后查表，获得相应的4个字符，就是编码后的字符串。 所以，Base64编码会把3字节的二进制数据编码为4字节的文本数据，长度增加33%，好处是编码后的文本数据可以在邮件正文、网页等直接显示。 如果要编码的二进制数据不是3的倍数，最后会剩下1个或2个字节怎么办？Base64用\\x00字节在末尾补足后，再在编码的末尾加上1个或2个=号，表示补了多少字节，解码的时候，会自动去掉。 Python内置的base64可以直接进行base64的编解码： 12345&gt;&gt;&gt; import base64&gt;&gt;&gt; base64.b64encode(b'binary\\x00string')b'YmluYXJ5AHN0cmluZw=='&gt;&gt;&gt; base64.b64decode(b'YmluYXJ5AHN0cmluZw==')b'binary\\x00string' 由于标准的Base64编码后可能出现字符+和/，在URL中就不能直接作为参数，所以又有一种&quot;url safe&quot;的base64编码，其实就是把字符+和/分别变成-和_： 123456&gt;&gt;&gt; base64.b64encode(b'i\\xb7\\x1d\\xfb\\xef\\xff')b'abcd++//'&gt;&gt;&gt; base64.urlsafe_b64encode(b'i\\xb7\\x1d\\xfb\\xef\\xff')b'abcd--__'&gt;&gt;&gt; base64.urlsafe_b64decode('abcd--__')b'i\\xb7\\x1d\\xfb\\xef\\xff' 还可以自己定义64个字符的排列顺序，这样就可以自定义Base64编码，不过，通常情况下完全没有必要。Base64是一种通过查表的编码方法，不能用于加密，即使使用自定义的编码表也不行。 Base64适用于小段内容的编码，比如数字证书签名、Cookie的内容等。 由于=字符也可能出现在Base64编码中，但=用在URL、Cookie里面会造成歧义，所以，很多Base64编码后会把=去掉： 1234# 标准Base64:'abcd' -&gt; 'YWJjZA=='# 自动去掉=:'abcd' -&gt; 'YWJjZA' 去掉=后怎么解码呢？因为Base64是把3个字节变为4个字节，所以，Base64编码的长度永远是4的倍数，因此，需要加上=把Base64字符串的长度变为4的倍数，就可以正常解码了。 struct准确地讲，Python没有专门处理字节的数据类型。但由于b&#39;str&#39;可以表示字节，所以，字节数组 ＝ 二进制str。而在C语言中，我们可以很方便地用struct、union来处理字节，以及字节和int，float的转换。 在Python中，比方说要把一个32位无符号整数变成字节，也就是4个长度的bytes，你得配合位运算符这么写： 12345678&gt;&gt;&gt; n = 10240099&gt;&gt;&gt; b1 = (n &amp; 0xff000000) &gt;&gt; 24&gt;&gt;&gt; b2 = (n &amp; 0xff0000) &gt;&gt; 16&gt;&gt;&gt; b3 = (n &amp; 0xff00) &gt;&gt; 8&gt;&gt;&gt; b4 = n &amp; 0xff&gt;&gt;&gt; bs = bytes([b1, b2, b3, b4])&gt;&gt;&gt; bsb'\\x00\\x9c@c' 非常麻烦。如果换成浮点数就无能为力了。 好在Python提供了一个struct模块来解决bytes和其他二进制数据类型的转换。 struct的pack函数把任意数据类型变成bytes： 123&gt;&gt;&gt; import struct&gt;&gt;&gt; struct.pack('&gt;I', 10240099)b'\\x00\\x9c@c' pack的第一个参数是处理指令，&#39;&gt;I&#39;的意思是： &gt;表示字节顺序是big-endian，也就是网络序，I表示4字节无符号整数。 后面的参数个数要和处理指令一致。 unpack把bytes变成相应的数据类型： 12&gt;&gt;&gt; struct.unpack('&gt;IH', b'\\xf0\\xf0\\xf0\\xf0\\x80\\x80')(4042322160, 32896) 根据 &gt;IH 的说明，后面的bytes依次变为I：4字节无符号整数和 H：2 字节无符号整数。 所以，尽管Python不适合编写底层操作字节流的代码，但在对性能要求不高的地方，利用struct就方便多了。struct模块定义的数据类型可以参考Python官方文档 Windows的位图文件（.bmp）是一种非常简单的文件格式，我们来用struct分析一下。首先找一个bmp文件，没有的话用“画图”画一个。 读入前30个字节来分析： 1&gt;&gt;&gt; s = b'\\x42\\x4d\\x38\\x8c\\x0a\\x00\\x00\\x00\\x00\\x00\\x36\\x00\\x00\\x00\\x28\\x00\\x00\\x00\\x80\\x02\\x00\\x00\\x68\\x01\\x00\\x00\\x01\\x00\\x18\\x00' BMP格式采用小端方式存储数据，文件头的结构按顺序如下： 两个字节：&#39;BM&#39;表示Windows位图，&#39;BA&#39;表示OS/2位图；一个4字节整数：表示位图大小；一个4字节整数：保留位，始终为0；一个4字节整数：实际图像的偏移量；一个4字节整数：Header的字节数；一个4字节整数：图像宽度；一个4字节整数：图像高度；一个2字节整数：始终为1；一个2字节整数：颜色数。 所以，组合起来用unpack读取： 12&gt;&gt;&gt; struct.unpack('&lt;ccIIIIIIHH', s)(b'B', b'M', 691256, 0, 54, 40, 640, 360, 1, 24) 结果显示，b&#39;B&#39;、b&#39;M&#39;说明是Windows位图，位图大小为640x360，颜色数为24。 请编写一个bmpinfo.py，可以检查任意文件是否是位图文件，如果是，打印出图片大小和颜色数。 hashlibPython的hashlib提供了常见的摘要算法，如MD5，SHA1等等。 什么是摘要算法呢？摘要算法又称哈希算法、散列算法。它通过一个函数，把任意长度的数据转换为一个长度固定的数据串（通常用16进制的字符串表示）。 举个例子，你写了一篇文章，内容是一个字符串’how to use python hashlib - by Michael’，并附上这篇文章的摘要是’2d73d4f15c0db7f5ecb321b6a65e5d6d’。如果有人篡改了你的文章，并发表为’how to use python hashlib - by Bob’，你可以一下子指出Bob篡改了你的文章，因为根据’how to use python hashlib - by Bob’计算出的摘要不同于原始文章的摘要。 可见，摘要算法就是通过摘要函数f()对任意长度的数据data计算出固定长度的摘要digest，目的是为了发现原始数据是否被人篡改过。 摘要算法之所以能指出数据是否被篡改过，就是因为摘要函数是一个 单向函数，计算f(data)很容易，但通过digest反推data却非常困难。而且，对原始数据做一个bit的修改，都会导致计算出的摘要完全不同。 我们以常见的摘要算法MD5为例，计算出一个字符串的MD5值： 12345import hashlibmd5 = hashlib.md5()md5.update('how to use md5 in python hashlib?'.encode('utf-8'))print(md5.hexdigest()) 计算结果如下：1d26a53750bc40b38b65a520292f69306 如果数据量很大，可以分块多次调用update()，最后计算的结果是一样的： 123456import hashlibmd5 = hashlib.md5()md5.update('how to use md5 in '.encode('utf-8'))md5.update('python hashlib?'.encode('utf-8'))print(md5.hexdigest()) MD5是最常见的摘要算法，速度很快，生成结果是固定的128 bit字节，通常用一个32位的16进制字符串表示。 另一种常见的摘要算法是SHA1，调用SHA1和调用MD5完全类似： 123456import hashlibsha1 = hashlib.sha1()sha1.update('how to use sha1 in '.encode('utf-8'))sha1.update('python hashlib?'.encode('utf-8'))print(sha1.hexdigest()) SHA1的结果是160 bit字节，通常用一个40位的16进制字符串表示。 比SHA1更安全的算法是SHA256和SHA512，不过越安全的算法不仅越慢，而且摘要长度更长。 有没有可能两个不同的数据通过某个摘要算法得到了相同的摘要？完全有可能，因为任何摘要算法都是把无限多的数据集合映射到一个有限的集合中。这种情况称为碰撞，比如Bob试图根据你的摘要反推出一篇文章’how to learn hashlib in python - by Bob’，并且这篇文章的摘要恰好和你的文章完全一致，这种情况也并非不可能出现，但是非常非常困难。 摘要算法应用 摘要算法能应用到什么地方？举个常用例子： 任何允许用户登录的网站都会存储用户登录的用户名和口令。如何存储用户名和口令呢？方法是存到数据库表,如果以明文保存用户口令，如果数据库泄露，所有用户的口令就落入黑客的手里。此外，网站运维人员是可以访问数据库的，也就是能获取到所有用户的口令。正确的保存口令的方式是不存储用户的明文口令，而是存储用户口令的摘要，比如MD5： name password Bob e10adc3949ba59abbe56e057f20f883e 当用户登录时，首先计算用户输入的明文口令的MD5，然后和数据库存储的MD5对比，如果一致，说明口令输入正确，如果不一致，口令肯定错误。 itertoolsPython的内建模块itertools提供了非常有用的用于操作迭代对象的函数。 首先，我们看看itertools提供的几个“无限”迭代器： 123456789&gt;&gt;&gt; import itertools&gt;&gt;&gt; natuals = itertools.count(1)&gt;&gt;&gt; for n in natuals:... print(n)...123... 因为count()会创建一个无限的迭代器，所以上述代码会打印出自然数序列，根本停不下来，只能按Ctrl+C退出。 cycle()会把传入的一个序列无限重复下去： 123456789101112&gt;&gt;&gt; import itertools&gt;&gt;&gt; cs = itertools.cycle('ABC') # 注意字符串也是序列的一种&gt;&gt;&gt; for c in cs:... print(c)...'A''B''C''A''B''C'... 同样停不下来。repeat()负责把一个元素无限重复下去，不过如果提供第二个参数就可以限定重复次数： 1234567&gt;&gt;&gt; ns = itertools.repeat('A', 3)&gt;&gt;&gt; for n in ns:... print(n)...AAA 无限序列只有在for迭代时才会无限地迭代下去，如果只是创建了一个迭代对象，它不会事先把无限个元素生成出来，事实上也不可能在内存中创建无限多个元素。 无限序列虽然可以无限迭代下去，但是通常我们会通过takewhile()等函数根据条件判断来截取出一个有限的序列： 1234&gt;&gt;&gt; natuals = itertools.count(1)&gt;&gt;&gt; ns = itertools.takewhile(lambda x: x &lt;= 10, natuals)&gt;&gt;&gt; list(ns)[1, 2, 3, 4, 5, 6, 7, 8, 9, 10] itertools提供的几个迭代器操作函数更加有用： chain() chain()可以把一组迭代对象串联起来，形成一个更大的迭代器： 123&gt;&gt;&gt; for c in itertools.chain('ABC', 'XYZ'):... print(c)# 迭代效果：'A' 'B' 'C' 'X' 'Y' 'Z' groupby() groupby()把迭代器中相邻的重复元素挑出来放在一起： 1234567&gt;&gt;&gt; for key, group in itertools.groupby('AAABBBCCAAA'):... print(key, list(group))...A ['A', 'A', 'A']B ['B', 'B', 'B']C ['C', 'C']A ['A', 'A', 'A'] 实际上挑选规则是通过函数完成的，只要作用于函数的两个元素返回的值相等，这两个元素就被认为是在一组的，而函数返回值作为组的key。如果我们要忽略大小写分组，就可以让元素’A’和’a’都返回相同的key： 1234567&gt;&gt;&gt; for key, group in itertools.groupby('AaaBBbcCAAa', lambda c: c.upper()):... print(key, list(group))...A ['A', 'a', 'a']B ['B', 'B', 'b']C ['c', 'C']A ['A', 'A', 'a'] 练习 计算圆周率可以根据公式(格雷戈里和莱布尼茨)： 利用Python提供的itertools模块，我们来计算这个序列的前N项和： 1234567891011def pi(N): ' 计算pi的值 ' # step 1: 创建一个奇数序列: 1, 3, 5, 7, 9, ... odd_numbers_1 = itertools.count(1, 2) # step 2: 取该序列的前N项: 1, 3, 5, 7, 9, ..., 2*N-1. odd_numbers_2 = itertools.takewhile(lambda x: x &lt;= 2 * N - 1, odd_numbers_1) # step 3: 添加正负符号并用4除: 4/1, -4/3, 4/5, -4/7, 4/9, ... symbol_cycles = itertools.cycle([1, -1]) odd_numbers_3 = map(lambda x: next(symbol_cycles) * 4 / x, odd_numbers_2) # step 4: 求和: return sum(odd_numbers_3) contextlib在Python中，读写文件这样的资源要特别注意，必须在使用完毕后正确关闭它们。正确关闭文件资源的一个方法是使用try...finally,Python的with语句允许我们非常方便地使用资源，而不必担心资源没有关闭： 12with open('/path/to/file', 'r') as f: f.read() 并不是只有open()函数返回的fp对象才能使用with语句。实际上，任何对象，只要正确实现了上下文管理，就可以用于with语句。 实现上下文管理是通过__enter__和__exit__这两个方法实现的。例如，下面的class实现了这两个方法： 1234567891011121314151617class Query(object): def __init__(self, name): self.name = name def __enter__(self): print('Begin') return self def __exit__(self, exc_type, exc_value, traceback): if exc_type: print('Error') else: print('End') def query(self): print('Query info about %s...' % self.name) 这样我们就可以把自己写的资源对象用于with语句： 12with Query('Bob') as q: q.query() @contextmanager 编写__enter__和__exit__仍然很繁琐，因此Python的标准库contextlib提供了更简单的写法，上面的代码可以改写如下： 12345678910111213141516from contextlib import contextmanagerclass Query(object): def __init__(self, name): self.name = name def query(self): print('Query info about %s...' % self.name)@contextmanagerdef create_query(name): print('Begin') q = Query(name) yield q print('End') @contextmanager这个decorator接受一个generator，用yield语句把with ... as var把变量输出出去，然后，with语句就可以正常地工作了： 12with create_query('Bob') as q: q.query() 很多时候，我们希望在某段代码执行前后自动执行特定代码，也可以用@contextmanager实现。例如： 123456789@contextmanagerdef tag(name): print(\"&lt;%s&gt;\" % name) yield print(\"&lt;/%s&gt;\" % name)with tag(\"h1\"): print(\"hello\") print(\"world\") 上述代码执行结果为： 1234&lt;h1&gt;helloworld&lt;/h1&gt; 代码的执行顺序是： with语句首先执行yield之前的语句，因此打印出&lt;h1&gt;； yield调用会执行with语句内部的所有语句，因此打印出hello和world； 最后执行yield之后的语句，打印出&lt;/h1&gt;。 因此，@contextmanager让我们通过编写generator来简化上下文管理。 @closing 如果一个对象没有实现上下文，我们就不能把它用于with语句。这个时候，可以用closing()来把该对象变为上下文对象。例如，用with语句使用urlopen()： 123456from contextlib import closingfrom urllib.request import urlopenwith closing(urlopen('https://www.python.org')) as page: for line in page: print(line) closing也是一个经过@contextmanager装饰的generator，这个generator编写起来其实非常简单： 123456@contextmanagerdef closing(thing): try: yield thing finally: thing.close() 它的作用就是把任意对象变为上下文对象，并支持with语句。@contextlib还有一些其他decorator，便于我们编写更简洁的代码。 urlliburllib提供了一系列用于操作URL的功能。 Get urllib的request模块可以非常方便地抓取URL内容，也就是发送一个GET请求到指定的页面，然后返回HTTP的响应： 例如，对豆瓣的一个URLhttps://api.douban.com/v2/book/2129650进行抓取，并返回响应： 12345678from urllib import requestwith request.urlopen('https://api.douban.com/v2/book/2129650') as f: data = f.read() print('Status:', f.status, f.reason) for k, v in f.getheaders(): print('%s: %s' % (k, v)) print('Data:', data.decode('utf-8')) 可以看到HTTP响应的头和JSON数据： 1234567891011Status: 200 OKServer: nginxDate: Tue, 26 May 2015 10:02:27 GMTContent-Type: application/json; charset=utf-8Content-Length: 2049Connection: closeExpires: Sun, 1 Jan 2006 01:00:00 GMTPragma: no-cacheCache-Control: must-revalidate, no-cache, privateX-DAE-Node: pidl1Data: &#123;\"rating\":&#123;\"max\":10,\"numRaters\":16,\"average\":\"7.4\",\"min\":0&#125;,\"subtitle\":\"\",\"author\":[\"廖雪峰编著\"],\"pubdate\":\"2007-6\",...&#125; 如果我们要想模拟浏览器发送GET请求，就需要使用Request对象，通过往Request对象添加HTTP头，我们就可以把请求伪装成浏览器。例如，模拟iPhone 6去请求豆瓣首页： 123456789from urllib import requestreq = request.Request('http://www.douban.com/')req.add_header('User-Agent', 'Mozilla/6.0 (iPhone; CPU iPhone OS 8_0 like Mac OS X) AppleWebKit/536.26 (KHTML, like Gecko) Version/8.0 Mobile/10A5376e Safari/8536.25')with request.urlopen(req) as f: print('Status:', f.status, f.reason) for k, v in f.getheaders(): print('%s: %s' % (k, v)) print('Data:', f.read().decode('utf-8')) 这样豆瓣会返回适合iPhone的移动版网页： 12345... &lt;meta name=\"viewport\" content=\"width=device-width, user-scalable=no, initial-scale=1.0, minimum-scale=1.0, maximum-scale=1.0\"&gt; &lt;meta name=\"format-detection\" content=\"telephone=no\"&gt; &lt;link rel=\"apple-touch-icon\" sizes=\"57x57\" href=\"http://img4.douban.com/pics/cardkit/launcher/57.png\" /&gt;... Post 如果要以POST发送一个请求，只需要把参数data以bytes形式传入。 我们模拟一个微博登录，先读取登录的邮箱和口令，然后按照weibo.cn的登录页的格式以username=xxx&amp;password=xxx的编码传入： 12345678910111213141516171819202122232425from urllib import request, parseprint('Login to weibo.cn...')email = input('Email: ')passwd = input('Password: ')login_data = parse.urlencode([ ('username', email), ('password', passwd), ('entry', 'mweibo'), ('client_id', ''), ('savestate', '1'), ('ec', ''), ('pagerefer', 'https://passport.weibo.cn/signin/welcome?entry=mweibo&amp;r=http%3A%2F%2Fm.weibo.cn%2F')])req = request.Request('https://passport.weibo.cn/sso/login')req.add_header('Origin', 'https://passport.weibo.cn')req.add_header('User-Agent', 'Mozilla/6.0 (iPhone; CPU iPhone OS 8_0 like Mac OS X) AppleWebKit/536.26 (KHTML, like Gecko) Version/8.0 Mobile/10A5376e Safari/8536.25')req.add_header('Referer', 'https://passport.weibo.cn/signin/login?entry=mweibo&amp;res=wel&amp;wm=3349&amp;r=http%3A%2F%2Fm.weibo.cn%2F')with request.urlopen(req, data=login_data.encode('utf-8')) as f: print('Status:', f.status, f.reason) for k, v in f.getheaders(): print('%s: %s' % (k, v)) print('Data:', f.read().decode('utf-8')) 如果登录成功，我们获得的响应如下： 123456Status: 200 OKServer: nginx/1.2.0...Set-Cookie: SSOLoginState=1432620126; path=/; domain=weibo.cn...Data: &#123;\"retcode\":20000000,\"msg\":\"\",\"data\":&#123;...,\"uid\":\"1658384301\"&#125;&#125; 如果登录失败，我们获得的响应如下： 12...Data: &#123;\"retcode\":50011015,\"msg\":\"\\u7528\\u6237\\u540d\\u6216\\u5bc6\\u7801\\u9519\\u8bef\",\"data\":&#123;\"username\":\"example@python.org\",\"errline\":536&#125;&#125; Handler 如果还需要更复杂的控制，比如通过一个Proxy去访问网站，我们需要利用ProxyHandler来处理，示例代码如下： 123456proxy_handler = urllib.request.ProxyHandler(&#123;'http': 'http://www.example.com:3128/'&#125;)proxy_auth_handler = urllib.request.ProxyBasicAuthHandler()proxy_auth_handler.add_password('realm', 'host', 'username', 'password')opener = urllib.request.build_opener(proxy_handler, proxy_auth_handler)with opener.open('http://www.example.com/login.html') as f: pass HTMLParser如果我们要编写一个搜索引擎，第一步是用爬虫把目标网站的页面抓下来，第二步就是解析该HTML页面，看看里面的内容到底是新闻、图片还是视频。 假设第一步已经完成了，第二步应该如何解析HTML呢？HTML本质上是XML的子集，但是HTML的语法没有XML那么严格，所以不能用标准的DOM或SAX来解析HTML。 好在Python提供了HTMLParser来非常方便地解析HTML，只需简单几行代码： 123456789101112131415161718192021222324252627282930313233from html.parser import HTMLParserfrom html.entities import name2codepointclass MyHTMLParser(HTMLParser): def handle_starttag(self, tag, attrs): print('&lt;%s&gt;' % tag) def handle_endtag(self, tag): print('&lt;/%s&gt;' % tag) def handle_startendtag(self, tag, attrs): print('&lt;%s/&gt;' % tag) def handle_data(self, data): print(data) def handle_comment(self, data): print('&lt;!--', data, '--&gt;') def handle_entityref(self, name): print('&amp;%s;' % name) def handle_charref(self, name): print('&amp;#%s;' % name)parser = MyHTMLParser()parser.feed('''&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;&lt;!-- test html parser --&gt; &lt;p&gt;Some &lt;a href=\\\"#\\\"&gt;html&lt;/a&gt; HTML&amp;nbsp;tutorial...&lt;br&gt;END&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;''') feed()方法可以多次调用，也就是不一定一次把整个HTML字符串都塞进去，可以一部分一部分塞进去。 特殊字符有两种，一种是英文表示的&amp;nbsp;，一种是数字表示的&amp;#1234;，这两种字符都可以通过Parser解析出来。 练习 找一个网页，例如https://www.python.org/events/python-events/，用浏览器查看源码并复制，然后尝试解析一下HTML，输出Python官网发布的会议时间、名称和地点。 NEXT..","tags":[{"name":"Python","slug":"Python","permalink":"http://wenbo.fun/tags/Python/"}]},{"title":"Python教程笔记 (2/4)","date":"2018-02-03T05:24:08.000Z","path":"2018/02/03/myPython-a/","text":"面向对象编程面向对象编程——Object Oriented Programming，简称OOP，是一种程序设计思想。OOP把对象作为程序的基本单元，一个对象包含了数据和操作数据的函数。在Python中，所有数据类型都可以视为对象，当然也可以自定义对象。自定义的对象数据类型就是面向对象中的 类（Class） 的概念。给对象发消息实际上就是调用对象对应的关联函数，我们称之为对象的 方法（Method）。数据封装、继承和多态是面向对象的三大特点，我们后面会详细讲解。 类和实例面向对象最重要的概念就是类（Class）和实例（Instance），必须牢记类是抽象的模板，比如Student类，而实例是根据类创建出来的一个个具体的“对象”，每个对象都拥有相同的方法，但各自的数据可能不同。 仍以Student类为例，在Python中，定义类是通过class关键字： 12class Student(object): pass class后面紧接着是类名，即Student，类名通常是大写开头的单词，紧接着是(object)，表示该类是从哪个类继承下来的，继承的概念我们后面再讲，通常，如果没有合适的继承类，就使用object类，这是所有类最终都会继承的类。 定义好了Student类，就可以根据Student类创建出Student的实例，创建实例是通过类名+()实现的： 12345&gt;&gt;&gt; bart = Student()&gt;&gt;&gt; bart&lt;__main__.Student object at 0x10a67a590&gt;&gt;&gt;&gt; Student&lt;class '__main__.Student'&gt; 可以看到，变量bart指向的就是一个Student的实例，后面的0x10a67a590是内存地址，每个object的地址都不一样，而Student本身则是一个类。 可以自由地给一个实例变量绑定属性，比如，给实例bart绑定一个name属性: 123&gt;&gt;&gt; bart.name = 'Bart Simpson'&gt;&gt;&gt; bart.name'Bart Simpson' 由于类可以起到模板的作用，因此，可以在创建实例的时候，把一些我们认为必须绑定的属性强制填写进去。通过定义一个特殊的__init__方法，在创建实例的时候，就把name，score等属性绑上去： 12345class Student(object): def __init__(self, name, score): self.name = name self.score = score 特殊方法__init__前后分别有两个下划线！！！ 注意到__init__方法的第一个参数永远是self，表示创建的实例本身，因此，在__init__方法内部，就可以把各种属性绑定到self，因为self就指向创建的实例本身。 有了__init__方法，在创建实例的时候，就不能传入空的参数了，必须传入与__init__方法匹配的参数，但self不需要传，Python解释器自己会把实例变量传进去： 12345&gt;&gt;&gt; bart = Student('Bart Simpson', 59)&gt;&gt;&gt; bart.name'Bart Simpson'&gt;&gt;&gt; bart.score59 和普通的函数相比，在类中定义的函数只有一点不同，就是第一个参数永远是实例变量self，并且，调用时，不用传递该参数。除此之外，类的方法和普通函数没有什么区别，所以，你仍然可以用默认参数、可变参数、关键字参数和命名关键字参数。 数据封装 面向对象编程的一个重要特点就是数据封装。在上面的Student类中，每个实例就拥有各自的name和score这些数据。我们可以通过函数来访问这些数据，比如打印一个学生的成绩： 12345 &gt;&gt;&gt; def print_score(std):... print('%s: %s' % (std.name, std.score))...&gt;&gt;&gt; print_score(bart)Bart Simpson: 59 但是，既然Student实例本身就拥有这些数据，要访问这些数据，就没有必要从外面的函数去访问，可以直接在Student类的内部定义访问数据的函数，这样，就把“数据”给封装起来了。这些封装数据的函数是和Student类本身是关联起来的，我们称之为 类的方法 ： 12345678class Student(object): def __init__(self, name, score): self.name = name self.score = score def print_score(self): print('%s: %s' % (self.name, self.score)) 这样一来，我们从外部看Student类，就只需要知道，创建实例需要给出name和score，而如何打印，都是在Student类的内部定义的，这些数据和逻辑被“封装”起来了，调用很容易，但却不用知道内部实现的细节。封装的另一个好处是可以给Student类增加新的方法，比如get_grade： 12345678910class Student(object): ... def get_grade(self): if self.score &gt;= 90: return 'A' elif self.score &gt;= 60: return 'B' else: return 'C' 同样的，get_grade方法可以直接在实例变量上调用，不需要知道内部实现细节： 1234lisa = Student('Lisa', 99)bart = Student('Bart', 59)print(lisa.name, lisa.get_grade())print(bart.name, bart.get_grade()) 访问限制在Class内部，可以有属性和方法，而外部代码可以通过直接调用实例变量的方法来操作数据，这样，就隐藏了内部的复杂逻辑。 但是，从前面Student类的定义来看，外部代码还是可以自由地修改一个实例的name、score属性： 123456&gt;&gt;&gt; bart = Student('Bart Simpson', 59)&gt;&gt;&gt; bart.score59&gt;&gt;&gt; bart.score = 99&gt;&gt;&gt; bart.score99 如果要让内部属性不被外部访问，可以把属性的名称前加上两个下划线__，在Python中，实例的变量名如果以__开头，就变成了一个私有变量（private），只有内部可以访问，外部不能访问，所以，我们把Student类改一改： 12345678class Student(object): def __init__(self, name, score): self.__name = name self.__score = score def print_score(self): print('%s: %s' % (self.__name, self.__score)) 改完后，对于外部代码来说，没什么变动，但是已经无法从外部访问实例变量.__name和实例变量.__score了： 12345&gt;&gt;&gt; bart = Student('Bart Simpson', 59)&gt;&gt;&gt; bart.__nameTraceback (most recent call last): File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;AttributeError: 'Student' object has no attribute '__name' 这样就确保了外部代码不能随意修改对象内部的状态，这样通过访问限制的保护，代码更加健壮。但是如果外部代码要获取name和score怎么办？可以给Student类增加get_name和get_score这样的方法： 12345678class Student(object): ... def get_name(self): return self.__name def get_score(self): return self.__score 如果又要允许外部代码修改score怎么办？可以再给Student类增加set_score方法,对参数做检查，避免传入无效的参数： 12345678class Student(object): ... def set_score(self, score): if 0 &lt;= score &lt;= 100: self.__score = score else: raise ValueError('bad score') 需要注意的是，在Python中，变量名类似__xxx__的，也就是以双下划线开头，并且以双下划线结尾的，是特殊变量，特殊变量是可以直接访问的，不是private变量，所以，不能用__name__、__score__这样的变量名。有些时候，你会看到以一个下划线开头的实例变量名，比如_name，这样的实例变量外部是可以访问的，但是，按照约定俗成的规定，当你看到这样的变量时，意思就是，“虽然我可以被访问，但是，请把我视为私有变量，不要随意访问”。 双下划线开头的实例变量是不是一定不能从外部访问呢？其实也不是。不能直接访问__name是因为Python解释器对外把__name变量改成了_Student__name，所以，仍然可以通过_Student__name来访问__name变量： 12&gt;&gt;&gt; bart._Student__name'Bart Simpson' 但是强烈建议你不要这么干，因为不同版本的Python解释器可能会把__name改成不同的变量名。总的来说就是，Python本身没有任何机制阻止你干坏事，一切全靠自觉。 最后注意下面的这种 错误写法： 123456&gt;&gt;&gt; bart = Student('Bart Simpson', 59)&gt;&gt;&gt; bart.get_name()'Bart Simpson'&gt;&gt;&gt; bart.__name = 'New Name' # 设置__name变量！&gt;&gt;&gt; bart.__name'New Name' 表面上看，外部代码“成功”地设置了__name变量，但实际上这个__name变量和class内部的__name变量不是一个变量！内部的__name变量已经被Python解释器自动改成了_Student__name，而外部代码给bart新增了一个__name变量。不信试试： 12&gt;&gt;&gt; bart.get_name() # get_name()内部返回self.__name'Bart Simpson' 继承和多态在OOP程序设计中，当我们定义一个class的时候，可以从某个现有的class继承，新的class称为子类（Subclass），而被继承的class称为基类、父类或超类（Base class、Super class）。在继承关系中，如果一个实例的数据类型是某个子类，那它的数据类型也可以被看做是父类。但是，反过来就不行. 要理解多态的好处，我们还需要再编写一个函数，这个函数接受一个Animal类型的变量： 1234567class Animal(object): def run(self): print('Animal is running...')def run_twice(animal): animal.run() animal.run() 当我们传入Animal的实例时，run_twice()就打印出： 123&gt;&gt;&gt; run_twice(Animal())Animal is running...Animal is running... 现在，如果我们再定义一个Tortoise类型，也从Animal派生： 123class Tortoise(Animal): def run(self): print('Tortoise is running slowly...') 当我们调用run_twice()时，传入Tortoise的实例： 123&gt;&gt;&gt; run_twice(Tortoise())Tortoise is running slowly...Tortoise is running slowly... 你会发现，新增一个Animal的子类，不必对run_twice()做任何修改，实际上，任何依赖Animal作为参数的函数或者方法都可以不加修改地正常运行，原因就在于多态。 多态的好处就是，当我们需要传入Dog、Cat、Tortoise……时，我们只需要接收Animal类型就可以了，因为Dog、Cat、Tortoise……都是Animal类型，然后，按照Animal类型进行操作即可。由于Animal类型有run()方法，因此，传入的任意类型，只要是Animal类或者子类，就会自动调用实际类型的run()方法，这就是多态的意思。 对于一个变量，我们只需要知道它是Animal类型，无需确切地知道它的子类型，就可以放心地调用run()方法，而具体调用的run()方法是作用在Animal、Dog、Cat还是Tortoise对象上，由运行时该对象的确切类型决定，这就是多态真正的威力：调用方只管调用，不管细节，而当我们新增一种Animal的子类时，只要确保run()方法编写正确，不用管原来的代码是如何调用的。这就是著名的 “开闭”原则： 对扩展开放：允许新增Animal子类； 对修改封闭：不需要修改依赖Animal类型的run_twice()等函数。 静态语言 vs 动态语言 对于静态语言（例如Java）来说，如果需要传入Animal类型，则传入的对象必须是Animal类型或者它的子类，否则，将无法调用run()方法。 对于Python这样的动态语言来说，则不一定需要传入Animal类型。我们只需要保证传入的对象有一个run()方法就可以了： 123class Timer(object): def run(self): print('Start...') 这就是动态语言的 鸭子类型，它并不要求严格的继承体系，一个对象只要“看起来像鸭子，走起路来像鸭子”，那它就可以被看做是鸭子。 Python的 “file-like object” 就是一种鸭子类型。对真正的文件对象，它有一个read()方法，返回其内容。但是，许多对象，只要有read()方法，都被视为 “file-like object” 。许多函数接收的参数就是 “file-like object”，你不一定要传入真正的文件对象，完全可以传入任何实现了read()方法的对象。 获取对象信息当我们拿到一个对象的引用时，如何知道这个对象是什么类型、有哪些方法呢？ 使用type() type()函数返回对应的Class类型。如果我们要在if语句中判断，就需要比较两个变量的type类型是否相同： 12345678&gt;&gt;&gt; type(123)==intTrue&gt;&gt;&gt; type('abc')==type('123')True&gt;&gt;&gt; type('abc')==strTrue&gt;&gt;&gt; type('abc')==type(123)False 判断基本数据类型可以直接写int，str等，但如果要判断一个对象是否是函数怎么办？可以使用types模块中定义的常量： 123456789101112&gt;&gt;&gt; import types&gt;&gt;&gt; def fn():... pass...&gt;&gt;&gt; type(fn)==types.FunctionTypeTrue&gt;&gt;&gt; type(abs)==types.BuiltinFunctionTypeTrue&gt;&gt;&gt; type(lambda x: x)==types.LambdaTypeTrue&gt;&gt;&gt; type((x for x in range(10)))==types.GeneratorTypeTrue 使用isinstance() 对于class的继承关系来说，使用type()就很不方便。我们要判断class的类型，可以使用isinstance()函数。 我们回顾上次的例子，如果继承关系是： 1object -&gt; Animal -&gt; Dog -&gt; Husky 那么，isinstance()就可以告诉我们，一个对象是否是某种类型。先创建3种类型的对象： 123&gt;&gt;&gt; a = Animal()&gt;&gt;&gt; d = Dog()&gt;&gt;&gt; h = Husky() 然后，判断： 1234&gt;&gt;&gt; isinstance(h, Husky)True&gt;&gt;&gt; isinstance(h, Dog)True isinstance()判断的是一个对象是否是该类型本身，或者位于该类型的父继承链上。 能用type()判断的基本类型也可以用isinstance()判断： 123456&gt;&gt;&gt; isinstance('a', str)True&gt;&gt;&gt; isinstance(123, int)True&gt;&gt;&gt; isinstance(b'a', bytes)True 并且还可以判断一个变量是否是某些类型中的一种，比如下面的代码就可以判断是否是list或者tuple： 1234&gt;&gt;&gt; isinstance([1, 2, 3], (list, tuple))True&gt;&gt;&gt; isinstance((1, 2, 3), (list, tuple))True 使用dir() 如果要获得一个对象的所有属性和方法，可以使用dir()函数，它返回一个包含字符串的list，比如，获得一个str对象的所有属性和方法： 12&gt;&gt;&gt; dir('ABC')['__add__', '__class__',..., '__subclasshook__', 'capitalize', 'casefold',..., 'zfill'] 类似__xxx__的属性和方法在Python中都是有特殊用途的，比如__len__方法返回长度。在Python中，如果你调用len()函数试图获取一个对象的长度，实际上，在len()函数内部，它自动去调用该对象的__len__()方法，所以，下面的代码是等价的： 1234&gt;&gt;&gt; len('ABC')3&gt;&gt;&gt; 'ABC'.__len__()3 我们自己写的类，如果也想用len(myObj)的话，就自己写一个__len__()方法： 1234567&gt;&gt;&gt; class MyDog(object):... def __len__(self):... return 100...&gt;&gt;&gt; dog = MyDog()&gt;&gt;&gt; len(dog)100 剩下的都是普通属性或方法，比如lower()返回小写的字符串： 12&gt;&gt;&gt; 'ABC'.lower()'abc' 仅仅把属性和方法列出来是不够的，配合getattr()、setattr()以及hasattr()，我们可以直接操作一个对象的状态： 1234567&gt;&gt;&gt; class MyObject(object):... def __init__(self):... self.x = 9... def power(self):... return self.x * self.x...&gt;&gt;&gt; obj = MyObject() 紧接着，可以测试该对象的属性： 12345678910111213&gt;&gt;&gt; hasattr(obj, 'x') # 有属性'x'吗？True&gt;&gt;&gt; obj.x9&gt;&gt;&gt; hasattr(obj, 'y') # 有属性'y'吗？False&gt;&gt;&gt; setattr(obj, 'y', 19) # 设置一个属性'y'&gt;&gt;&gt; hasattr(obj, 'y') # 有属性'y'吗？True&gt;&gt;&gt; getattr(obj, 'y') # 获取属性'y'19&gt;&gt;&gt; obj.y # 获取属性'y'19 如果试图获取不存在的属性，会抛出AttributeError的错误： 1234&gt;&gt;&gt; getattr(obj, 'z') # 获取属性'z'Traceback (most recent call last): File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;AttributeError: 'MyObject' object has no attribute 'z' 可以传入一个default参数，如果属性不存在，就返回默认值： 12&gt;&gt;&gt; getattr(obj, 'z', 404) # 获取属性'z'，如果不存在，返回默认值404404 也可以获得对象的方法： 123456789&gt;&gt;&gt; hasattr(obj, 'power') # 有属性'power'吗？True&gt;&gt;&gt; getattr(obj, 'power') # 获取属性'power'&lt;bound method MyObject.power of &lt;__main__.MyObject object at 0x10077a6a0&gt;&gt;&gt;&gt;&gt; fn = getattr(obj, 'power') # 获取属性'power'并赋值到变量fn&gt;&gt;&gt; fn # fn指向obj.power&lt;bound method MyObject.power of &lt;__main__.MyObject object at 0x10077a6a0&gt;&gt;&gt;&gt;&gt; fn() # 调用fn()与调用obj.power()是一样的81 小结 通过内置的一系列函数，我们可以对任意一个Python对象进行剖析，拿到其内部的数据。要注意的是，只有在不知道对象信息的时候，我们才会去获取对象信息。一个正确的用法的例子如下： 1234def readImage(fp): if hasattr(fp, 'read'): return readData(fp) return None 假设我们希望从文件流fp中读取图像，我们首先要判断该fp对象是否存在read方法，如果存在，则该对象是一个流，如果不存在，则无法读取。hasattr()就派上了用场。 请注意，在Python这类动态语言中，根据鸭子类型，有read()方法，不代表该fp对象就是一个文件流，它也可能是网络流，也可能是内存中的一个字节流，但只要read()方法返回的是有效的图像数据，就不影响读取图像的功能。 实例属性和类属性由于Python是动态语言，根据类创建的实例可以任意绑定属性。给实例绑定属性的方法是通过实例变量，或者通过self变量： 123456class Student(object): def __init__(self, name): self.name = names = Student('Bob')s.score = 90 但是，如果Student类本身需要绑定一个属性呢？可以直接在class中定义属性，这种属性是类属性，归Student类所有： 12class Student(object): name = 'Student' 当我们定义了一个类属性后，这个属性虽然归类所有，但类的所有实例都可以访问到。来测试一下： 12345678910111213141516&gt;&gt;&gt; class Student(object):... name = 'Student'...&gt;&gt;&gt; s = Student() # 创建实例s&gt;&gt;&gt; print(s.name) # 打印name属性，因为实例并没有name属性，所以会继续查找class的name属性Student&gt;&gt;&gt; print(Student.name) # 打印类的name属性Student&gt;&gt;&gt; s.name = 'Michael' # 给实例绑定name属性&gt;&gt;&gt; print(s.name) # 由于实例属性优先级比类属性高，因此，它会屏蔽掉类的name属性Michael&gt;&gt;&gt; print(Student.name) # 但是类属性并未消失，用Student.name仍然可以访问Student&gt;&gt;&gt; del s.name # 如果删除实例的name属性&gt;&gt;&gt; print(s.name) # 再次调用s.name，由于实例的name属性没有找到，类的name属性就显示出来了Student 从上面的例子可以看出，在编写程序的时候，千万不要对实例属性和类属性使用相同的名字，因为相同名称的实例属性将屏蔽掉类属性，但是当你删除实例属性后，再使用相同的名称，访问到的将是类属性。 面向对象高级编程使用slots正常情况下，当我们定义了一个class，创建了一个class的实例后，我们可以给该实例绑定任何属性和方法，这就是动态语言的灵活性。先定义class： 12class Student(object): pass 然后，尝试给实例绑定一个属性： 1234&gt;&gt;&gt; s = Student()&gt;&gt;&gt; s.name = 'Michael' # 动态给实例绑定一个属性&gt;&gt;&gt; print(s.name)Michael 还可以尝试给实例绑定一个方法： 12345678&gt;&gt;&gt; def set_age(self, age): # 定义一个函数作为实例方法... self.age = age...&gt;&gt;&gt; from types import MethodType&gt;&gt;&gt; s.set_age = MethodType(set_age, s) # 给实例绑定一个方法&gt;&gt;&gt; s.set_age(25) # 调用实例方法&gt;&gt;&gt; s.age # 测试结果25 但是，给一个实例绑定的方法，对另一个实例是不起作用的： 12345&gt;&gt;&gt; s2 = Student() # 创建新的实例&gt;&gt;&gt; s2.set_age(25) # 尝试调用方法Traceback (most recent call last): File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;AttributeError: 'Student' object has no attribute 'set_age' 为了给所有实例都绑定方法，可以给class绑定方法： 1234&gt;&gt;&gt; def set_score(self, score):... self.score = score...&gt;&gt;&gt; Student.set_score = set_score 给class绑定方法后，所有实例均可调用： 123456&gt;&gt;&gt; s.set_score(100)&gt;&gt;&gt; s.score100&gt;&gt;&gt; s2.set_score(99)&gt;&gt;&gt; s2.score99 通常情况下，上面的set_score方法可以直接定义在class中，但动态绑定允许我们在程序运行的过程中动态给class加上功能，这在静态语言中很难实现。 使用slots 但是，如果我们想要限制实例的属性怎么办？比如，只允许对Student实例添加name和age属性。为了达到限制的目的，Python允许在定义class的时候，定义一个特殊的__slots__变量，来限制该class实例能添加的属性： 12class Student(object): __slots__ = ('name', 'age') # 用tuple定义允许绑定的属性名称 然后，我们试试： 1234567&gt;&gt;&gt; s = Student() # 创建新的实例&gt;&gt;&gt; s.name = 'Michael' # 绑定属性'name'&gt;&gt;&gt; s.age = 25 # 绑定属性'age'&gt;&gt;&gt; s.score = 99 # 绑定属性'score'Traceback (most recent call last): File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;AttributeError: 'Student' object has no attribute 'score' 由于score没有被放到__slots__中，所以不能绑定score属性，试图绑定score将得到AttributeError的错误。 使用__slots__要注意，__slots__定义的属性仅对当前类实例起作用，对继承的子类是不起作用的： 12345&gt;&gt;&gt; class GraduateStudent(Student):... pass...&gt;&gt;&gt; g = GraduateStudent()&gt;&gt;&gt; g.score = 9999 除非在子类中也定义__slots__，这样，子类实例允许定义的属性就是自身的__slots__加上父类的__slots__。 使用@property在绑定属性时，如果我们直接把属性暴露出去，虽然写起来很简单，但是，没办法检查参数，导致可以把成绩随便改： 12s = Student()s.score = 9999 这显然不合逻辑。为了限制score的范围，可以通过一个set_score()方法来设置成绩，再通过一个get_score()来获取成绩，这样，在set_score()方法里，就可以检查参数： 1234567891011class Student(object): def get_score(self): return self._score def set_score(self, value): if not isinstance(value, int): raise ValueError('score must be an integer!') if value &lt; 0 or value &gt; 100: raise ValueError('score must between 0 ~ 100!') self._score = value 现在，对任意的Student实例进行操作，就不能随心所欲地设置score了： 12345678&gt;&gt;&gt; s = Student()&gt;&gt;&gt; s.set_score(60) # ok!&gt;&gt;&gt; s.get_score()60&gt;&gt;&gt; s.set_score(9999)Traceback (most recent call last): ...ValueError: score must between 0 ~ 100! 但是，上面的调用方法又略显复杂，没有直接用属性这么直接简单。 有没有既能检查参数，又可以用类似属性这样简单的方式来访问类的变量呢？对于追求完美的Python程序员来说，这是必须要做到的！ 还记得装饰器（decorator）可以给函数动态加上功能吗？对于类的方法，装饰器一样起作用。Python内置的@property装饰器就是负责把一个方法变成属性调用的： 12345678910111213class Student(object): @property def score(self): return self._score @score.setter def score(self, value): if not isinstance(value, int): raise ValueError('score must be an integer!') if value &lt; 0 or value &gt; 100: raise ValueError('score must between 0 ~ 100!') self._score = value @property的实现比较复杂，我们先考察如何使用。把一个getter方法变成属性，只需要加上@property就可以了，此时，@property本身又创建了另一个装饰器@score.setter，负责把一个setter方法变成属性赋值，于是，我们就拥有一个可控的属性操作： 12345678&gt;&gt;&gt; s = Student()&gt;&gt;&gt; s.score = 60 # OK，实际转化为s.set_score(60)&gt;&gt;&gt; s.score # OK，实际转化为s.get_score()60&gt;&gt;&gt; s.score = 9999Traceback (most recent call last): ...ValueError: score must between 0 ~ 100! 注意到这个神奇的@property，我们在对实例属性操作的时候，就知道该属性很可能不是直接暴露的，而是通过getter和setter方法来实现的。 还可以定义只读属性，只定义getter方法，不定义setter方法就是一个只读属性： 123456789101112class Student(object): @property def birth(self): return self._birth @birth.setter def birth(self, value): self._birth = value @property def age(self): return 2015 - self._birth 上面的birth是可读写属性，而age就是一个只读属性，因为age可以根据birth和当前时间计算出来。 多重继承通过多重继承，一个子类就可以同时获得多个父类的所有功能。 MixIn 在设计类的继承关系时，通常，主线都是单一继承下来的，但是，如果需要“混入”额外的功能，通过多重继承就可以实现。这种设计通常称之为MixIn。 MixIn的目的就是给一个类增加多个功能，这样，在设计类的时候，我们优先考虑通过多重继承来组合多个MixIn的功能，而不是设计多层次的复杂的继承关系。 Python自带的很多库也使用了MixIn。举个例子，Python自带了TCPServer和UDPServer这两类网络服务，而要同时服务多个用户就必须使用多进程或多线程模型，这两种模型由ForkingMixIn和ThreadingMixIn提供。通过组合，我们就可以创造出合适的服务来。 比如，编写一个多进程模式的TCP服务，定义如下： 12class MyTCPServer(TCPServer, ForkingMixIn): pass 编写一个多线程模式的UDP服务，定义如下： 12class MyUDPServer(UDPServer, ThreadingMixIn): pass 如果你打算搞一个更先进的协程模型，可以编写一个CoroutineMixIn： 12class MyTCPServer(TCPServer, CoroutineMixIn): pass 这样一来，我们不需要复杂而庞大的继承链，只要选择组合不同的类的功能，就可以快速构造出所需的子类。 由于Python允许使用多重继承，因此，MixIn就是一种常见的设计,只允许单一继承的语言（如Java）不能使用MixIn的设计。 定制类看到类似__slots__这种形如__xxx__的变量或者函数名就要注意，这些在Python中是有特殊用途的。 __slots__我们已经知道怎么用了，__len__()方法我们也知道是为了能让class作用于len()函数。 除此之外，Python的class中还有许多这样有特殊用途的函数，可以帮助我们 定制类。 str 我们先定义一个Student类，打印一个实例： 123456&gt;&gt;&gt; class Student(object):... def __init__(self, name):... self.name = name...&gt;&gt;&gt; print(Student('Michael'))&lt;__main__.Student object at 0x109afb190&gt; 怎么才能打印得好看呢？只需要定义好str()方法，返回一个好看的字符串就可以了： 12345678&gt;&gt;&gt; class Student(object):... def __init__(self, name):... self.name = name... def __str__(self):... return 'Student object (name: %s)' % self.name...&gt;&gt;&gt; print(Student('Michael'))Student object (name: Michael) 这样打印出来的实例，不但好看，而且容易看出实例内部重要的数据。但是细心的朋友会发现直接敲变量不用print，打印出来的实例还是不好看： 123&gt;&gt;&gt; s = Student('Michael')&gt;&gt;&gt; s&lt;__main__.Student object at 0x109afb310&gt; 这是因为直接显示变量调用的不是__str__()，而是__repr__()，两者的区别是__str__()返回用户看到的字符串，而__repr__()返回程序开发者看到的字符串，也就是说，__repr__()是为调试服务的。解决办法是再定义一个__repr__()。但是通常__str__()和__repr__()代码都是一样的，所以，有个偷懒的写法： 123456class Student(object): def __init__(self, name): self.name = name def __str__(self): return 'Student object (name=%s)' % self.name __repr__ = __str__ iter 如果一个类想被用于for ... in循环，类似list或tuple那样，就必须实现一个__iter__()方法，该方法返回一个 迭代对象，然后，Python的for循环就会不断调用该迭代对象的__next__()方法拿到循环的下一个值，直到遇到StopIteration错误时退出循环。 我们以斐波那契数列为例，写一个Fib类，可以作用于for循环： 123456789101112class Fib(object): def __init__(self): self.a, self.b = 0, 1 # 初始化两个计数器a，b def __iter__(self): return self # 实例本身就是迭代对象，故返回自己 def __next__(self): self.a, self.b = self.b, self.a + self.b # 计算下一个值 if self.a &gt; 100000: # 退出循环的条件 raise StopIteration() return self.a # 返回下一个值 then: 12for n in Fib(): print(n) getitem Fib实例虽然能作用于for循环，看起来和list有点像，但是，把它当成list来使用还是不行，比如，取第5个元素： 1234&gt;&gt;&gt; Fib()[5]Traceback (most recent call last): File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;TypeError: 'Fib' object does not support indexing 要表现得像list那样按照下标取出元素，需要实现__getitem__()方法： 123456class Fib(object): def __getitem__(self, n): a, b = 1, 1 for x in range(n): a, b = b, a + b return a 现在，就可以按下标访问数列的任意一项了： 123&gt;&gt;&gt; f = Fib()&gt;&gt;&gt; f[0]1 但是list有个神奇的切片方法： 12&gt;&gt;&gt; list(range(100))[5:10][5, 6, 7, 8, 9] 对于Fib却报错。原因是__getitem__()传入的参数可能是一个int，也可能是一个切片对象slice，所以要做判断： 12345678910111213141516171819class Fib(object): def __getitem__(self, n): if isinstance(n, int): # n是索引 a, b = 1, 1 for x in range(n): a, b = b, a + b return a if isinstance(n, slice): # n是切片 start = n.start stop = n.stop if start is None: start = 0 a, b = 1, 1 L = [] for x in range(stop): if x &gt;= start: L.append(a) a, b = b, a + b return L 现在试试Fib的切片,没有对step参数作处理, 也没有对负数作处理，所以，要正确实现一个__getitem__()还是有很多工作要做的。 12&gt;&gt;&gt; f[:10:2][1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89] 此外，如果把对象看成dict，__getitem__()的参数也可能是一个可以作key的object，例如str。 与之对应的是__setitem__()方法，把对象视作list或dict来对集合赋值。最后，还有一个__delitem__()方法，用于删除某个元素。 总之，通过上面的方法，我们自己定义的类表现得和Python自带的list、tuple、dict没什么区别，这完全归功于动态语言的“鸭子类型”，不需要强制继承某个接口。 getattr 正常情况下，当我们调用类的方法或属性时，如果不存在，就会报错。比如定义Student类： 1234class Student(object): def __init__(self): self.name = 'Michael' 调用name属性，没问题，但是，调用不存在的score属性，就有问题了： 1234567&gt;&gt;&gt; s = Student()&gt;&gt;&gt; print(s.name)Michael&gt;&gt;&gt; print(s.score)Traceback (most recent call last): ...AttributeError: 'Student' object has no attribute 'score' 错误信息很清楚地告诉我们，没有找到score这个attribute。 要避免这个错误，除了可以加上一个score属性外，Python还有另一个机制，那就是写一个__getattr__()方法，动态返回一个属性。修改如下： 12345678class Student(object): def __init__(self): self.name = 'Michael' def __getattr__(self, attr): if attr=='score': return 99 当调用不存在的属性时，比如score，Python解释器会试图调用__getattr__(self, &#39;score&#39;)来尝试获得属性，这样，我们就有机会返回score的值： 12345&gt;&gt;&gt; s = Student()&gt;&gt;&gt; s.name'Michael'&gt;&gt;&gt; s.score99 返回函数也是完全可以的： 12345class Student(object): def __getattr__(self, attr): if attr=='age': return lambda: 25 只是调用方式要变为： 12&gt;&gt;&gt; s.age()25 注意，只有在没有找到属性的情况下，才调用__getattr__，已有的属性，比如name，不会在__getattr__中查找。 此外，注意到任意调用如s.abc都会返回None，这是因为我们定义的__getattr__默认返回就是None。要让class只响应特定的几个属性，我们就要按照约定，抛出AttributeError的错误： 123456class Student(object): def __getattr__(self, attr): if attr=='age': return lambda: 25 raise AttributeError('\\'Student\\' object has no attribute \\'%s\\'' % attr) 这实际上可以把一个类的所有属性和方法调用全部动态化处理了，不需要任何特殊手段。 这种完全动态调用的特性有什么实际作用呢？作用就是，可以针对 完全动态 的情况作调用。 举个例子： 现在很多网站都搞REST API，比如新浪微博、豆瓣啥的，调用API的URL类似： http://api.server/user/friends http://api.server/user/timeline/list 如果要写SDK，给每个URL对应的API都写一个方法，那得累死，而且，API一旦改动，SDK也要改。 利用完全动态的__getattr__，我们可以写出一个 链式调用： 123456789101112class Chain(object): def __init__(self, path=''): self._path = path def __getattr__(self, path): return Chain('%s/%s' % (self._path, path)) def __str__(self): return self._path __repr__ = __str__ Try: 12&gt;&gt;&gt; Chain().status.user.timeline.list'/status/user/timeline/list' 这样，无论API怎么变，SDK都可以根据URL实现完全动态的调用，而且，不随API的增加而改变！(?) 还有些REST API会把参数放到URL中，比如GitHub的API： 1GET /users/:user/repos 调用时，需要把:user替换为实际用户名。如果我们能写出这样的链式调用： 1Chain().users('michael').repos 就可以非常方便地调用API了。 call 一个对象实例可以有自己的属性和方法，当我们调用实例方法时，我们用instance.method()来调用。能不能直接在实例本身上调用呢？在Python中，答案是肯定的。 任何类，只需要定义一个__call__()方法，就可以直接对实例进行调用。请看示例： 123456class Student(object): def __init__(self, name): self.name = name def __call__(self): print('My name is %s.' % self.name) 调用方式如下： 123&gt;&gt;&gt; s = Student('Michael')&gt;&gt;&gt; s() # self参数不要传入My name is Michael. __call__()还可以定义参数。对实例进行直接调用就好比对一个函数进行调用一样，所以你完全可以把对象看成函数，把函数看成对象，因为这两者之间本来就没啥根本的区别。 如果你把对象看成函数，那么函数本身其实也可以在运行期动态创建出来，因为类的实例都是运行期创建出来的，这么一来，我们就模糊了对象和函数的界限。 那么，怎么判断一个变量是对象还是函数呢？其实，更多的时候，我们需要判断一个对象是否能被调用，能被调用的对象就是一个Callable对象，比如函数和我们上面定义的带有__call__()的类实例： 12345678910&gt;&gt;&gt; callable(Student)True&gt;&gt;&gt; callable(max)True&gt;&gt;&gt; callable([1, 2, 3])False&gt;&gt;&gt; callable(None)False&gt;&gt;&gt; callable('str')False 通过callable()函数，我们就可以判断一个对象是否是“可调用”对象。 还有很多可定制的方法，请参考Python的官方文档。 使用枚举类当我们需要定义常量时，一个办法是用大写变量通过整数来定义，例如月份： JAN = 1FEB = 2MAR = 3…NOV = 11DEC = 12好处是简单，缺点是类型是int，并且仍然是变量。 更好的方法是为这样的枚举类型定义一个class类型，然后，每个常量都是class的一个唯一实例。Python提供了Enum类来实现这个功能： 123from enum import EnumMonth = Enum('Month', ('Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec')) 这样我们就获得了Month类型的枚举类，可以直接使用Month.Jan来引用一个常量，或者枚举它的所有成员： 12for name, member in Month.__members__.items(): print(name, '=&gt;', member, ',', member.value) value属性则是自动赋给成员的int常量，默认从1开始计数。 如果需要更精确地控制枚举类型，可以从Enum派生出自定义类： 1234567891011from enum import Enum, unique@uniqueclass Weekday(Enum): Sun = 0 # Sun的value被设定为0 Mon = 1 Tue = 2 Wed = 3 Thu = 4 Fri = 5 Sat = 6 @unique装饰器可以帮助我们检查保证没有重复值。 访问这些枚举类型可以有若干种方法： 12345678910111213141516171819202122232425262728293031&gt;&gt;&gt; day1 = Weekday.Mon&gt;&gt;&gt; print(day1)Weekday.Mon&gt;&gt;&gt; print(Weekday.Tue)Weekday.Tue&gt;&gt;&gt; print(Weekday['Tue'])Weekday.Tue&gt;&gt;&gt; print(Weekday.Tue.value)2&gt;&gt;&gt; print(day1 == Weekday.Mon)True&gt;&gt;&gt; print(day1 == Weekday.Tue)False&gt;&gt;&gt; print(Weekday(1))Weekday.Mon&gt;&gt;&gt; print(day1 == Weekday(1))True&gt;&gt;&gt; Weekday(7)Traceback (most recent call last): ...ValueError: 7 is not a valid Weekday&gt;&gt;&gt; for name, member in Weekday.__members__.items():... print(name, '=&gt;', member)...Sun =&gt; Weekday.SunMon =&gt; Weekday.MonTue =&gt; Weekday.TueWed =&gt; Weekday.WedThu =&gt; Weekday.ThuFri =&gt; Weekday.FriSat =&gt; Weekday.Sat 可见，既可以用成员名称引用枚举常量，又可以直接根据value的值获得枚举常量。 Test: 12345678910from enum import Enum, uniqueclass Gender(Enum): Male = 0 Female = 1class Student(object): def __init__(self, name, gender): self.name = name self.gender = gender 使用元类type() 动态语言和静态语言最大的不同，就是函数和类的定义，不是编译时定义的，而是运行时动态创建的。 比方说我们要定义一个Hello的class，就写一个hello.py模块： 123class Hello(object): def hello(self, name='world'): print('Hello, %s.' % name) 当Python解释器载入hello模块时，就会依次执行该模块的所有语句，执行结果就是动态创建出一个Hello的class对象，测试如下： 12345678&gt;&gt;&gt; from hello import Hello&gt;&gt;&gt; h = Hello()&gt;&gt;&gt; h.hello()Hello, world.&gt;&gt;&gt; print(type(Hello))&lt;class 'type'&gt;&gt;&gt;&gt; print(type(h))&lt;class 'hello.Hello'&gt; type()函数可以查看一个类型或变量的类型，Hello是一个class，它的类型就是type，而h是一个实例，它的类型就是class Hello。 我们说class的定义是运行时动态创建的，而创建class的方法就是使用type()函数。 type()函数既可以返回一个对象的类型，又可以创建出新的类型，比如，我们可以通过type()函数创建出Hello类，而无需通过class Hello(object)...的定义： 1234567891011&gt;&gt;&gt; def fn(self, name='world'): # 先定义函数 print('Hello, %s.' % name)&gt;&gt;&gt; Hello = type('Hello', (object,), dict(hello=fn)) # 创建Hello class&gt;&gt;&gt; h = Hello()&gt;&gt;&gt; h.hello()Hello, world.&gt;&gt;&gt; print(type(Hello))&lt;class 'type'&gt;&gt;&gt;&gt; print(type(h))&lt;class '__main__.Hello'&gt; 要创建一个class对象，type()函数依次传入3个参数： class的名称； 继承的父类集合，注意Python支持多重继承，如果只有一个父类，别忘了tuple的单元素写法； class的方法名称与函数绑定，这里我们把函数fn绑定到方法名hello上。 通过type()函数创建的类和直接写class是完全一样的，因为Python解释器遇到class定义时，仅仅是扫描一下class定义的语法，然后调用type()函数创建出class。 正常情况下，我们都用class Xxx...来定义类，但是，type()函数也允许我们动态创建出类来，也就是说，动态语言本身支持运行期动态创建类，这和静态语言有非常大的不同，要在静态语言运行期创建类，必须构造源代码字符串再调用编译器，或者借助一些工具生成字节码实现，本质上都是动态编译，会非常复杂。 metaclass 除了使用type()动态创建类以外，要控制类的创建行为，还可以使用metaclass直译为元类，简单的解释就是： 当我们定义了类以后，就可以根据这个类创建出实例，所以：先定义类，然后创建实例。 但是如果我们想创建出类呢？那就必须根据metaclass创建出类，所以：先定义metaclass，然后创建类。 连接起来就是：先定义metaclass，就可以创建类，最后创建实例。 所以，metaclass允许你创建类或者修改类。换句话说，你可以把类看成是metaclass创建出来的“实例”。 metaclass是Python面向对象里最难理解，也是最难使用的魔术代码。正常情况下，你不会碰到需要使用metaclass的情况，所以，暂且不谈。 错误、调试和测试Python内置了一套异常处理机制，来帮助我们进行错误处理。 此外，我们也需要跟踪程序的执行，查看变量的值是否正确，这个过程称为调试。Python的pdb可以让我们以单步方式执行代码。 最后，编写测试也很重要。有了良好的测试，就可以在程序修改后反复运行，确保程序输出符合我们编写的测试。 错误处理在程序运行的过程中，如果发生了错误，可以事先约定返回一个错误代码，这样，就可以知道是否有错，以及出错的原因。在操作系统提供的调用中，返回错误码非常常见。比如打开文件的函数open()，成功时返回文件描述符（就是一个整数），出错时返回-1。 用错误码来表示是否出错十分不便，因为函数本身应该返回的正常结果和错误码混在一起，造成调用者必须用大量的代码来判断是否出错： 12345678910111213def foo(): r = some_function() if r==(-1): return (-1) # do something return rdef bar(): r = foo() if r==(-1): print('Error') else: pass 一旦出错，还要一级一级上报，直到某个函数可以处理该错误（比如，给用户输出一个错误信息）。 所以高级语言通常都内置了一套try...except...finally...的错误处理机制，Python也不例外。 try 让我们用一个例子来看看try的机制： 123456789try: print('try...') r = 10 / 0 print('result:', r)except ZeroDivisionError as e: print('except:', e)finally: print('finally...')print('END') 当我们认为某些代码可能会出错时，就可以用try来运行这段代码，如果执行出错，则后续代码不会继续执行，而是直接跳转至错误处理代码，即except语句块，执行完except后，如果有finally语句块，则执行finally语句块，至此，执行完毕。 上面的代码在计算 $10 / 0$ 时会产生一个除法运算错误： 1234try...except: division by zerofinally...END 从输出可以看到，当错误发生时，后续语句print(&#39;result:&#39;, r)不会被执行，except由于捕获到ZeroDivisionError，因此被执行。最后，finally语句被执行。然后，程序继续按照流程往下走。 如果把除数0改成2，则执行结果如下： 1234try...result: 5finally...END finally如果有，则一定会被执行（可以没有finally语句）。 你还可以猜测，错误应该有很多种类，如果发生了不同类型的错误，应该由不同的except语句块处理。没错，可以有多个except来捕获不同类型的错误： 1234567891011try: print('try...') r = 10 / int('a') print('result:', r)except ValueError as e: print('ValueError:', e)except ZeroDivisionError as e: print('ZeroDivisionError:', e)finally: print('finally...')print('END') int()函数可能会抛出ValueError，所以我们用一个except捕获ValueError，用另一个except捕获ZeroDivisionError。 此外，如果没有错误发生，可以在except语句块后面加一个else，当没有错误发生时，会自动执行else语句： 12345678910111213try: print('try...') r = 10 / int('2') print('result:', r)except ValueError as e: print('ValueError:', e)except ZeroDivisionError as e: print('ZeroDivisionError:', e)else: print('no error!')finally: print('finally...')print('END') Python的错误其实也是class，所有的错误类型都继承自BaseException，所以在使用except时需要注意的是，它不但捕获该类型的错误，还把其子类也“一网打尽”。比如： 123456try: foo()except ValueError as e: print('ValueError')except UnicodeError as e: print('UnicodeError') 第二个except永远也捕获不到UnicodeError，因为UnicodeError是ValueError的子类，如果有，也被第一个except给捕获了。 Python所有的错误都是从BaseException类派生的，常见的错误类型和继承关系看这里：Exception 使用try...except捕获错误还有一个巨大的好处，就是可以跨越多层调用，比如函数main()调用foo()，foo()调用bar()，结果bar()出错了，这时，只要main()捕获到了，就可以处理： 12345678910111213def foo(s): return 10 / int(s)def bar(s): return foo(s) * 2def main(): try: bar('0') except Exception as e: print('Error:', e) finally: print('finally...') 也就是说，不需要在每个可能出错的地方去捕获错误，只要在合适的层次去捕获错误就可以了。这样一来，就大大减少了写try...except...finally的麻烦。 调用栈 如果错误没有被捕获，它就会一直往上抛，最后被Python解释器捕获，打印一个错误信息，然后程序退出。来看看err.py： 1234567891011# err.py:def foo(s): return 10 / int(s)def bar(s): return foo(s) * 2def main(): bar('0')main() 执行，结果如下： 1234567891011$ python3 err.pyTraceback (most recent call last): File \"err.py\", line 11, in &lt;module&gt; main() File \"err.py\", line 9, in main bar('0') File \"err.py\", line 6, in bar return foo(s) * 2 File \"err.py\", line 3, in foo return 10 / int(s)ZeroDivisionError: division by zero 出错并不可怕，可怕的是不知道哪里出错了。解读错误信息是定位错误的关键。我们从上往下可以看到整个错误的调用函数链： 错误信息第1行： 1Traceback (most recent call last): 告诉我们这是错误的跟踪信息。 第2~3行： 12File \"err.py\", line 11, in &lt;module&gt; main() 调用main()出错了，在代码文件err.py的第11行代码，但原因是第9行： 12File \"err.py\", line 9, in main bar('0') 调用bar(&#39;0&#39;)出错了，在代码文件err.py的第9行代码，但原因是第6行： 123 File \"err.py\", line 6, in bar return foo(s) * 2` 原因是return foo(s) * 2这个语句出错了，但这还不是最终原因，继续往下看： 12File \"err.py\", line 3, in foo return 10 / int(s) 原因是return 10 / int(s)这个语句出错了，这是错误产生的源头，因为下面打印了： 1ZeroDivisionError: integer division or modulo by zero 根据错误类型ZeroDivisionError，我们判断，int(s)本身并没有出错，但是int(s)返回0，在计算 $10 / 0$ 时出错，至此，找到错误源头。 记录错误 如果不捕获错误，自然可以让Python解释器来打印出错误堆栈，但程序也被结束了。既然我们能捕获错误，就可以把错误堆栈打印出来，然后分析错误原因，同时，让程序继续执行下去。 Python内置的logging模块可以非常容易地记录错误信息： 123456789101112131415161718# err_logging.pyimport loggingdef foo(s): return 10 / int(s)def bar(s): return foo(s) * 2def main(): try: bar('0') except Exception as e: logging.exception(e)main()print('END') 同样是出错，但程序打印完错误信息后会继续执行，并正常退出： 1234567891011$ python3 err_logging.pyERROR:root:division by zeroTraceback (most recent call last): File \"err_logging.py\", line 13, in main bar('0') File \"err_logging.py\", line 9, in bar return foo(s) * 2 File \"err_logging.py\", line 6, in foo return 10 / int(s)ZeroDivisionError: division by zeroEND 通过配置，logging还可以把错误记录到日志文件里，方便事后排查。 抛出错误 因为错误是class，捕获一个错误就是捕获到该class的一个实例。因此，错误并不是凭空产生的，而是有意创建并抛出的。Python的内置函数会抛出很多类型的错误，我们自己编写的函数也可以抛出错误。 如果要抛出错误，首先根据需要，可以定义一个错误的class，选择好继承关系，然后，用raise语句抛出一个错误的实例： 1234567891011# err_raise.pyclass FooError(ValueError): passdef foo(s): n = int(s) if n==0: raise FooError('invalid value: %s' % s) return 10 / nfoo('0') 执行，可以最后跟踪到我们自己定义的错误： 1234567$ python3 err_raise.pyTraceback (most recent call last): File \"err_throw.py\", line 11, in &lt;module&gt; foo('0') File \"err_throw.py\", line 8, in foo raise FooError('invalid value: %s' % s)__main__.FooError: invalid value: 0 只有在必要的时候才定义我们自己的错误类型。如果可以选择Python已有的内置的错误类型（比如ValueError，TypeError），尽量使用Python内置的错误类型。 最后，我们来看另一种错误处理的方式： 12345678910111213141516# err_reraise.pydef foo(s): n = int(s) if n==0: raise ValueError('invalid value: %s' % s) return 10 / ndef bar(): try: foo('0') except ValueError as e: print('ValueError!') raisebar() 在bar()函数中，我们明明已经捕获了错误，但是，打印一个ValueError!后，又把错误通过raise语句抛出去了，这不有病么？ 其实这种错误处理方式不但没病，而且相当常见。捕获错误目的只是记录一下，便于后续追踪。但是，由于当前函数不知道应该怎么处理该错误，所以，最恰当的方式是继续往上抛，让顶层调用者去处理。好比一个员工处理不了一个问题时，就把问题抛给他的老板，如果他的老板也处理不了，就一直往上抛，最终会抛给CEO去处理。 raise语句如果不带参数，就会把当前错误原样抛出。此外，在except中raise一个Error，还可以把一种类型的错误转化成另一种类型： 1234try: 10 / 0except ZeroDivisionError: raise ValueError('input error!') 只要是合理的转换逻辑就可以，但是，决不应该把一个IOError转换成毫不相干的ValueError。 调试print() 简单粗暴 断言 凡是用print()来辅助查看的地方，都可以用断言（assert）来替代： 1234567def foo(s): n = int(s) assert n != 0, 'n is zero!' return 10 / ndef main(): foo('0') assert的意思是，表达式n != 0应该是True，否则，根据程序运行的逻辑，后面的代码肯定会出错。 如果断言失败，assert语句本身就会抛出AssertionError： 1234$ python err.pyTraceback (most recent call last): ...AssertionError: n is zero! 程序中如果到处充斥着assert，和print()相比也好不到哪去。不过，启动Python解释器时可以用-O参数来关闭assert： 1234$ python -O err.pyTraceback (most recent call last): ...ZeroDivisionError: division by zero 关闭后，你可以把所有的assert语句当成pass来看。 logging 把print()替换为logging是第3种方式，和assert比，logging不会抛出错误，而且可以输出到文件： 123456import loggings = '0'n = int(s)logging.info('n = %d' % n)print(10 / n) logging.info()就可以输出一段文本。运行，发现除了ZeroDivisionError，没有任何信息。怎么回事？ 别急，在import logging之后添加一行配置再试试： 12345678910import logginglogging.basicConfig(level=logging.INFO)看到输出了：$ python err.pyINFO:root:n = 0Traceback (most recent call last): File \"err.py\", line 8, in &lt;module&gt; print(10 / n)ZeroDivisionError: division by zero 这就是logging的好处，它允许你指定记录信息的级别，有debug，info，warning，error等几个级别，当我们指定level=INFO时，logging.debug就不起作用了。同理，指定level=WARNING后，debug和info就不起作用了。这样一来，你可以放心地输出不同级别的信息，也不用删除，最后统一控制输出哪个级别的信息。 logging的另一个好处是通过简单的配置，一条语句可以同时输出到不同的地方，比如console和文件。 pdb 第4种方式是启动Python的调试器pdb，让程序以单步方式运行，可以随时查看运行状态。我们先准备好程序： 1234# err.pys = '0'n = int(s)print(10 / n) 然后启动： 123$ python -m pdb err.py&gt; /Users/michael/Github/learn-python3/samples/debug/err.py(2)&lt;module&gt;()-&gt; s = '0' 以参数-m pdb启动后，pdb定位到下一步要执行的代码-&gt; s = &#39;0&#39;。输入命令l来查看代码： 12345(Pdb) l 1 # err.py 2 -&gt; s = '0' 3 n = int(s) 4 print(10 / n) 输入命令n可以单步执行代码： 123456(Pdb) n&gt; /Users/michael/Github/learn-python3/samples/debug/err.py(3)&lt;module&gt;()-&gt; n = int(s)(Pdb) n&gt; /Users/michael/Github/learn-python3/samples/debug/err.py(4)&lt;module&gt;()-&gt; print(10 / n) 任何时候都可以输入命令p 变量名来查看变量： 1234(Pdb) p s'0'(Pdb) p n0 输入命令q结束调试，退出程序,这种通过pdb在命令行调试的方法理论上是万能的，但实在是太麻烦了，如果有一千行代码，要运行到第999行得敲多少命令啊。还好，我们还有另一种调试方法。 pdb.set_trace() 这个方法也是用pdb，但是不需要单步执行，我们只需要import pdb，然后，在可能出错的地方放一个pdb.set_trace()，就可以设置一个断点： 1234567# err.pyimport pdbs = '0'n = int(s)pdb.set_trace() # 运行到这里会自动暂停print(10 / n) 运行代码，程序会自动在pdb.set_trace()暂停并进入pdb调试环境，可以用命令p查看变量，或者用命令c继续运行： 12345678910$ python err.py&gt; /Users/michael/Github/learn-python3/samples/debug/err.py(7)&lt;module&gt;()-&gt; print(10 / n)(Pdb) p n0(Pdb) cTraceback (most recent call last): File \"err.py\", line 7, in &lt;module&gt; print(10 / n)ZeroDivisionError: division by zero IDE 如果要比较爽地设置断点、单步执行，就需要一个支持调试功能的IDE。目前比较好的Python IDE有： Visual Studio Code：https://code.visualstudio.com/，需要安装Python插件。 PyCharm：http://www.jetbrains.com/pycharm/ 另外，Eclipse加上pydev插件也可以调试Python程序。虽然用IDE调试起来比较方便，但是最后你会发现，logging才是终极武器。 单元测试如果你听说过“测试驱动开发”（TDD：Test-Driven Development），单元测试就不陌生。 单元测试是用来对一个模块、一个函数或者一个类来进行正确性检验的测试工作。 比如对函数abs()，我们可以编写出以下几个测试用例： 输入正数，比如1、1.2、0.99，期待返回值与输入相同； 输入负数，比如-1、-1.2、-0.99，期待返回值与输入相反； 输入0，期待返回0； 输入非数值类型，比如None、[]、{}，期待抛出TypeError。 把上面的测试用例放到一个测试模块里，就是一个完整的单元测试。 如果单元测试通过，说明我们测试的这个函数能够正常工作。如果单元测试不通过，要么函数有bug，要么测试条件输入不正确，总之，需要修复使单元测试能够通过。 单元测试通过后有什么意义呢？如果我们对abs()函数代码做了修改，只需要再跑一遍单元测试，如果通过，说明我们的修改不会对abs()函数原有的行为造成影响，如果测试不通过，说明我们的修改与原有行为不一致，要么修改代码，要么修改测试。 这种以测试为驱动的开发模式最大的好处就是确保一个程序模块的行为符合我们设计的测试用例。在将来修改的时候，可以极大程度地保证该模块行为仍然是正确的。 我们来编写一个Dict类，这个类的行为和dict一致，但是可以通过属性来访问，用起来就像下面这样： 12345&gt;&gt;&gt; d = Dict(a=1, b=2)&gt;&gt;&gt; d['a']1&gt;&gt;&gt; d.a1 mydict.py代码如下： 12345678910111213class Dict(dict): def __init__(self, **kw): super().__init__(**kw) def __getattr__(self, key): try: return self[key] except KeyError: raise AttributeError(r\"'Dict' object has no attribute '%s'\" % key) def __setattr__(self, key, value): self[key] = value 为了编写单元测试，我们需要引入Python自带的unittest模块，编写mydict_test.py如下： 1234567891011121314151617181920212223242526272829303132import unittestfrom mydict import Dictclass TestDict(unittest.TestCase): def test_init(self): d = Dict(a=1, b='test') self.assertEqual(d.a, 1) self.assertEqual(d.b, 'test') self.assertTrue(isinstance(d, dict)) def test_key(self): d = Dict() d['key'] = 'value' self.assertEqual(d.key, 'value') def test_attr(self): d = Dict() d.key = 'value' self.assertTrue('key' in d) self.assertEqual(d['key'], 'value') def test_keyerror(self): d = Dict() with self.assertRaises(KeyError): value = d['empty'] def test_attrerror(self): d = Dict() with self.assertRaises(AttributeError): value = d.empty 编写单元测试时，我们需要编写一个测试类，从unittest.TestCase继承。 以test开头的方法就是测试方法，不以test开头的方法不被认为是测试方法，测试的时候不会被执行。 对每一类测试都需要编写一个test_xxx()方法。由于unittest.TestCase提供了很多内置的条件判断，我们只需要调用这些方法就可以断言输出是否是我们所期望的。最常用的断言就是assertEqual()： 1self.assertEqual(abs(-1), 1) # 断言函数返回的结果与1相等 另一种重要的断言就是期待抛出指定类型的Error，比如通过d[&#39;empty&#39;]访问不存在的key时，断言会抛出KeyError： 12with self.assertRaises(KeyError): value = d['empty'] 而通过d.empty访问不存在的key时，我们期待抛出AttributeError： 12with self.assertRaises(AttributeError): value = d.empty 运行单元测试 一旦编写好单元测试，我们就可以运行单元测试。最简单的运行方式是在mydict_test.py的最后加上两行代码： 12if __name__ == '__main__': unittest.main() 这样就可以把mydict_test.py当做正常的python脚本运行： 1$ python mydict_test.py 另一种方法是在命令行通过参数-m unittest直接运行单元测试： 123456$ python -m unittest mydict_test.....----------------------------------------------------------------------Ran 5 tests in 0.000sOK 这是推荐的做法，因为这样可以一次批量运行很多单元测试，并且，有很多工具可以自动来运行这些单元测试。 setUp与tearDown 可以在单元测试中编写两个特殊的setUp()和tearDown()方法。这两个方法会分别在每调用一个测试方法的前后分别被执行。 setUp()和tearDown()方法有什么用呢？设想你的测试需要启动一个数据库，这时，就可以在setUp()方法中连接数据库，在tearDown()方法中关闭数据库，这样，不必在每个测试方法中重复相同的代码： 1234567class TestDict(unittest.TestCase): def setUp(self): print('setUp...') def tearDown(self): print('tearDown...') 可以再次运行测试看看每个测试方法调用前后是否会打印出setUp...和tearDown...。 文档测试如果你经常阅读Python的官方文档，可以看到很多文档都有示例代码。比如re模块就带了很多示例代码： 1234&gt;&gt;&gt; import re&gt;&gt;&gt; m = re.search('(?&lt;=abc)def', 'abcdef')&gt;&gt;&gt; m.group(0)'def' 可以把这些示例代码在Python的交互式环境下输入并执行，结果与文档中的示例代码显示的一致。 这些代码与其他说明可以写在注释中，然后，由一些工具来自动生成文档。既然这些代码本身就可以粘贴出来直接运行，那么，可不可以自动执行写在注释中的这些代码呢？ 答案是肯定的。 当我们编写注释时，如果写上这样的注释： 1234567891011121314def abs(n): ''' Function to get absolute value of number. Example: &gt;&gt;&gt; abs(1) 1 &gt;&gt;&gt; abs(-1) 1 &gt;&gt;&gt; abs(0) 0 ''' return n if n &gt;= 0 else (-n) 无疑更明确地告诉函数的调用者该函数的期望输入和输出。 并且，Python内置的“文档测试”（doctest）模块可以直接提取注释中的代码并执行测试。 doctest严格按照Python交互式命令行的输入和输出来判断测试结果是否正确。只有测试异常的时候，可以用...表示中间一大段烦人的输出。 让我们用doctest来测试上次编写的Dict类： 123456789101112131415161718192021222324252627282930313233343536373839# mydict2.pyclass Dict(dict): ''' Simple dict but also support access as x.y style. &gt;&gt;&gt; d1 = Dict() &gt;&gt;&gt; d1['x'] = 100 &gt;&gt;&gt; d1.x 100 &gt;&gt;&gt; d1.y = 200 &gt;&gt;&gt; d1['y'] 200 &gt;&gt;&gt; d2 = Dict(a=1, b=2, c='3') &gt;&gt;&gt; d2.c '3' &gt;&gt;&gt; d2['empty'] Traceback (most recent call last): ... KeyError: 'empty' &gt;&gt;&gt; d2.empty Traceback (most recent call last): ... AttributeError: 'Dict' object has no attribute 'empty' ''' def __init__(self, **kw): super(Dict, self).__init__(**kw) def __getattr__(self, key): try: return self[key] except KeyError: raise AttributeError(r\"'Dict' object has no attribute '%s'\" % key) def __setattr__(self, key, value): self[key] = valueif __name__=='__main__': import doctest doctest.testmod() 运行python mydict2.py： 1$ python mydict2.py 什么输出也没有。这说明我们编写的doctest运行都是正确的。如果程序有问题，比如把__getattr__()方法注释掉，再运行就会报错： 123456789101112131415161718192021$ python mydict2.py**********************************************************************File \"/Users/michael/Github/learn-python3/samples/debug/mydict2.py\", line 10, in __main__.DictFailed example: d1.xException raised: Traceback (most recent call last): ... AttributeError: 'Dict' object has no attribute 'x'**********************************************************************File \"/Users/michael/Github/learn-python3/samples/debug/mydict2.py\", line 16, in __main__.DictFailed example: d2.cException raised: Traceback (most recent call last): ... AttributeError: 'Dict' object has no attribute 'c'**********************************************************************1 items had failures: 2 of 9 in __main__.Dict***Test Failed*** 2 failures. 注意到最后3行代码。当模块正常导入时，doctest不会被执行。只有在命令行直接运行时，才执行doctest。所以，不必担心doctest会在非测试环境下执行。 (NEXT) ^-^","tags":[{"name":"Python","slug":"Python","permalink":"http://wenbo.fun/tags/Python/"}]},{"title":"ML_Foundation","date":"2018-01-26T05:39:12.000Z","path":"2018/01/26/MLforward/","text":"Linear AlgebraPoint Scalar、Vector、Tensor Identity Matrix、$A^T$ (transpose of A) Matrix-Vector Product Span、Generate Space、Subspace、Eigenspace Linear Dependent、Rank $[A|b]$ (Augmented Matrix) Matrix Multiplication Inverse of a Matrix Basis Eigenvalue &amp; Eigenvector Characteristic Polynomial、Similar Matrix Diagonalizable Norm、Orthogonality 概率论、信息论数值计算","tags":[{"name":"Other","slug":"Other","permalink":"http://wenbo.fun/tags/Other/"}]},{"title":"Python教程笔记(1/4)","date":"2018-01-10T07:49:18.000Z","path":"2018/01/10/myPython/","text":"Python Python 3 From Python基础Python使用缩进来组织代码块，请务必遵守约定俗成的习惯，坚持使用4个空格的缩进。在文本编辑器中，需要设置把Tab自动转换为4个空格，确保不混用Tab和空格。 数据类型和变量整数十六进制用0x前缀和0-9，a-f表示。 注: Python的整数没有大小限制，而某些语言的整数根据其存储长度是有大小限制的，例如Java对32位整数的范围限制在-2147483648~2147483647。 Python的浮点数也没有大小限制，但是超出一定范围就直接表示为inf（无限大） Floating point numbers are usually implemented using double in C. ——Python官方文档 浮点数(小数)科学计数法: 0.000012可以写成1.2e-5字符串字符串是以单引号’或双引号”括起来的任意文本 转义字符\\,如果字符串里面有很多字符都需要转义，就需要加很多\\，为了简化，Python还允许用r&#39;&#39;表示’’内部的字符串默认不转义. 1234&gt;&gt;&gt; print('\\\\\\t\\\\')\\ \\&gt;&gt;&gt; print(r'\\\\\\t\\\\')\\\\\\t\\\\ 布尔值 True、False 布尔值可以用and、or和not运算 空值 空值是Python里一个特殊的值，用None表示。 $ None /neq 0 $ 变量变量不仅可以是数字，还可以是任意数据类型。 在Python中，可以把任意数据类型赋值给变量，同一个变量可以反复赋值，而且可以是不同类型的变量。这种变量本身类型不固定的语言称之为 动态语言。 注:123a = &apos;ABC&apos;b = aa = &apos;XYZ&apos; 执行b = a，解释器创建了变量b，并把b指向a指向的字符串’ABC’,a改变值时b不改变。 常量在Python中，通常用全部大写的变量名表示常量，如: PI 在Python中，有两种除法，一种除法是/,还有一种除法是//，称为地板除，只取结果的整数部分。 字符串和编码字符编码ASCII 编码GB2312 编码(中文编)Unicode 把所有语言都统一到一套编码里，这样就不会再有乱码问题了。 ASCII编码和Unicode编码的区别：ASCII编码是1个字节，而Unicode编码通常是2个字节。如果你写的文本基本上全部是英文的话，用Unicode编码比ASCII编码需要多一倍的存储空间。所以，本着节约的精神，又出现了把Unicode编码转化为“可变长编码”的UTF-8编码。 计算机系统通用的字符编码工作方式：在计算机内存中，统一使用Unicode编码，当需要保存到硬盘或者需要传输的时候，就转换为UTF-8编码。用记事本编辑的时候，从文件读取的UTF-8字符被转换为Unicode字符到内存里，编辑完成后，保存的时候再把Unicode转换为UTF-8保存到文件。 浏览网页的时候，服务器会把动态生成的Unicode内容转换为UTF-8再传输到浏览器。 Python的字符串字符串是以Unicode编码的。对于单个字符的编码，Python提供了ord()函数获取字符的整数表示，chr()函数把编码转换为对应的字符： 12345678&gt;&gt;&gt; ord('A')65&gt;&gt;&gt; ord('中')20013&gt;&gt;&gt; chr(66)'B'&gt;&gt;&gt; chr(25991)'文' 如果知道字符的整数编码，还可以用十六进制这么写str： 12&gt;&gt;&gt; '\\u4e2d\\u6587''中文' 由于Python的字符串类型是str，在内存中以Unicode表示，一个字符对应若干个字节。如果要在网络上传输，或者保存到磁盘上，就需要把str变为以字节为单位的bytes,一个字符都只占用一个字节。Python对bytes类型的数据用带b前缀的单引号或双引号表示： 1x = b'ABC' 以Unicode表示的str通过encode()方法可以编码为指定的bytes，例如： 1234&gt;&gt;&gt; 'ABC'.encode('ascii')b'ABC'&gt;&gt;&gt; '中文'.encode('utf-8')b'\\xe4\\xb8\\xad\\xe6\\x96\\x87' 在bytes中，无法显示为ASCII字符的字节，用\\x##显示。 反过来，如果我们从网络或磁盘上读取了字节流，那么读到的数据就是bytes。要把bytes变为str，就需要用decode()方法： 1234&gt;&gt;&gt; b'ABC'.decode('ascii')'ABC'&gt;&gt;&gt; b'\\xe4\\xb8\\xad\\xe6\\x96\\x87'.decode('utf-8')'中文' 如果bytes中包含无法解码的字节，decode()方法会报错,如果bytes中只有一小部分无效的字节，可以传入errors=&#39;ignore&#39;忽略错误的字节： 12&gt;&gt;&gt; b'\\xe4\\xb8\\xad\\xff'.decode('utf-8', errors='ignore')'中' 要计算str包含多少个字符，可以用len()函数,如果换成bytes，len()函数就计算字节数： 123456&gt;&gt;&gt; len('ABC')3&gt;&gt;&gt; len(b'\\xe4\\xb8\\xad\\xe6\\x96\\x87')6&gt;&gt;&gt; len('中文'.encode('utf-8'))6 当Python解释器读取源代码时，为了让它按UTF-8编码读取，我们通常在文件开头写上这两行： 12#!/usr/bin/env python3# -*- coding: utf-8 -*- 第一行注释是为了告诉Linux/OS X系统，这是一个Python可执行程序，Windows系统会忽略这个注释；第二行注释是为了告诉Python解释器，按照UTF-8编码读取源代码，否则，你在源代码中写的中文输出可能会有乱码。注: 申明了UTF-8编码并不意味着你的.py文件就是UTF-8编码的，你还要确保文本编辑器正在使用UTF-8 without BOM编码。 格式化和C语言是一致常见的占位符有： 占位符 替换内容 %d 整数 %f 浮点数 %s 字符串 %x 十六进制整数 %2d 格式化整数 格式化整数和浮点数还可以指定是否补0和整数与小数的位数： 12345678&gt;&gt;&gt; print('%02d' % 45)45&gt;&gt;&gt; print('%03d' % 45)045&gt;&gt;&gt; print('%.2f' % 3.1415926)3.14&gt;&gt;&gt; print('%05.2f' % 3.1415926)03.14 format()另一种格式化字符串的方法是使用字符串的format()方法,它会用传入的参数依次替换字符串内的占位符{0}、{1}……，不过这种方式写起来比%要麻烦得多： 12&gt;&gt;&gt; '&#123;0&#125;成绩提升了 &#123;1:.2f&#125;%'.format('小明', 17.125)'小明成绩提升了 17.1%' list和tuplelistPython内置的一种数据类型。list是一种有序的集合，可以随时添加和删除其中的元素,l里面的元素的数据类型可以不同。用len()函数可以获得list元素的个数。用索引来访问与其他语言类似。 12345classmates = ['Michael', 'Bob', 'Tracy']&gt;&gt;&gt; classmates['Michael', 'Bob', 'Tracy']&gt;&gt;&gt; len(classmates)3 可以用-1做索引，直接获取最后一个元素,以此类推，可以获取倒数第2个、倒数第3个： 1234&gt;&gt;&gt; classmates[-1]'Tracy'&gt;&gt;&gt; classmates[-3]'Michael' list是一个可变的有序表，可以用append追加元素到末尾： 1&gt;&gt;&gt; classmates.append('Adam') insert插入到指定的位置: 1&gt;&gt;&gt; classmates.insert(1, 'Jack') 删除指定位置的元素，用pop(i)方法删除list末尾的元素，用pop()方法： 12&gt;&gt;&gt; classmates.pop()'Adam' list元素也可以是另一个list: 123&gt;&gt;&gt; s = ['python', 'java', ['asp', 'php'], 'scheme']&gt;&gt;&gt; len(s)4 要拿到 php 可以写s[2][1]，因此s可以看成是一个二维数组 tupletuple和list非常类似，但是tuple一旦初始化就不能修改,用的是()。 1&gt;&gt;&gt; classmates = ('Michael', 'Bob', 'Tracy') 现在，classmates这个tuple不能变了，它也没有append()，insert()这样的方法。其他获取元素的方法和list是一样的，你可以正常地使用classmates[0]，classmates[-1]，但不能赋值成另外的元素。 因为tuple不可变，所以代码更安全。如果可能，能用tuple代替list就尽量用tuple。 括号()既可以表示tuple，又可以表示数学公式中的小括号，这就产生了歧义，因此，只有1个元素的tuple定义时必须加一个逗号,，来消除歧义： 123&gt;&gt;&gt; t = (1,)&gt;&gt;&gt; t(1,) 而 t = (1) 代表t是1这个数。 tuple中可含有可变的list。12345&gt;&gt;&gt; t = ('a', 'b', ['A', 'B'])&gt;&gt;&gt; t[2][0] = 'X'&gt;&gt;&gt; t[2][1] = 'Y'&gt;&gt;&gt; t('a', 'b', ['X', 'Y']) tuple指向变化: tuple一开始指向的list并没有改成别的list。这就是指向不变。 判断与循环条件判断if-else: 循环for: 1234sum = 0for x in range(101): sum = sum + xprint(sum) while:12while n &gt; 0: n++ break and continue:continue跳过当前的这次循环，直接开始下一次循环。 使用dict和setDictPython内置了字典：dict的支持，dict全称dictionary，在其他语言中也称为map，使用键-值（key-value）存储，具有极快的查找速度。ex: 123&gt;&gt;&gt; d = &#123;'Michael': 95, 'Bob': 75, 'Tracy': 85&#125;&gt;&gt;&gt; d['Michael']95 通过key放入数据： 123&gt;&gt;&gt; d['Adam'] = 67&gt;&gt;&gt; d['Adam']67 要避免key不存在的错误，有两种办法，一是通过in判断key是否存在： 12&gt;&gt;&gt; 'Thomas' in dFalse 二是通过dict提供的get()方法，如果key不存在，可以返回None，或者自己指定的value： 123&gt;&gt;&gt; d.get('Thomas')&gt;&gt;&gt; d.get('Thomas', -1)-1 注：dict内部存放的顺序和key放入的顺序是没有关系的,dict是用空间来换取时间的一种方法。 dict可以用在需要高速查找的很多地方，在Python代码中几乎无处不在，正确使用dict非常重要，需要牢记的第一条就是dict的key必须是不可变对象。通过key计算位置的算法称为哈希算法（Hash）。 要保证hash的正确性，作为key的对象就不能变。在Python中，字符串、整数等都是不可变的，因此，可以放心地作为key。而list是可变的，就不能作为key,tuple可以作为key。 Setset是一组key的集合，但不存储value。要创建一个set，需要提供一个list(tuple也行)作为输入集合,重复元素在set中自动被过滤： 123&gt;&gt;&gt; s = set([1, 1, 2, 2, 4, 3, 3, 1, 2])&gt;&gt;&gt; s&#123;1, 2, 3, 4&#125; 通过add(key)方法可以添加元素到set中: 123&gt;&gt;&gt; s.add(0)&gt;&gt;&gt; s&#123;0, 1, 2, 3, 4&#125; remove(key)方法可以删除元素： 123&gt;&gt;&gt; s.remove(4)&gt;&gt;&gt; s&#123;0, 1, 2, 3&#125; set可以看成数学意义上的无序、无重复元素的集合，因此，两个set可以做数学意义上的交集、并集等操作： 123456&gt;&gt;&gt; s1 = set([1, 2, 3])&gt;&gt;&gt; s2 = set([2, 3, 4])&gt;&gt;&gt; s1 &amp; s2&#123;2, 3&#125;&gt;&gt;&gt; s1 | s2&#123;1, 2, 3, 4&#125; 试试不变对象tuple: 12345678&gt;&gt;&gt; b = set ([(2,3),4])&gt;&gt;&gt; b&#123;(2, 3), 4&#125;&gt;&gt;&gt; b = set ([(2,3),[4,5]])Traceback (most recent call last): File \"&lt;pyshell#64&gt;\", line 1, in &lt;module&gt; b = set ([(2,3),[4,5]])TypeError: unhashable type: 'list' 函数内置函数 Built-in Functions 定义函数定义一个函数要使用def语句，依次写出函数名、括号、括号中的参数和冒号: 12345def my_abs(x): if x &gt;= 0: return x else: return -x 如果你已经把my_abs()的函数定义保存为abstest.py文件了，那么，可以在该文件的当前目录下启动Python解释器，用from abstest import my_abs来导入my_abs()函数，注意abstest是文件名（不含.py扩展名）： 123&gt;&gt;&gt; from abstest import my_abs &gt;&gt;&gt; my_abs(-9) 9 空函数如果想定义一个什么事也不做的空函数，可以用pass语句： 12def nop(): pass pass可以用来作为占位符，比如现在还没想好怎么写函数的代码，就可以先放一个pass，让代码能运行起来。 12if age &gt;= 18: pass 缺少了pass，代码运行就会有语法错误。 返回多个值 123456import mathdef move(x, y, step, angle=0): nx = x + step * math.cos(angle) ny = y - step * math.sin(angle) return nx, ny Python的函数返回多值其实就是返回一个tuple，多个变量可以同时接收一个tuple,在语法上，返回一个tuple可以省略括号。 123456&gt;&gt;&gt; x, y = move(100, 100, 60, math.pi / 6)&gt;&gt;&gt; print(x, y)151.96152422706632 70.0&gt;&gt;&gt;move(100, 100, 60, math.pi / 6)(151.96152422706632, 70.0) 函数的参数Python的函数定义非常简单，但灵活度却非常大。除了正常定义的必选参数外，还可以使用默认参数、可变参数和关键字参数，使得函数定义出来的接口，不但能处理复杂的参数，还可以简化调用者的代码。 位置参数写一个计算$x^n$函数: 123456def power(x, n): s = 1 while n &gt; 0: n = n - 1 s = s * x return s power(x, n) 函数有两个参数：x和n，这两个参数都是位置参数，调用函数时，传入的两个值按照位置顺序依次赋给参数x和n。 默认参数 由于我们经常计算x2，所以，完全可以把第二个参数n的默认值设定为2： 123456def power(x, n=2): s = 1 while n &gt; 0: n = n - 1 s = s * x return s 这样，当我们调用power(5)时，相当于调用power(5, 2)： 1234&gt;&gt;&gt; power(5)25&gt;&gt;&gt; power(5, 2)25 设置默认参数时，有几点要注意： 必选参数在前，默认参数在后，否则Python的解释器会报错。 当函数有多个参数时，把变化大的参数放前面，变化小的参数放后面。变化小的参数就可以作为默认参数。 Python函数在定义的时候，默认参数L的值就被计算出来了，即[].要注意如下情况: 123def add_end(L=[]): L.append('END') return L 用 [] 作默认参数进行调用: 12&gt;&gt;&gt; add_end()['END'] 第一次调用没毛病，但是: 1234&gt;&gt;&gt; add_end()['END', 'END']&gt;&gt;&gt; add_end()['END', 'END', 'END'] 可以用None这个不变对象来实现： 12345def add_end(L=None): if L is None: L = [] L.append('END') return L 可变参数定义一个list或tuple参数： 12345def calc(numbers): sum = 0 for n in numbers: sum = sum + n * n return sum 调用: 1234&gt;&gt;&gt; calc([1, 2, 3])14&gt;&gt;&gt; calc((1, 3, 5, 7))84 定义可变参数: 12345def calc(*numbers): sum = 0 for n in numbers: sum = sum + n * n return sum 调用: 1234&gt;&gt;&gt; calc(1, 2, 3)14&gt;&gt;&gt; calc(1, 3, 5, 7)84 Python允许你在list或tuple前面加一个*号，把list或tuple的元素变成可变参数传进去： 123&gt;&gt;&gt; nums = [1, 2, 3]&gt;&gt;&gt; calc(*nums)14 关键字参数可变参数允许你传入0个或任意个参数，这些可变参数在函数调用时自动组装为一个tuple。而关键字参数允许你传入0个或任意个含参数名的参数，这些关键字参数在函数内部自动组装为一个dict。请看示例： 1函数person除了必选参数name和age外，还接受关键字参数kw。在调用该函数时，可以只传入必选参数： 函数person除了必选参数name和age外，还接受关键字参数kw。在调用该函数时，可以只传入必选参数,也可以传入任意个数的关键字参数： 123456&gt;&gt;&gt; person('Michael', 30)name: Michael age: 30 other: &#123;&#125;&gt;&gt;&gt; person('Bob', 35, city='Beijing')name: Bob age: 35 other: &#123;'city': 'Beijing'&#125;&gt;&gt;&gt; person('Adam', 45, gender='M', job='Engineer')name: Adam age: 45 other: &#123;'gender': 'M', 'job': 'Engineer'&#125; 和可变参数类似，也可以先组装出一个dict，然后，把该dict转换为关键字参数传进去： 123&gt;&gt;&gt; extra = &#123;'city': 'Beijing', 'job': 'Engineer'&#125;&gt;&gt;&gt; person('Jack', 24, **extra)name: Jack age: 24 other: &#123;'city': 'Beijing', 'job': 'Engineer'&#125; 命名关键字参数 对于关键字参数，函数的调用者可以传入任意不受限制的关键字参数。至于到底传入了哪些，就需要在函数内部通过kw检查。 仍以person()函数为例，我们希望检查是否有city和job参数： 123456789101112131415161718192021def person(name, age, **kw): if 'city' in kw: # 有city参数 pass if 'job' in kw: # 有job参数 pass print('name:', name, 'age:', age, 'other:', kw)``` 调用者仍可以传入不受限制的关键字参数,如果要限制关键字参数的名字，就可以用命名关键字参数，例如，只接收city和job作为关键字参数。这种方式定义的函数如下：```pythondef person(name, age, *, city, job): print(name, age, city, job)``` 和关键字参数`**kw`不同，命名关键字参数需要一个特殊分隔符`*`，`*`后面的参数被视为命名关键字参数。如果函数定义中已经有了一个可变参数，后面跟着的命名关键字参数就不再需要一个特殊分隔符`*`了：```pythondef person(name, age, *args, city, job): print(name, age, args, city, job) 命名关键字参数必须传入参数名，这和位置参数不同。如果没有传入参数名，调用将报错。命名关键字参数可以有缺省值: 12345678910111213def person(name, age, *, city='Beijing', job): print(name, age, city, job)``` **参数组合**在Python中定义函数，可以用必选参数、默认参数、可变参数、关键字参数和命名关键字参数，这5种参数都可以组合使用。但是请注意，参数定义的顺序必须是：必选参数、默认参数、可变参数、命名关键字参数和关键字参数。```pythondef f1(a, b, c=0, *args, **kw): print('a =', a, 'b =', b, 'c =', c, 'args =', args, 'kw =', kw)def f2(a, b, c=0, *, d, **kw): print('a =', a, 'b =', b, 'c =', c, 'd =', d, 'kw =', kw) 对于任意函数，都可以通过类似func(*args, **kw)的形式调用它，无论它的参数是如何定义的: 123456789&gt;&gt;&gt; args = (1, 2, 3, 4, 5)&gt;&gt;&gt; kw = &#123;'d': 99, 'x': '#'&#125;&gt;&gt;&gt; f1(*args, **kw)f1(*args, **kw)a = 1 b = 2 c = 3 args = (4, 5) kw = &#123;'d': 99, 'x': '#'&#125;&gt;&gt;&gt; args = (1, 2, 3)&gt;&gt;&gt; kw = &#123;'d': 88, 'x': '#'&#125;&gt;&gt;&gt; f2(*args, **kw)a = 1 b = 2 c = 3 d = 88 kw = &#123;'x': '#'&#125; 可见，使用太多的组合，函数接口的可理解性很差。 参数检查 数据类型检查可以用内置函数isinstance()实现： 1234567def my_abs(x): if not isinstance(x, (int, float)): raise TypeError('bad operand type') if x &gt;= 0: return x else: return -x 添加了参数检查后，如果传入错误的参数类型，函数就可以抛出一个错误：12345&gt;&gt;&gt; my_abs('A')Traceback (most recent call last): File \"&lt;stdin&gt;\", line 1, in &lt;module&gt; File \"&lt;stdin&gt;\", line 3, in my_absTypeError: bad operand type 递归函数在函数内部，可以调用其他函数。如果一个函数在内部调用自身本身，这个函数就是递归函数。 利用递归函数移动汉诺塔: 1234567def move(n, a, b, c): if n == 1: print('move', a, '--&gt;', c) else: move(n-1, a, c, b) move(1, a, b, c) move(n-1, b, a, c) 调用: 1move(4, 'A', 'B', 'C') 高级特性在Python中，代码不是越多越好，而是越少越好。代码不是越复杂越好，而是越简单越好。 切片先创建一个0-99的数列： 1&gt;&gt;&gt; L = list(range(100)) 通过切片轻松取出某一段数列。比如前10个数： 12&gt;&gt;&gt; L[:10][0, 1, 2, 3, 4, 5, 6, 7, 8, 9] 后10个数: 12&gt;&gt;&gt; L[-10:][90, 91, 92, 93, 94, 95, 96, 97, 98, 99] 前10个数，每两个取一个：12&gt;&gt;&gt; L[:10:2][0, 2, 4, 6, 8] 所有数，每5个取一个： 12&gt;&gt;&gt; L[::5][0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95] 只写[:]就可以原样复制一个list。tuple也可以用切片操作，只是操作的结果仍是tuple字符串&#39;xxx&#39;也可以看成是一种list，每个元素就是一个字符。因此，字符串也可以用切片操作，只是操作结果仍是字符串 1234&gt;&gt;&gt; 'ABCDEFG'[:3]'ABC'&gt;&gt;&gt; 'ABCDEFG'[::2]'ACEG' 迭代如果给定一个list或tuple，我们可以通过for循环来遍历这个list或tuple，这种遍历我们称为迭代（Iteration）。在Python中，迭代是通过for ... in来完成的，java类似。 123456&gt;&gt;&gt; d = &#123;'a': 1, 'b': 2, 'c': 3&#125;&gt;&gt;&gt; for key in d: print(key)acb 默认情况下，dict迭代的是key。如果要迭代value，可以用for value in d.values()，如果要同时迭代key和value，可以用for k, v in d.items()。 如何 判断 一个对象是可迭代对象呢？方法是通过collections模块的Iterable类型判断： isinstance是Python中的一个内建函数。是用来判断一个对象是否是一个已知的类型。 1234567&gt;&gt;&gt; from collections import Iterable&gt;&gt;&gt; isinstance('abc', Iterable) # str是否可迭代True&gt;&gt;&gt; isinstance([1,2,3], Iterable) # list是否可迭代True&gt;&gt;&gt; isinstance(123, Iterable) # 整数是否可迭代False 如果要对list实现类似Java那样的下标循环怎么办？Python内置的enumerate函数可以把一个list变成索引-元素对，这样就可以在for循环中同时迭代索引和元素本身： 123456&gt;&gt;&gt; for i, value in enumerate(['A', 'B', 'C']): print(i, value)0 A1 B2 C 类似同时引入两个变量: 123456&gt;&gt;&gt; for x, y in [(1, 1), (2, 4), (3, 9)]:... print(x, y)...1 12 43 9 列表生成式列表生成式即List Comprehensions，是Python内置的非常简单却强大的可以用来创建list的生成式。 要生成list [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]可以用list(range(1, 11))，但如果要生成[1x1, 2x2, 3x3, …, 10x10]怎么做？方法一是循环： 123456&gt;&gt;&gt; L = []&gt;&gt;&gt; for x in range(1, 11):... L.append(x * x)...&gt;&gt;&gt; L[1, 4, 9, 16, 25, 36, 49, 64, 81, 100] 太繁琐，而列表生成式则可以用一行语句代替循环生成上面的list： 12&gt;&gt;&gt; [x * x for x in range(1, 11)][1, 4, 9, 16, 25, 36, 49, 64, 81, 100] 生成的元素x * x放到前面，后面跟for循环,for循环后面还可以加上if判断，这样我们就可以筛选出仅偶数的平方： 12&gt;&gt;&gt; [x * x for x in range(1, 11) if x % 2 == 0][4, 16, 36, 64, 100] 还可以使用两层循环，可以生成全排列： 12&gt;&gt;&gt; [m + n for m in 'ABC' for n in 'XYZ']['AX', 'AY', 'AZ', 'BX', 'BY', 'BZ', 'CX', 'CY', 'CZ'] 运用列表生成式，可以写出非常简洁的代码。例如，列出当前目录下的所有文件和目录名: 12&gt;&gt;&gt; import os # 导入os模块，模块的概念后面讲到&gt;&gt;&gt; [d for d in os.listdir('.')] # os.listdir可以列出文件和目录 列表生成式也可以使用两个变量来生成list： 123&gt;&gt;&gt; d = &#123;'x': 'A', 'y': 'B', 'z': 'C' &#125;&gt;&gt;&gt; [k + '=' + v for k, v in d.items()]['y=B', 'x=A', 'z=C'] 最后把一个list中所有的字符串变成小写： 123&gt;&gt;&gt; L = ['Hello', 'World', 'IBM', 'Apple']&gt;&gt;&gt; [s.lower() for s in L]['hello', 'world', 'ibm', 'apple'] 生成器通过列表生成式，我们可以直接创建一个列表。但是，受到内存限制，列表容量肯定是有限的。而且，创建一个包含100万个元素的列表，不仅占用很大的存储空间，如果我们仅仅需要访问前面几个元素，那后面绝大多数元素占用的空间都白白浪费了。所以，如果列表元素可以按照某种算法推算出来，那我们是否可以在循环的过程中不断推算出后续的元素呢？这样就不必创建完整的list，从而节省大量的空间。在Python中，这种一边循环一边计算的机制，称为生成器：generator。创建一个generator，有很多种方法。第一种方法很简单，只要把一个列表生成式的[]改成()，就创建了一个generator： 123456&gt;&gt;&gt; L = [x * x for x in range(10)]&gt;&gt;&gt; L[0, 1, 4, 9, 16, 25, 36, 49, 64, 81]&gt;&gt;&gt; g = (x * x for x in range(10))&gt;&gt;&gt; g&lt;generator object &lt;genexpr&gt; at 0x1022ef630&gt; 怎么打印出generator的每一个元素呢？一个一个打印出来，可以通过next()函数获得generator的下一个返回值： 12345&gt;&gt;&gt; next(g)0&gt;&gt;&gt; next(g)1&gt;&gt;&gt; next(g) 一般用for循环: 1234567891011121314&gt;&gt;&gt; g = (x * x for x in range(10))&gt;&gt;&gt; for n in g:... print(n)...0149162536496481 如果推算的算法比较复杂，用类似列表生成式的for循环无法实现的时候，还可以用函数来实现,如斐波拉契数列: 1234567def fib(max): n, a, b = 0, 0, 1 while n &lt; max: print(b) a, b = b, a + b n = n + 1 return 'done' 注: a, b = b, a + b 相当于: 123t = (b, a + b) # t是一个tuplea = t[0]b = t[1] fib函数实际上是定义了斐波拉契数列的推算规则，可以从第一个元素开始，推算出后续任意的元素，这种逻辑其实非常类似generator。也就是说，上面的函数和generator仅一步之遥。要把fib函数变成generator，只需要把print(b)改为yield b就可以了： 1234567def fib(max): n, a, b = 0, 0, 1 while n &lt; max: yield b a, b = b, a + b n = n + 1 return 'done' 如果一个函数定义中包含yield关键字，那么这个函数就不再是一个普通函数，而是一个generator。 迭代器我们已经知道，可以直接作用于for循环的数据类型有以下几种：一类是集合数据类型，如list、tuple、dict、set、str等；一类是generator，包括生成器和带yield的generator function。 这些可以直接作用于for循环的对象统称为**可迭代对象：Iterable。可以被next()函数调用并不断返回下一个值的对象称为迭代器：Iterator。 可以使用isinstance()判断一个对象是否是Iterable对象或Iterator对象。 生成器都是Iterator对象，但list、dict、str虽然是Iterable，却不是Iterator。把list、dict、str等Iterable变成Iterator可以使用iter()函数： 123456789101112131415&gt;&gt;&gt; from collections import Iterator&gt;&gt;&gt; isinstance([], Iterable)True&gt;&gt;&gt; isinstance((x for x in range(10)), Iterator)True&gt;&gt;&gt; isinstance([], Iterator)False&gt;&gt;&gt; isinstance(&#123;&#125;, Iterator)False&gt;&gt;&gt; isinstance('abc', Iterator)False&gt;&gt;&gt; isinstance(iter([]), Iterator)True&gt;&gt;&gt; isinstance(iter('abc'), Iterator)True 函数式编程函数就是面向过程的程序设计的基本单元。函数式编程（请注意多了一个“式”字）——Functional Programming，虽然也可以归结到面向过程的程序设计，但其思想更接近数学计算。 函数式编程就是一种抽象程度很高的编程范式，纯粹的函数式编程语言编写的函数没有变量，因此，任意一个函数，只要输入是确定的，输出就是确定的，这种纯函数我们称之为没有副作用。而允许使用变量的程序设计语言，由于函数内部的变量状态不确定，同样的输入，可能得到不同的输出，因此，这种函数是有副作用的。 函数式编程的一个 特点 就是允许把函数本身作为参数传入另一个函数，还允许返回一个函数！Python对函数式编程提供部分支持。由于Python允许使用变量，因此，Python不是纯函数式编程语言。 高阶函数Higher-order function一个函数可以接收另一个函数作为参数，这种函数就称之为高阶函数。一个最简单的高阶函数： 12def add(x, y, f): return f(x) + f(y) 当我们调用add(-5, 6, abs)时，参数x，y和f分别接收-5，6和abs，根据函数定义，我们可以推导计算过程为： 12345x = -5y = 6f = absf(x) + f(y) ==&gt; abs(-5) + abs(6) ==&gt; 11return 11 编写高阶函数，就是让函数的参数能够接收别的函数。 map/reducemapmap()函数接收两个参数，一个是函数，一个是Iterable，map将传入的函数依次作用到序列的每个元素，并把结果作为新的Iterator返回。 举例说明，比如我们有一个函数 $f(x)=x^2$ ，要把这个函数作用在一个list [1, 2, 3, 4, 5, 6, 7, 8, 9]上，就可以用map()实现如下： 12345678910111213141516 f(x) = x * x │ │ ┌───┬───┬───┬───┼───┬───┬───┬───┐ │ │ │ │ │ │ │ │ │ ▼ ▼ ▼ ▼ ▼ ▼ ▼ ▼ ▼[ 1 2 3 4 5 6 7 8 9 ] │ │ │ │ │ │ │ │ │ │ │ │ │ │ │ │ │ │ ▼ ▼ ▼ ▼ ▼ ▼ ▼ ▼ ▼[ 1 4 9 16 25 36 49 64 81 ] 现在，我们用Python代码实现： 123456&gt;&gt;&gt; def f(x):... return x * x...&gt;&gt;&gt; r = map(f, [1, 2, 3, 4, 5, 6, 7, 8, 9])&gt;&gt;&gt; list(r)[1, 4, 9, 16, 25, 36, 49, 64, 81] map()传入的第一个参数是f，即函数对象本身。由于结果r是一个Iterator，Iterator是惰性序列，因此通过list()函数让它把整个序列都计算出来并返回一个list。 你可能会想，不需要map()函数，写一个循环，也可以计算出结果： 1234L = []for n in [1, 2, 3, 4, 5, 6, 7, 8, 9]: L.append(f(n))print(L) 的确可以，但是，从上面的循环代码，能一眼看明白 把f(x)作用在list的每一个元素并把结果生成一个新的list 吗？ 所以，map()作为高阶函数，事实上它把运算规则 抽象 了，因此，我们不但可以计算简单的 $f(x)=x^2$ ，还可以计算任意复杂的函数，比如，把这个list所有数字转为字符串： 12&gt;&gt;&gt; list(map(str, [1, 2, 3, 4, 5, 6, 7, 8, 9]))['1', '2', '3', '4', '5', '6', '7', '8', '9'] 只需要一行代码。 reducereduce把一个函数作用在一个序列[x1, x2, x3, …]上，这个函数必须接收两个参数，reduce把结果继续和序列的下一个元素做 累积计算，其效果就是： 1reduce(f, [x1, x2, x3, x4]) = f(f(f(x1, x2), x3), x4) 把序列[1, 3, 5, 7, 9]变换成整数13579： 123456&gt;&gt;&gt; from functools import reduce&gt;&gt;&gt; def fn(x, y):... return x * 10 + y...&gt;&gt;&gt; reduce(fn, [1, 3, 5, 7, 9])13579 考虑到字符串str也是一个序列，对上面的例子稍加改动，配合map()，我们就可以写出把str转换为int的函数： 12345678910from functools import reduceDIGITS = &#123;'0': 0, '1': 1, '2': 2, '3': 3, '4': 4, '5': 5, '6': 6, '7': 7, '8': 8, '9': 9&#125;def str2int(s): def fn(x, y): return x * 10 + y def char2num(s): return DIGITS[s] return reduce(fn, map(char2num, s)) 还可以用lambda函数进一步简化成： 123456789from functools import reduceDIGITS = &#123;'0': 0, '1': 1, '2': 2, '3': 3, '4': 4, '5': 5, '6': 6, '7': 7, '8': 8, '9': 9&#125;def char2num(s): return DIGITS[s]def str2int(s): return reduce(lambda x, y: x * 10 + y, map(char2num, s)) lambda-&gt;参见后续的 匿名函数。 filterPython内建的filter()函数用于过滤序列。filter()把传入的函数依次作用于每个元素，然后根据返回值是True还是False决定保留还是丢弃该元素。 例如，在一个list中，删掉偶数，只保留奇数，可以这么写： 12345def is_odd(n): return n % 2 == 1list(filter(is_odd, [1, 2, 4, 5, 6, 9, 10, 15]))# 结果: [1, 5, 9, 15] 把一个序列中的空字符串删掉，可以这么写： 12345def not_empty(s): return s and s.strip()list(filter(not_empty, ['A', '', 'B', None, 'C', ' ']))# 结果: ['A', 'B', 'C'] 用filter求素数 计算素数的一个方法是埃氏筛法，它的算法理解起来非常简单： 首先，列出从2开始的所有自然数，构造一个序列： 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, … 取序列的第一个数2，它一定是素数，然后用2把序列的2的倍数筛掉： 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, … 取新序列的第一个数3，它一定是素数，然后用3把序列的3的倍数筛掉： 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, … 取新序列的第一个数5，然后用5把序列的5的倍数筛掉： 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, … 不断筛下去，就可以得到所有的素数。 用Python来实现这个算法，可以先构造一个从3开始的奇数序列： 12345def _odd_iter(): n = 1 while True: n = n + 2 yield n 注意这是一个 生成器，并且是一个无限序列。然后定义一个筛选函数： 12def _not_divisible(n): return lambda x: x % n &gt; 0 最后，定义一个生成器，不断返回下一个素数： 1234567def primes(): yield 2 it = _odd_iter() # 初始序列 while True: n = next(it) # 返回序列的第一个数 yield n it = filter(_not_divisible(n), it) # 构造新序列 这个生成器先返回第一个素数2，然后，利用filter()不断产生筛选后的新的序列。 由于primes()也是一个无限序列，所以调用时需要设置一个退出循环的条件： 打印1000以内的素数： 12345for n in primes(): if n &lt; 1000: print(n) else: break 注意到Iterator是惰性计算的序列，所以我们可以用Python表示“全体自然数”，“全体素数”这样的序列，而代码非常简洁。 练习 回数是指从左向右读和从右向左读都是一样的数，例如12321，909。请利用filter()筛选出回数： 提示: 12def is_palindrome(n): return n == int(str(n)[::-1]) sortedPython里的 排序算法 Python内置的sorted()函数就可以对list进行排序,此外，sorted()函数也是一个高阶函数，它还可以接收一个key函数来实现自定义的排序，例如按绝对值大小排序： 12&gt;&gt;&gt; sorted([36, 5, -12, 9, -21], key=abs)[5, 9, -12, -21, 36] key指定的函数将作用于list的每一个元素上，并根据key函数返回的结果进行排序。要进行反向排序，不必改动key函数，可以传入第三个参数reverse=True： 12&gt;&gt;&gt; sorted(['bob', 'about', 'Zoo', 'Credit'], key=str.lower, reverse=True)['Zoo', 'Credit', 'bob', 'about'] 上述例子可以看出，高阶函数的抽象能力是非常强大的。 返回函数函数作为返回值高阶函数除了可以接受函数作为参数外，还可以把函数作为结果值返回。 我们来实现一个可变参数的求和。通常情况下，求和的函数是这样定义的：12345def calc_sum(*args): ax = 0 for n in args: ax = ax + n return ax 但是，如果不需要立刻求和，而是在后面的代码中，根据需要再计算怎么办？可以不返回求和的结果，而是返回求和的函数： 1234567def lazy_sum(*args): def sum(): ax = 0 for n in args: ax = ax + n return ax return sum 当我们调用lazy_sum()时，返回的并不是求和结果，而是求和函数： 123&gt;&gt;&gt; f = lazy_sum(1, 3, 5, 7, 9)&gt;&gt;&gt; f&lt;function lazy_sum.&lt;locals&gt;.sum at 0x101c6ed90&gt; 调用函数f时，才真正计算求和的结果： 12&gt;&gt;&gt; f()25 在这个例子中，我们在函数lazy_sum中又定义了函数sum，并且，内部函数sum可以引用外部函数lazy_sum的参数和局部变量，当lazy_sum返回函数sum时，相关参数和变量都保存在返回的函数中，这种称为 闭包（Closure） 的程序结构拥有极大的威力。 请再注意一点，当我们调用lazy_sum()时，每次调用都会返回一个新的函数，即使传入相同的参数： 1234&gt;&gt;&gt; f1 = lazy_sum(1, 3, 5, 7, 9)&gt;&gt;&gt; f2 = lazy_sum(1, 3, 5, 7, 9)&gt;&gt;&gt; f1==f2False 闭包返回的函数在其定义内部引用了局部变量args，所以，当一个函数返回了一个函数后，其内部的局部变量还被另一个函数引用，所以，闭包用起来简单，实现起来可不容易。另一个需要注意的问题是，返回的函数并没有立刻执行，而是直到调用了f()才执行。我们来看一个例子： 123456789def count(): fs = [] for i in range(1, 4): def f(): return i*i fs.append(f) return fsf1, f2, f3 = count() 在上面的例子中，每次循环，都创建了一个新的函数，然后，把创建的3个函数都返回了。你可能认为调用f1()，f2()和f3()结果应该是1，4，9，但实际结果是： 123456&gt;&gt;&gt; f1()9&gt;&gt;&gt; f2()9&gt;&gt;&gt; f3()9 原因就在于返回的函数引用了变量i，但它并非立刻执行。等到3个函数都返回时，它们所引用的变量i已经变成了3，因此最终结果为9。 返回闭包时牢记一点：返回函数不要引用任何循环变量，或者后续会发生变化的变量。如果一定要引用循环变量怎么办？方法是再创建一个函数，用该函数的参数绑定循环变量当前的值，无论该循环变量后续如何更改，已绑定到函数参数的值不变： 123456789def count(): def f(j): def g(): return j*j return g fs = [] for i in range(1, 4): fs.append(f(i)) # f(i)立刻被执行，因此i的当前值被传入f() return fs 缺点是代码较长，可利用lambda函数缩短代码。 练习利用闭包返回一个计数器函数，每次调用它返回递增整数： 12345678def createCounter(): def f(): i=0 while True: i += 1 yield i a = f() return lambda: next(a) 调用： 12345&gt;&gt;&gt; a= createCounter()&gt;&gt;&gt; a()1&gt;&gt;&gt; a()2 匿名函数当我们在传入函数时，有些时候，不需要显式地定义函数，直接传入匿名函数更方便。在Python中，对匿名函数提供了有限支持。还是以map()函数为例，计算 $f(x)=x^2$ 时，除了定义一个f(x)的函数外，还可以直接传入匿名函数： 12&gt;&gt;&gt; list(map(lambda x: x * x, [1, 2, 3, 4, 5, 6, 7, 8, 9]))[1, 4, 9, 16, 25, 36, 49, 64, 81] 通过对比可以看出，匿名函数lambda x: x * x实际上就是： 12def f(x): return x * x 关键字lambda表示匿名函数，冒号前面的x表示函数参数。 匿名函数有个 限制，就是只能有一个表达式，不用写return，返回值就是该表达式的结果。用匿名函数有个好处，因为函数没有名字，不必担心函数名冲突。此外，匿名函数也是一个函数对象，也可以把匿名函数赋值给一个变量，再利用变量来调用该函数： 12345&gt;&gt;&gt; f = lambda x: x * x&gt;&gt;&gt; f&lt;function &lt;lambda&gt; at 0x101c6ef28&gt;&gt;&gt;&gt; f(5)25 装饰器由于函数也是一个对象，而且函数对象可以被赋值给变量，所以，通过变量也能调用该函数。 123456&gt;&gt;&gt; def now():... print('2015-3-25')...&gt;&gt;&gt; f = now&gt;&gt;&gt; f()2015-3-25 函数对象有一个__name__属性，可以拿到函数的名字： 1234&gt;&gt;&gt; now.__name__'now'&gt;&gt;&gt; f.__name__'now' 现在，假设我们要增强now()函数的功能，比如，在函数调用前后自动打印日志，但又不希望修改now()函数的定义，这种在代码运行期间动态增加功能的方式，称之为 装饰器（Decorator）。本质上，decorator就是一个返回函数的高阶函数。所以，我们要定义一个能打印日志的decorator，可以定义如下： 12345def log(func): def wrapper(*args, **kw): print('call %s():' % func.__name__) return func(*args, **kw) return wrapper 观察上面的log，因为它是一个decorator，所以接受一个函数作为参数，并返回一个函数。我们要借助Python的@语法，把decorator置于函数的定义处： 123@logdef now(): print('2015-3-25') 调用: 123&gt;&gt;&gt; now()call now():2015-3-25 把@log放到now()函数的定义处，相当于执行了语句： 1now = log(now) 因为wrapper()函数的参数定义是(*args, **kw)，因此，wrapper()函数可以接受任意参数的调用，在wrapper()函数内，首先打印日志，再紧接着调用原始函数。如果decorator本身需要传入参数，那就需要编写一个返回decorator的高阶函数，写出来会更复杂。比如，要自定义log的文本： 1234567def log(text): def decorator(func): def wrapper(*args, **kw): print('%s %s():' % (text, func.__name__)) return func(*args, **kw) return wrapper return decorator 这个3层嵌套的decorator用法如下： 123@log('execute')def now(): print('2015-3-25') 和两层嵌套的decorator相比，3层嵌套的效果是这样的： 1now = log('execute')(now) 我们来剖析上面的语句，首先执行log(&#39;execute&#39;)，返回的是decorator函数，再调用返回的函数，参数是now函数，返回值最终是wrapper函数。 以上两种decorator的定义都没有问题，但还差最后一步。因为我们讲了函数也是对象，它有__name__等属性，但你去看经过decorator装饰之后的函数，它们的__name__已经从原来的&#39;now&#39;变成了&#39;wrapper‘： 12&gt;&gt;&gt; now.__name__'wrapper' 因为返回的那个wrapper()函数名字就是&#39;wrapper&#39;，所以，需要把原始函数的__name__等属性复制到wrapper()函数中，否则，有些依赖函数签名的代码执行就会出错。 不需要编写wrapper.__name__ = func.__name__这样的代码，Python内置的functools.wraps就是干这个事的，所以，一个完整的decorator的写法如下： 12345678import functoolsdef log(func): @functools.wraps(func) def wrapper(*args, **kw): print('call %s():' % func.__name__) return func(*args, **kw) return wrapper 或者针对带参数的decorator： 12345678910import functoolsdef log(text): def decorator(func): @functools.wraps(func) def wrapper(*args, **kw): print('%s %s():' % (text, func.__name__)) return func(*args, **kw) return wrapper return decorator import functools是导入functools模块。模块的概念稍候讲解。现在，只需记住在定义wrapper()的前面加上@functools.wraps(func)即可。 偏函数Python的functools模块提供了很多有用的功能，其中一个就是偏函数（Partial function）functools.partial的作用就是，把一个函数的某些参数给固定住（也就是设置默认值），返回一个新的函数，调用这个新函数会更简单。 123456&gt;&gt;&gt; import functools&gt;&gt;&gt; int2 = functools.partial(int, base=2)&gt;&gt;&gt; int2('1000000')64&gt;&gt;&gt; int2('1010101')85 注意到上面的新的int2函数，仅仅是把base参数重新设定默认值为2，但也可以在函数调用时传入其他值： 12&gt;&gt;&gt; int2('1000000', base=10)1000000 当函数的参数个数太多，需要简化时，使用functools.partial可以创建一个新的函数，这个新函数可以固定住原函数的部分参数，从而在调用时更简单。 模块在Python中，一个.py文件就称之为一个 模块（Module）。为了避免模块名冲突，Python又引入了按目录来组织模块的方法，称为 包（Package）。每一个包目录下面都会有一个__init__.py的文件，这个文件是必须存在的，否则，Python就把这个目录当成普通目录，而不是一个包。__init__.py可以是空文件，也可以有Python代码，因为__init__.py本身就是一个模块。 创建自己的模块时，要注意： 模块名要遵循Python变量命名规范，不要使用中文、特殊字符； 模块名不要和系统模块名冲突，最好先查看系统是否已存在该模块，检查方法是在Python交互环境执行import abc，若成功则说明系统存在此模块。 使用模块使用模块的第一步，就是导入该模块： 1import sys 当我们在命令行运行模块文件时，Python解释器把一个特殊变量__name__置为__main__，而如果在其他地方导入该模块时，if判断将失败，因此，这种if测试可以让一个模块通过命令行运行时执行一些额外的代码，最常见的就是运行测试。 12if __name__=='__main__': test() 作用域类似__xxx__这样的变量是特殊变量，可以被直接引用，但是有特殊用途，比如__author__ ，__name__就是特殊变量.类似_xxx和__xxx这样的函数或变量就是非公开的（private），不应该被直接引用，比如_abc，__abc等； 之所以我们说private函数和变量“不应该”被直接引用，而不是“不能”被直接引用，是因为Python并没有一种方法可以完全限制访问private函数或变量，但是，从编程习惯上不应该引用private函数或变量。 private函数或变量不应该被别人引用，那它们有什么用呢？请看例子： 1234567891011def _private_1(name): return 'Hello, %s' % namedef _private_2(name): return 'Hi, %s' % namedef greeting(name): if len(name) &gt; 3: return _private_1(name) else: return _private_2(name) 我们在模块里公开greeting()函数，而把内部逻辑用private函数隐藏起来了，这样，调用greeting()函数不用关心内部的private函数细节，这也是一种非常有用的 代码封装和抽象 的方法，即：外部不需要引用的函数全部定义成private，只有外部需要引用的函数才定义为public。 安装第三方模块在Python中，安装第三方模块，是通过包管理工具pip完成的。比如: 1pip install Pillow 一般来说，第三方库都会在Python官方的pypi.python.org网站注册. 另外，可以直接使用 Anaconda。 模块搜索路径 默认情况下，Python解释器会搜索当前目录、所有已安装的内置模块和第三方模块，搜索路径存放在sys模块的path变量中： 12&gt;&gt;&gt; import sys&gt;&gt;&gt; sys.path 添加自己的搜索目录，有两种方法：一是直接修改sys.path，添加要搜索的目录： 12&gt;&gt;&gt; import sys&gt;&gt;&gt; sys.path.append('/Users/michael/my_py_scripts') 这种方法是在运行时修改，运行结束后失效。第二种方法是设置环境变量PYTHONPATH，该环境变量的内容会被自动添加到模块搜索路径中。设置方式与设置Path环境变量类似。","tags":[{"name":"Python","slug":"Python","permalink":"http://wenbo.fun/tags/Python/"}]},{"title":"分支限界---批处理作业调度","date":"2017-12-31T07:56:44.000Z","path":"2017/12/31/sf012/","text":"批处理作业调度问题描述 计算机算法设计与分析(第四版)，王晓东 n 个作业的集合: $$J={J1,J2,…,Jn}$$ 第 i 个作业需要机器 j 的处理时间: $$ t_{ij}$$ 已安排作业集合: $$M⊆{1,2…,n}$$ 所有作业在机器2完成时间和（包括等待时间）: $$f=\\sum_{i=1}^n{F_{2i}}$$ 以某个节点为根的子树中所含叶节点的完成时间和可表示为:$$f=\\sum_{i \\in M} F_{2i} + \\sum_{i \\notin M} F_{2i} $$ 实例: 排列树: 设 |M|=r , p_k 是第k个安排的作业,相应的作业调度为: $${p_k,k=1,2,……n}$$其中。如果从节点E到叶节点L的路上，每一个作业p_k在机器1上完成处理后都能立即在机器2上开始处理，即从p_r+1开始，机器1没有空闲时间,则对于该叶节点有:$$\\sum_{i \\notin M}F_{2i} = \\sum_{k=r+1}^n[F_{1p_r}+(n-k+1)t_{1p_k}+t_{2p_k}]$$ 式子中(n-k+1),因为是求的是作业完成时间和，所以，后续的(n-k+1)个作业完成时间和都得算上 如果不是按照上面安排，则s1会增加，从而有:$$\\sum_{i \\notin M}F_{2i} \\ge S_1 $$类似，从p_r+1开始，让机器2没有空闲时间:$$\\sum_{i \\notin M}F_{2i} = \\sum_{k=r+1}^n[ \\max(F_{2p_r},F_{1p_r} + \\min\\limits_{i \\notin M} t_{1i}) + (n-k+1)t_{2p_k}] = S_2$$从而得到下界(限界函数):$$f \\ge \\sum_{i \\notin M}F_{2i} + \\max \\{S_1,S_2\\}$$ java代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205/** * 批处理作业调度问题--优先队列式分支限界法 **/import java.util.Collections;import java.util.LinkedList;public class BBFlow &#123; public int n;//作业数 public int bestc;//最小完成时间和 public int [][]m;//个作业所需的处理时间数组 public int [][]b;//个作业所需的处理时间排序数组 public int[][] a;//数组m和b的对应关系数组 public int[] bestx;//最优解 public boolean[][] y;//工作数组 public BBFlow(int n,int[][] m)&#123; this.n=n; bestc=10000; this.m=m; b=new int[n][2]; a=new int[n][2]; bestx=new int[n]; y=new boolean[n][2]; &#125; public void swap(int[][] b,int i,int j,int k,int t)&#123; int temp=b[i][j]; b[i][j]=b[k][t]; b[k][t]=temp; &#125; public void swap(int[] x,int i,int j)&#123; int temp=x[i]; x[i]=x[j]; x[j]=temp; &#125;/*** 对个作业在机器1和2上所需时间排序*/ public void sort()&#123; int[] c=new int[n]; for(int j=0;j&lt;2;j++)&#123; for(int i=0;i&lt;n;i++)&#123; b[i][j]=m[i][j]; c[i]=i; &#125; for(int i=0;i&lt;n-1;i++)&#123; for(int k=n-1;k&gt;i;k--)&#123; if(b[k][j]&lt;b[k-1][j])&#123; swap(b,k,j,k-1,j); swap(c,k,k-1); &#125; &#125; &#125; for(int i=0;i&lt;n;i++) a[c[i]][j]=i; &#125; &#125; /** * 计算完成时间和下界 * @param enode * @param f * @return */ public int bound(Nodes enode,int[] f)&#123; for(int k=0;k&lt;n;k++)&#123; for(int j=0;j&lt;2;j++)&#123; y[k][j]=false; &#125; &#125; for(int k=0;k&lt;enode.s;k++)&#123; for(int j=0;j&lt;2;j++)&#123; y[a[enode.x[k]][j]][j]=true; &#125; &#125; f[1]=enode.f[1]+m[enode.x[enode.s]][0]; f[2]=((f[1]&gt;enode.f[2])?f[1]:enode.f[2])+m[enode.x[enode.s]][1]; int sf2=enode.sf2+f[2]; int s1=0; int s2=0; int k1=n-enode.s; int k2=n-enode.s; int f3=f[2]; //计算s1的值 for(int j=0;j&lt;n;j++)&#123; if(!y[j][0])&#123; k1--; if(k1==n-enode.s-1) f3=(f[2]&gt;f[1]+b[j][0])?f[2]:f[1]+b[j][0]; s1+=f[1]+k1*b[j][0]; &#125; &#125; //计算s2的值 for(int j=0;j&lt;n;j++)&#123; if(!y[j][1])&#123; k2--; s1+=b[j][1]; s2+=f3+k2*b[j][1]; &#125; &#125; //返回完成时间和下界 return sf2+((s1&gt;s2)?s1:s2); &#125; /** * 优先队列式分支限界法解批处理作业调度问题 * @param nn * @return */ public int bbFlow(int nn)&#123; n=nn; sort();//对个作业在机器1和2上所需时间排序 LinkedList&lt;Nodes&gt; heap=new LinkedList&lt;Nodes&gt;(); Nodes enode =new Nodes(n); //搜索排列空间树 do&#123; if(enode.s==n)&#123; //叶节点 if(enode.sf2&lt;bestc)&#123; bestc=enode.sf2; for(int i=0;i&lt;n;i++)&#123; bestx[i]=enode.x[i]; &#125; &#125; &#125;else&#123; //产生当前扩展结点的儿子结点 for(int i=enode.s;i&lt;n;i++)&#123; swap(enode.x,enode.s,i); int[] f=new int[3]; int bb=bound(enode,f); if(bb&lt;bestc)&#123; //子树可能含有最优解 //结点插入最小堆 Nodes node=new Nodes(enode,f,bb,n); heap.add(node); Collections.sort(heap); &#125; swap(enode.x,enode.s,i); &#125;//完成结点扩展 &#125; //取下一个扩展结点 enode=heap.poll(); &#125;while(enode!=null&amp;&amp;enode.s&lt;=n); return bestc; &#125; public static void main(String[] args) &#123; int n=3; int[][] m=&#123;&#123;2,1&#125;,&#123;3,1&#125;,&#123;2,3&#125;&#125;;//m的下标从0开始 BBFlow f=new BBFlow(n,m); f.bbFlow(n); System.out.println(\"最优批处理作业调度顺序为：\"); for(int i=0;i&lt;n;i++) System.out.print((f.bestx[i]+1)+\" \"); System.out.println(); System.out.println(\"最优调度所需的最短时间为：\"+f.bestc); &#125;&#125;class Nodes implements Comparable&#123; int s;//已安排作业数 int sf2;//当前机器2上的完成时间和 int bb;//当前完成时间和下界 int[] f;//f[1]机器1上最后完成时间，f[2]机器2上最后完成时间 int[] x;//当前作业调度 public Nodes(int n)&#123; //最小堆结点初始化 x=new int[n]; for(int i=0;i&lt;n;i++) x[i]=i; s=0; f=new int[3]; f[1]=0; f[2]=0; sf2=0; bb=0; &#125; public Nodes(Nodes e,int[] ef,int ebb,int n)&#123; //最小堆新结点 x=new int[n]; for(int i=0;i&lt;n;i++) x[i]=e.x[i]; f=ef; sf2=e.sf2+f[2]; bb=ebb; s=e.s+1; &#125; @Override public int compareTo(Object o) &#123; int xbb=((Nodes) o).bb; if(bb&lt;xbb) return -1; if(bb==xbb) return 0; return 1; &#125;&#125;/*运行结果：最优批处理作业调度顺序为：1 3 2最优调度所需的最短时间为：18*/","tags":[{"name":"算法","slug":"算法","permalink":"http://wenbo.fun/tags/算法/"}]},{"title":"分支限界---布线问题","date":"2017-12-30T11:07:34.000Z","path":"2017/12/30/sf011/","text":"分支限界与回溯回溯法盲目搜索从根节点出发，按照状态空间树的结构，向下搜索它的所有儿子结点，对不满足约束条件的儿子结点，把它丢弃；对满足约束条件的结点，继续相似搜索它的所有儿子结点。该搜索过程一直进行，当搜索到一个满足约束条件的叶结点时，就得到了一个可行解；或者所有的儿子结点都不满足约束条件时，该结点就被丢弃，向上回溯到它的父亲结点。 分支限界法启发式搜索 在结点估算沿着它的各儿子结点搜索时，目标函数可能取得的“界” 把儿子结点和目标函数可能取得的“界”，保存在优先队列或堆中 从队列或堆中选取“界”最大或最小的结点向下搜索，直到叶子结点 若叶子结点的目标函数的值，是结点表中的最大值或最小值，则沿叶子结点到根结点的路径所确定的解，就是问题的最优解，由该叶子结点所确定的目标函数的值，就是解这个问题所得到的最大值或最小值 界 对最小值问题，称为下界，意思是向下搜索所可能取得的值最小不会小于这些下界。 对最大值问题，称为上界，意思是向下搜索所可能取得的值最大不会大于这些上界。 子集树与排列树子集树问题可以理解为在一个集合中选取一个满足某种条件的子集。我们对待选集合任意排序，然后顺次对其中的元素给出两种决策： 左: 将当前元素选入集合右: 放弃当前元素 整个解空间所呈现出的是一棵二叉树。算法复杂度为$$O(2 ^ n)$$ E.g. 0-1背包问题 排列树问题可以理解为在一个集合的排列中寻找一个符合要求的排列。我们采用这样的策略： 每一次的决策都是从当前集合选出一个元素，将其余元素放入下次决策。当集合为空，我们生成了一个排列。 算法复杂度$$O(n!)$$ E.g. 旅行商问题 回溯与分支限界的比较 搜索模型的不同 回溯使用递归栈作为辅助，按既定顺序逐条搜索每一条从树根到叶子的路径。 分支限界法使用优先级队列或队列作为辅助，寻找“最优”路径。 目标不同 回溯法更为通用，而分支限界法用于寻找最优解 时间效率(搜索最优解问题) 回溯法搜索的时间正比于搜索解空间的大小 分支限界法的时间敏感于最优解所在位置，当最优解偏树根，则时间相对于搜索整个解空间树非常少。 空间效率 回溯法的空间消耗与解空间（最大）树高有关 分支限界法的空间消耗一般表现出广度优先搜索的特点，与解空间的宽度有关。 一般树高远小于树宽,所以可以认为分支限界法是回溯法的一种空间换时间的技术 布线问题问题描述 计算机算法设计与分析(第四版)，王晓东 时间复杂度扩展节点: $$O(1)$$每个方格此为活结点进入队列最多一次，总结点数:$$O(mn)$$构造最短距离(L是最短路径的长度):$$O(L)$$ java代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148import java.util.LinkedList; import java.util.Scanner; /** * 布线问题 * @author Lican * */ public class WireRouter &#123; public int[][] grid;//方格阵列;=0表示该方格运行布线；=1表示被封锁，不允许布线 public int size;//方格阵列大小 public int pathLen;//最短线路长度 public LinkedList&lt;Position&gt; q;//扩展结点队列，用list存储 public Position start;//起始位置 public Position finish;//终点 public Position[] path;//最短路径 public WireRouter(int[][] grid,int size,Position start,Position finish)&#123; this.grid=grid; this.size=size; this.start=start; this.finish=finish; &#125; /** * 方格所在位置 * @author Lican * */ public static class Position&#123; public int row;//行 public int col;//列 public Position(int r ,int c)&#123; row=r; col=c; &#125; &#125; /** *计算从起始位置start到目标位置finish的最短布线路径 * @return 找到最短布线路径则返回true，否则返回false */ public boolean findPath()&#123; if(start.row==finish.row&amp;&amp;start.col==finish.col)&#123;//start==finish,最短路径为0 pathLen=0; return true; &#125; //初始化相对位移 Position[] offset=new Position[4]; offset[0]=new Position(0,1);//右 offset[1]=new Position(1,0);//下 offset[2]=new Position(0,-1);//左 offset[3]=new Position(-1,0);//上 //设置方格阵列“围墙”，方便处理方格边界的情况 for(int i=0;i&lt;=size+1;i++)&#123; grid[0][i]=grid[size+1][i]=1;//顶部和底部 grid[i][0]=grid[i][size+1]=1;//左边和右边 &#125; Position here=new Position(start.row,start.col); grid[start.row][start.col]=2;//数字0,1表示方格的开放或封锁所以，表示距离时，让所有距离都加2；起始位置的距离为0+2=2 int numOfNbrs=4;//相邻方格数 //以下为标记可达的方格位置 q=new LinkedList&lt;Position&gt;(); Position nbr=new Position(0,0); do&#123; //标记可达的相邻方格(每个方格有四个相邻方格) for(int i=0;i&lt;numOfNbrs;i++)&#123; nbr.row=here.row+offset[i].row; nbr.col=here.col+offset[i].col; if(grid[nbr.row][nbr.col]==0)&#123;//该方格未被标记，且该方格允许布线 grid[nbr.row][nbr.col]=grid[here.row][here.col]+1; if(nbr.row==finish.row&amp;&amp;nbr.col==finish.col) break; q.add(new Position(nbr.row,nbr.col)); &#125; &#125; //检测是否到达目标位置finish if(nbr.row==finish.row&amp;&amp;nbr.col==finish.col) break; if(q.isEmpty()) return false; here=q.poll(); &#125;while(true); //构造最短布线路径 pathLen=grid[finish.row][finish.col]-2; path=new Position[pathLen]; //从目标位置finish开始向起始位置回溯 here=finish; for(int j=pathLen-1;j&gt;=0;j--)&#123; path[j]=here; //找前驱位置 for(int i=0;i&lt;numOfNbrs;i++)&#123; nbr.row=here.row+offset[i].row; nbr.col=here.col+offset[i].col; if(grid[nbr.row][nbr.col]==j+2) break; &#125; here=new Position(nbr.row,nbr.col); &#125; System.out.println(\"最短路线为：\"); for(int j=0;j&lt;pathLen-1;j++)&#123; System.out.println(\"点\"+(j+1)+\"位置: 行-\"+path[j].row+\" 列-\"+path[j].col); &#125; System.out.println(\"布线长度为：\"+pathLen); return true; &#125; public static void main(String[] args) &#123; Scanner sc=new Scanner(System.in); System.out.println(\"请输入方格阵列大小：\"); String s1=sc.nextLine(); Integer size=Integer.parseInt(s1); System.out.println(\"请输入起始点的行和列，用空格隔开：\"); String s2=sc.nextLine(); String[] s3=s2.split(\" \"); int startRow=Integer.parseInt(s3[0]); int startCol=Integer.parseInt(s3[1]); Position start=new Position(startRow,startCol); System.out.println(\"请输入结束点的行和列，用空格隔开：\"); String s4=sc.nextLine(); String[] s5=s4.split(\" \"); int finishRow=Integer.parseInt(s5[0]); int finishCol=Integer.parseInt(s5[1]); Position finish=new Position(finishRow,finishCol); int[][] grid=new int[size+2][size+2]; System.out.println(\"请输入方格阵列：\"); for(int i=1;i&lt;=size;i++)&#123; String str=sc.nextLine(); String[] strs=str.split(\" \"); for(int j=0;j&lt;strs.length;j++)&#123; grid[i][j+1]=Integer.parseInt(strs[j]); &#125; &#125; WireRouter w=new WireRouter(grid,size,start,finish); w.findPath(); &#125; &#125;","tags":[{"name":"算法","slug":"算法","permalink":"http://wenbo.fun/tags/算法/"}]},{"title":"回溯法---0-1背包问题","date":"2017-12-30T09:04:17.000Z","path":"2017/12/30/sf010/","text":"0-1背包问题问题描述 计算机算法设计与分析(第四版)，王晓东 时间复杂度计算上界时间:$$O(n)$$ 最坏情况有:$$O(2^n)$$个右儿子节点需计算上界。所以回溯算法Backtrack所需时间:$$O(n2^n)$$ java代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114/** * 0-1背包问题--回溯法 * */ public class Knapsack &#123; public static class Element implements Comparable&#123; int id;//物品编号 double d; public Element(int id,double d)&#123; this.id=id; this.d=d; &#125; @Override public int compareTo(Object x) &#123; double xd=((Element)x).d;//递减顺序排列 if(d&lt;xd) return -1; if(d==xd) return 0; return 1; &#125; &#125; double c;//背包容量 int n;//物品数 double[] w;//物品重量数组 double[] p;//物品价值数组 double cw;//当前重量 double cp;//当前价值 double bestp;//最优价值 int[] x;//当前装入背包顺序 int[] bestx;//最优装入背包顺序 Element[] q;//q为单位重量价值数组 public double knapsack(double[] pp,double[] ww,double cc)&#123; //初始化 c=cc; n=pp.length-1; cw=0; cp=0; bestp=0; x=new int[n+1]; bestx=new int[n+1]; //q为单位重量价值数组 q=new Element[n+1]; for(int i=0;i&lt;=n;i++)&#123; q[i]=new Element(i,pp[i]/ww[i]); &#125; //将个物品依单位重量价值从大到小排列 java.util.Arrays.sort(q); p=new double[n+1]; w=new double[n+1]; for(int i=1;i&lt;=n;i++)&#123; p[i]=pp[q[i].id]; w[i]=ww[q[i].id]; &#125; backtrack(1); return bestp; &#125; public void backtrack(int i)&#123; if(i&gt;n)&#123;//到达叶子节点 bestp=cp; for(int j=1;j&lt;=n;j++)&#123;//保存最优值对应的包的编号 bestx[j]=x[j]; &#125; return; &#125; if(cw+w[i]&lt;=c)&#123;//左子树 x[i]=1; cw+=w[i]; cp+=p[i]; backtrack(i+1); cw-=w[i];//恢复现场 cp-=p[i]; &#125; if(bound(i+1)&gt;bestp)&#123; x[i]=0; backtrack(i+1); &#125; &#125; public double bound(int i)&#123;//上界函数 double cleft=c-cw; double bound=cp; while(i&lt;=n&amp;&amp;w[i]&lt;=cleft)&#123; cleft-=w[i]; bound+=p[i]; i++; &#125; if(i&lt;=n)&#123; bound+=p[i]*cleft/w[i]; &#125; return bound; &#125; public static void main(String[] args) &#123; double[] weight=&#123;0,71,34,82,23,1,88,12,57,10,68,5,33,37,69,98,24,26,83,16,26,18,43,52,71,22,65,68,8,40,40,24,72,16,34,10,19,28,13,34,98,29,31,79,33,60,74,44,56,54,17&#125;; double[] price=&#123;0,26,59,30,19,66,85,94,8,3,44,5,1,41,82,76,1,12,81,73,32,74,54,62,41,19,10,65,53,56,53,70,66,58,22,72,33,96,88,68,45,44,61,78,78,6,66,11,59,83,48&#125;; double cc=300; //double[] weight=&#123;0,7,3,4,5&#125;; //double[] price=&#123;0,42,12,40,25&#125;; //double cc=10; Knapsack k=new Knapsack(); double best=k.knapsack(price,weight,cc); System.out.println(\"最优值：\"+best); System.out.println(\"选中的物品编号分别是：\"); for(int i=1;i&lt;k.bestx.length;i++)&#123; if(k.bestx[i]==1)&#123; System.out.print(k.q[i].id+\" \"); &#125; &#125; &#125; &#125; /* 输出结果：最优值：1063.0选中的物品编号分别是：5 7 35 38 28 19 21 33 37 31 50 44 39 42 2 11 */","tags":[{"name":"算法","slug":"算法","permalink":"http://wenbo.fun/tags/算法/"}]},{"title":"回溯法---符号三角形","date":"2017-12-30T05:07:21.000Z","path":"2017/12/30/sf009/","text":"回溯法 问题解空间 深度优先遍历 深度搜索至任意一个节点，若包括问题解，继续按深度优先策略搜索;如果不包含问题解，则跳过对该结点为根的子树的搜索，逐层向其祖先结点回溯。 减枝函数 符号三角形问题描述 计算机算法设计与分析(第四版)，王晓东 时间复杂度计算可行性约束(符号个数)需要时间:$$O(n)$$最坏情况(n个符号排列)有:$$O(2^n)$$个节点需要计算可行性约束，故回溯算法 Backtrack 所需计算时间: $$O(n2^n)$$ java代码12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849public class Triangles &#123; private int n;// 第一行符号个数 private int half;// n*(n+1)/4 private int count;//当前\"+\"号个数 private int[][] p;//符号三角形矩阵 private long sum;//已找到符号三角形数 private void Backtrack(int t) &#123; if((count&gt;half)||(t*(t-1)/2-count&gt;half)) return;// 控制\"+\"\"-\"个数相等 if(t&gt;n) sum++; else &#123; for (int i = 0; i &lt; 2; i++) &#123;// 1 为\"+\" , 0为\"-\" p[1][t] = i; count += i; for (int j = 2; j &lt;= t; j++) &#123; p[j][t-j+1] = p[j-1][t-j+1]^p[j-1][t-j+2]; count += p[j][t-j+1]; &#125; Backtrack(t+1); for (int j = 2; j &lt;= t; j++) &#123; count -= p[j][t-j+1]; &#125; count -= i; &#125; &#125; &#125; private static long Compute(int n) &#123; Triangles X = new Triangles(); X.n = n; X.count = 0; X.half = n*(n+1)/2; if (X.half%2 == 1)return 0;// 符号数为奇数 X.half = X.half/2; X.sum = 0; int[][] p = new int[n+1][n+1]; for (int i = 0; i &lt;= n; i++) for (int j = 0; j &lt;= n; j++) &#123; p[i][j] = 0; &#125; X.p = p; X.Backtrack(1); return X.sum; &#125; public static void main(String[] args) &#123; int m = 3; long n = Compute(m); System.out.println(\"第一行符号数 n=\"+m+\"时，有\"+n+\"个符号三角形\"); &#125;&#125;","tags":[{"name":"算法","slug":"算法","permalink":"http://wenbo.fun/tags/算法/"}]},{"title":"贪心算法---最小生成树","date":"2017-12-30T03:17:39.000Z","path":"2017/12/30/sf008/","text":"最小生成树问题描述 计算机算法设计与分析(第四版)，王晓东 最小生成树性质 假设 N = (V,{ E })是一个连通网，U 是顶点集 V 的一个非空子集。若(u , v)是一条具有最小权值(代价)的边，其中u∈U， v∈V - U，则必存在一棵包含边(u，v)的最小生成树。 反证: 假设网N的任何一棵最小生成树都不包含(u，v)。设T是连通网上的一棵最小生成树，当将边(u，v)加入到T中时，由生成树的定义，T中必存在一条包含(u，v)的回路。另一方面，由于T是生成树，则在T上必存在另一条边(u’，v’)，其中u’∈U，v’∈V - U，且u和u’之间，v和v’之间均有路径相通。删去边(u’，v’)，便可消除上述回路，同时得到另一棵生成树T’。因为(u，v)的代价不高于(u’，v’)，则T’的代价亦不高于T，T’是包含(u，v)的一棵最小生成树，和假设矛盾。 时间复杂度 略 Prim(普里姆算法) 算法选取满足条件 i∈S，j∈V-S，且 c[i][j] 最小的边，将顶点 j 添加到 S 中。这个过程一直进行到 S=V 时为止。在这个过程中选取到的所有边恰好构成G的一棵最小生成树T。 java 代码 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849public class MinSpanTree &#123; public static void Prim(int n, int[][] c) &#123; int[] lowcost= new int[n]; int[] closest= new int[n]; boolean[] s= new boolean[n]; s[1] = true; // 初始化lowcost[i](到顶点i最小代价)，closest[i](i的连接点) for (int i = 2; i &lt; n; i++) &#123; lowcost[i] = c[1][i]; closest[i] = 1; s[i] = false; &#125; for (int i = 1; i &lt; n-1; i++) &#123; int min = Integer.MAX_VALUE; int j = 1; for (int k = 2; k &lt; n; k++) &#123; if ((lowcost[k] &lt; min)&amp;&amp;(!s[k])) &#123; min = lowcost[k];j=k; &#125; &#125; System.out.println(j); s[j] = true; //将第j个顶点纳入S for (int k = 2; k &lt; n; k++) &#123; if ((c[j][k] &lt; lowcost[k])&amp;&amp;(!s[k])) &#123; lowcost[k] = c[j][k];closest[k] = j; &#125; &#125; System.out.println(closest[j]+\"&lt;--&gt;\"+j); &#125; &#125; public static void main(String[] args) &#123; int MAX_WEIGHT =Integer.MAX_VALUE; // matri[i][j] = a 表示顶点i到j的权值为a int[][] matrix = &#123; &#123;MAX_WEIGHT,MAX_WEIGHT,MAX_WEIGHT,MAX_WEIGHT,MAX_WEIGHT,MAX_WEIGHT,MAX_WEIGHT&#125;, &#123;MAX_WEIGHT,0,6,1,5,MAX_WEIGHT,MAX_WEIGHT&#125;, &#123;MAX_WEIGHT,6,0,5,MAX_WEIGHT,3,MAX_WEIGHT&#125;, &#123;MAX_WEIGHT,1,5,0,5,6,4&#125;, &#123;MAX_WEIGHT,5,MAX_WEIGHT,5,0,MAX_WEIGHT,2&#125;, &#123;MAX_WEIGHT,MAX_WEIGHT,3,6,MAX_WEIGHT,0,6&#125;, &#123;MAX_WEIGHT,MAX_WEIGHT,MAX_WEIGHT,4,2,6,0&#125; &#125;; int n = matrix.length; Prim(n, matrix); &#125;&#125; Kruskal 算法首先将G的n个顶点看成n个孤立的连通分支。将所有的边按权从小到大排序。然后从第一条边开始，依边权递增的顺序查看每一条边，并按下述方法连接2个不同的连通分支：当查看到第k条边(v,w)时，如果端点v和w分别是当前2个不同的连通分支T1和T2中的顶点时，就用边(v,w)将T1和T2连接成一个连通分支，然后继续查看第k+1条边；如果端点v和w在当前的同一个连通分支中，就直接再查看第k+1条边。这个过程一直进行到只剩下一个连通分支时为止。","tags":[{"name":"算法","slug":"算法","permalink":"http://wenbo.fun/tags/算法/"}]},{"title":"贪心算法---单源最短路径","date":"2017-12-30T02:34:34.000Z","path":"2017/12/30/sf007/","text":"单源最短路径问题描述 计算机算法设计与分析(第四版)，王晓东 给定一个带权有向图 G=（V,E），其中每条边的权是一个非负实数。另外，还给定V中的一个顶点，称为源。现在要计算从源到其他所有各顶点的最短路径长度。这里的长度就是指路上各边权之和。 其基本思想是，设置顶点集合S并不断地作贪心选择来扩充这个集合。每一次加入S 的点都是距离S之外最短的点。 贪心选择性质该算法所作出的贪心选择是从V-S中选择具有最短特殊路径的顶点u，从而确定从源S到u的最短路径长度为dist[u]. 反证: dist[u]表示从已挑选集合源S到u的最短路径长度，假设有一点x不属于S使得从源经过x再到u的距离比dist[u]更短。d(v,x)表示从S中源点v到x的距离，显然:$$dist[x] &lt;= d(v,x)$$由假设可得:$$d(v,x) + d(x,u) = d(v,u) &lt; dist[u] $$因为$$d(x,u) &gt;= 0$$推出$$dist[x]&lt;dist[u]$$这就表明x就是距离源外最短的点，也就是原来的u，而不是假设的 中间点，故此为矛盾，证毕。 最优子结构性质java 代码12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364public class Dijkstra &#123; static float maxint = Float.MAX_VALUE-1000; /** * @param args */ public static void main(String[] args) &#123; // TODO Auto-generated method stubfloat c[ ][ ] =&#123;&#123;maxint,maxint,maxint,maxint,maxint,maxint&#125;,&#123;maxint,maxint,10,maxint,30,100&#125;,&#123;maxint,10,maxint,50,maxint,maxint&#125;,&#123;maxint,maxint,50,maxint,20,10&#125;,&#123;maxint,maxint,maxint,20,maxint,60&#125;,&#123;maxint,100,maxint,10,60,maxint&#125;&#125;; float dist[ ] = new float [7]; int prev[ ] = new int [7]; int n = 5; int v = 1; dijkstra(n, v, dist, prev, c); &#125; /** * @param n n个顶点 * @param v 顶点v为源点 * @param dist dist[i]记录源点到i顶点最短特殊路径长度 * @param prev prev[i]记录源点到i顶点最短路径上，i的前一个顶点 * @param c c[i][j]表示边(i,j)的权 */ public static void dijkstra(int n, int v, float dist[ ],int prev[ ],float c[ ][ ]) &#123; boolean[ ] s = new boolean[n + 1]; for (int i = 1; i &lt;= n ; i++) &#123; dist[i] = c[v][i]; s[i] = false; if(dist[i] == maxint) prev[i] = 0; else prev[i] = v; &#125; dist[v] = 0; s[v] = true; for (int i = 1; i &lt; n; i++) &#123; float temp = maxint; int u =v; for (int j = 1; j &lt; n; j++) &#123; if ( (!s[j]) &amp;&amp; (dist[j] &lt; temp) ) &#123; u = j; temp = dist[j]; &#125; &#125; s[u] = true; for (int j = 1; j &lt;= n; j++) &#123; if ((!s[j]) &amp;&amp; (c[u][j] &lt; maxint) ) &#123; float newdist = dist[u] + c[u][j]; if(newdist &lt; dist[j] ) &#123; dist[j] = newdist; prev[j] = u; &#125; &#125; &#125; &#125; for(int i=2;i&lt;=5;i++)&#123; System.out.println(\"prev[\"+i+\"] = \"+prev[i]); &#125; for(int i=2;i&lt;=5;i++)&#123; System.out.println(\"dist[\"+i+\"] = \"+dist[i]); &#125; &#125;&#125;","tags":[{"name":"算法","slug":"算法","permalink":"http://wenbo.fun/tags/算法/"}]},{"title":"贪心算法---活动安排问题","date":"2017-12-28T11:46:02.000Z","path":"2017/12/28/sf006/","text":"活动安排问题问题描述 计算机算法设计与分析(第四版)，王晓东 贪心选择性质 所谓贪心选择性质是指所求问题的整体最优解可以通过一系列局部最优的选择. 动态规划: 每步所做的选择往往依赖于相关子问题的解。只有解除子问题的解才能做出选择。_自底向上_ 贪心算法: 仅在当前状态下做出最好选择，即局部最优选择。_自顶向下_ 对于一个具体问题，要确定它是否具有 贪心选择性质,必须证明每一步所作的贪心选择最终导致问题的整体最优解。 活动安排问题贪的是 活动最早完成时间 ，直观上为未安排活动留下更多时间。 最优子结构性质若集合 A 是所有活动集 E 中包含活动 1 的一个最优解，则集合 A’ = A - {1} 也是对于 E’ (以活动1结束时间为起始时间)的一个最优解。 反证: 若能找到 E’ 的一个解 B’ ,它包含比 A’ 更多的活动，则将活动1加入到 B’ 将产生 E 的一个解 B ，它包含比 A 更多的活动，这与 A 的最优性矛盾。 时间复杂度算法排序起始时间: $$O(nlogn)$$算法安排活动只需: $$\\theta(n)$$ java 代码1234567891011121314151617181920212223242526272829303132333435public class GreedySelector &#123; /** * * @param s 活动开始时间 * @param f 活动结束时间 * @param A 是否选择活动 * @return 已选择活动数 */ public static int greedyselector(int[] s,int[] f,boolean[] A) &#123; int n = s.length - 1; int count = 1; A[1] = true; int j = 1; System.out.print(\"start:(1)\"+ s[1] + \"-&gt;\" + f[1] + \"-&gt;\"); for(int i=2;i&lt;=n;i++) &#123; if (s[i] &gt;= f[j]) &#123; A[i] = true; j=i;count++; System.out.print(\"(\"+i+\")\"+ s[i] + \"-&gt;\" + f[i] + \"-&gt;\"); &#125;else &#123; A[i] = false; &#125; &#125; System.out.print(\"end\\n\"); return count; &#125; public static void main(String[] args) &#123; int s[]=&#123;1,3,0,5,3,5,6,8,8,2,12&#125;; int f[]=&#123;4,5,6,7,8,9,10,11,12,13,14&#125;; boolean [] a=new boolean[11]; int c = greedyselector(s, f, a); System.out.println(\"共有\"+c+\"个活动被安排。\"); &#125;&#125;","tags":[{"name":"算法","slug":"算法","permalink":"http://wenbo.fun/tags/算法/"}]},{"title":"动态规划---最长公共子序列","date":"2017-12-26T11:33:31.000Z","path":"2017/12/26/sf005/","text":"最长公共子序列问题描述 计算机算法设计与分析(第四版)，王晓东 最优子结构性质设序列 X={x1,x2,…,xm} 和 Y={y1,y2,…,yn} 的最长公共子序列为 Z={z1,z2,…,zk} ，则 若xm=yn，则zk=xm=yn，且Z k-1是xm-1和yn-1的最长公共子序列。 若xm≠yn且zk≠xm，则Z是xm-1和Y的最长公共子序列。 若xm≠yn且zk≠yn，则Z是X和yn-1的最长公共子序列。 两个序列的最长公共子序列包含了这两个序列的前缀的最长公共子序列。因此，最长公共子序列问题具有 最优子结构性质。 子问题递归结构用c[i][j]记录序列Xi和Yj的最长公共子序列的长度。其中，$$ Xi={x1,x2,…,xi};Yj={y1,y2,…,yj}$$当 i=0 或 j=0 时，空序列是 Xi 和 Yj 的最长公共子序列。故此时C[i][j]=0。其他情况下，由最优子结构性质可建立递归关系如下: java 代码 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162package dtgh;public class LongestCommonSub &#123; public static void main(String[] args) &#123; // TODO Auto-generated method stub String str1 = \"ABCBDAB\"; String str2 = \"BDCABA\"; //计算lcs递归矩阵 int[][] re = longestCommonSubsequence(str1, str2); //打印矩阵 for (int i = 0; i &lt;= str1.length(); i++) &#123; for (int j = 0; j &lt;= str2.length(); j++) &#123; System.out.print(re[i][j] + \" \"); &#125; System.out.println(); &#125; System.out.println(); System.out.println(); //输出LCS print(re, str1, str2, str1.length(), str2.length()); &#125; // 假如返回两个字符串的最长公共子序列的长度 public static int[][] longestCommonSubsequence(String str1, String str2) &#123; int[][] matrix = new int[str1.length() + 1][str2.length() + 1];//建立二维矩阵 // 初始化边界条件 for (int i = 0; i &lt;= str1.length(); i++) &#123; matrix[i][0] = 0;//每行第一列置零 &#125; for (int j = 0; j &lt;= str2.length(); j++) &#123; matrix[0][j] = 0;//每列第一行置零 &#125; // 填充矩阵 for (int i = 1; i &lt;= str1.length(); i++) &#123; for (int j = 1; j &lt;= str2.length(); j++) &#123; if (str1.charAt(i - 1) == str2.charAt(j - 1)) &#123; matrix[i][j] = matrix[i - 1][j - 1] + 1; &#125; else &#123; matrix[i][j] = (matrix[i - 1][j] &gt;= matrix[i][j - 1] ? matrix[i - 1][j] : matrix[i][j - 1]); &#125; &#125; &#125; return matrix; &#125; //根据矩阵输出LCS public static void print(int[][] opt, String X, String Y, int i, int j) &#123; if (i == 0 || j == 0) &#123; return; &#125; if (X.charAt(i - 1) == Y.charAt(j - 1)) &#123; print(opt, X, Y, i - 1, j - 1); System.out.print(X.charAt(i - 1)); &#125; else if (opt[i - 1][j] &gt;= opt[i][j]) &#123; print(opt, X, Y, i - 1, j); &#125; else &#123; print(opt, X, Y, i, j - 1); &#125; &#125; &#125; python代码12345678910111213141516171819202122232425262728293031323334353637383940def lcs(a,b): lena=len(a) lenb=len(b) c=[[0 for i in range(lenb+1)] for j in range(lena+1)] flag=[[0 for i in range(lenb+1)] for j in range(lena+1)] for i in range(lena): for j in range(lenb): if a[i]==b[j]: c[i+1][j+1]=c[i][j]+1 flag[i+1][j+1]='ok' elif c[i+1][j]&gt;c[i][j+1]: c[i+1][j+1]=c[i+1][j] flag[i+1][j+1]='left' else: c[i+1][j+1]=c[i][j+1] flag[i+1][j+1]='up' return c,flag def printLcs(flag,a,i,j): if i==0 or j==0: return if flag[i][j]=='ok': printLcs(flag,a,i-1,j-1) print(a[i-1],end='') elif flag[i][j]=='left': printLcs(flag,a,i,j-1) else: printLcs(flag,a,i-1,j) a='ABCBDAB' b='BDCABA' c,flag=lcs(a,b) for i in c: print(i) print('') for j in flag: print(j) print('') printLcs(flag,a,len(a),len(b)) print('')","tags":[{"name":"算法","slug":"算法","permalink":"http://wenbo.fun/tags/算法/"}]},{"title":"动态规划---矩阵连乘","date":"2017-12-23T14:08:08.000Z","path":"2017/12/23/sf004/","text":"矩阵连乘问题描述 计算机算法设计与分析(第四版)，王晓东 动态规划基本要素最优子结构 当一个问题的最优解包括其子问题的最优解时，称此问题具有最优子结构性质。 计算 A[1:n] 的最优次序，其所包含的矩阵子链 A[1:k] 和 A[k+1:n] 的次序也是最优的。即该问题具有 最优子结构性质。 反证法证明：如果有一个计算 A[1:k] 的次序需要的计算量更少，则用此次序替换原来计算 A[1:k] 次序，得到的计算 A[1:n] 的计算量将比按最优次序计算量更少，这与前提矛盾。计算最优值:因为n个元素对2求组合数最多为 n(n-1)/2，另外还有 n 个单矩阵的子问题,因此，不同子问题的个数最多只有: 重叠子问题 区别于分治法,动态规划可以解决有些问题用分治法计算存在的大量重复计算。 最优解递归关系将矩阵连乘积 $$ A_{i}A_{i-1}A_{i-2}A_{i-3}…. $$简记为 $$A[i:j],i≤j$$设计算 $$A[i:j]，1≤i≤j≤n$$ 所需要的最少数乘次数m[i,j]，则原问题的最优值为m[1,n].则 m[i][j] 可递归定义为: 算法复杂度分析算法MatrixChain的主要计算量取决于算法中的3重循环。循环体内的计算量为O(1)，而3重循环的总次数为O(n^3)。因此算法的计算时间上界为O(n^3)。算法所占用的空间显然为O(n^2)。 java代码 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980public class MatrixMul &#123; public static void main(String[] args) &#123; // TODO Auto-generated method stub /** *p[i]的含义有两层指的是： *1. 第i个矩阵的列 *2. 第i+1个矩阵的行 **/ int []p = &#123;30,35,15,5,10,20,25&#125;; int len = p.length; int [][]m = new int [len+1][len+1];//记录连乘次数 int [][]s = new int [len+1][len+1];//记录最佳分割位置 MatrixChain(p,len,m,s); System.out.println(\"矩阵计算量最小次数矩阵m[i][j]:\"); PrintMatrixMul(m,p.length); System.out.println(\"相对于M矩阵的最优断开位置矩阵s[i][j]:\"); PrintMatrixMul(s,p.length); System.out.println(\"乘法的最优次序：\"); traceback(1,len-1,s); &#125; // 打印数组 private static void PrintMatrixMul(int[][] m,int n) &#123; for(int i = 1;i&lt;=n-1;i++)&#123; for(int j =1;j&lt;=n-1;j++)&#123; System.out.print(m[i][j]+\"\\t\"); if(j % (n-1) ==0)System.out.print(\"\\n\"); &#125; &#125; &#125; /** * * @param p 输入参数&#123;p0，p1，p2，...&#125; * @param n * @param m 最优值数组 * @param s 最优断开位置数组 */ public static void MatrixChain(int[] p, int n, int [][]m, int [][]s) &#123; for (int i = 1; i &lt;= n; i++) &#123;m[i][i] = 0;&#125;// 初始化单个矩阵连乘 for (int r = 2; r &lt;= n; r++) &#123; for (int i = 1; i &lt;= n-r; i++) &#123; int j = i+r-1; m[i][j] = m[i+1][j]+p[i-1]*p[i]*p[j]; s[i][j] = i; for (int k = i+1; k &lt; j; k++) &#123; int t = m[i][k] + m[k+1][j] + p[i-1]*p[k]*p[j]; if (t &lt; m[i][j]) &#123; m[i][j] = t;s[i][j] = k; &#125; &#125; &#125; &#125; &#125; /** * * @param i * @param j * @param s */ private static void traceback(int i, int j, int[][] s) &#123; if (i == j) &#123; System.out.print(\"A\"+i); &#125;else if(i+1 == j )&#123; System.out.print(\" (A\"+i+\" * \"+\" A\"+j+\") \"); &#125;else &#123; System.out.print(\" (\"); traceback(i, s[i][j], s); traceback(s[i][j]+1,j, s); System.out.print(\") \"); &#125; &#125;&#125; python代码12345678910111213141516171819202122232425262728293031323334353637class Matrix: def __init__(self, row_num=0, col_num=0, matrix=None): if matrix != None: self.row_num = len(matrix) self.col_num = len(matrix[0]) else: self.row_num = row_num self.col_num = col_num self.matrix = matrix def matrix_chain(matrixs): matrix_num = len(matrixs) count = [[0 for j in range(matrix_num)] for i in range(matrix_num)] flag = [[0 for j in range(matrix_num)] for i in range(matrix_num)] for r in range(1, matrix_num + 1): for i in range(matrix_num - r): j = i + interval count[i][j] = count[i][i] + count[i + 1][j] + matrixs[i].row_num * matrixs[i + 1].row_num * matrixs[j].col_num flag[i][j] = i for k in range(i + 1, j): temp = count[i][k] + count[k + 1][j] + matrixs[i].row_num * matrixs[k + 1].row_num * matrixs[j].col_num if temp &lt; count[i][j]: count[i][j] = temp flag[i][j] = k traceback(0, matrix_num - 1, flag) return count[0][matrix_num - 1] def traceback(i, j, flag): if i == j: return if j - i &gt; 1: print(str(i + 1) + '~' + str(j + 1), end=': ') print(str(i + 1) + \":\" + str(flag[i][j] + 1), end=',') print(str(flag[i][j] + 2) + \":\" + str(j + 1)) traceback(i, flag[i][j], flag) traceback(flag[i][j] + 1, j, flag) matrixs = [Matrix(30, 35), Matrix(35, 15), Matrix(15, 5), Matrix(5, 10), Matrix(10, 20), Matrix(20, 25)] result = matrix_chain(matrixs) print(result)","tags":[{"name":"算法","slug":"算法","permalink":"http://wenbo.fun/tags/算法/"}]},{"title":"递归与分治---快速排序","date":"2017-12-22T11:33:07.000Z","path":"2017/12/22/sf003/","text":"快速排序问题描述 计算机算法设计与分析(第四版)，王晓东 时间复杂度分析最坏情况:每次划分的基准刚好为中值。 $$T(n)=2*T(n/2)+n$$ 其平均算法复杂度为: $$O(nlogn)$$ java代码1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283import java.util.Random;public class QuickSort &#123; public static void main (String[] args) &#123; int[] array = &#123;6,7,4,5,2,3,1&#125;; int[] arr = QuickSort.Quicksort(array, 0, array.length-1); System.out.println(\"最终结果\"); for (int i : arr) &#123; System.out.print(i + \" \"); &#125; System.out.println(\"&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;\"); System.out.println(\"随机选择策略：\"); int[] array2 = &#123;6,7,4,5,2,3,1&#125;; int[] arr2 = QuickSort.RandomizedQuicksort(array2, 0, array.length-1); System.out.println(\"最终结果\"); for (int i : arr2) &#123; System.out.print(i + \" \"); &#125; &#125; public static int Partition(int[] arr, int low, int high) &#123; int i = low,j = high+1; int base = arr[low];// 基准元素 while(true) &#123; while(arr[++i] &lt; base &amp;&amp; i &lt; high) &#123; System.out.println(\"从左往右找比\" + base + \"大的，左指针变动：--&gt;\" + i); &#125; while(arr[--j] &gt; base) &#123; System.out.println(\"从右往左找比\" + base + \"小的，右指针变动：--&gt;\" + j); &#125; if(i &gt;= j) break; Swap(arr, i, j); &#125; arr[low] = arr[j]; arr[j] = base; System.out.println(\"对调\" + arr[low] + \"与\" + arr[j] + \",得到\"); for (int k : arr) &#123; System.out.print(k + \" \"); &#125; System.out.println(); return j; &#125; private static int[] Quicksort(int[] arr, int low, int high) &#123; if (low &lt; high) &#123; int division = Partition(arr, low, high); Quicksort (arr, low, division - 1); Quicksort (arr, division + 1 , high); &#125; return arr; &#125; public static void Swap(int[] array, int a, int b) &#123; if(a != b) &#123; int temp = array[a]; array[a] = array[b]; array[b] = temp; System.out.println(\"对调\" + array[a] + \"与\" + array[b] + \",得到\"); for (int i : array) &#123; System.out.print(i + \" \"); &#125; System.out.println(); &#125; &#125; private static int[] RandomizedQuicksort(int[] arr, int low, int high) &#123; if (low &lt; high) &#123; int q = RandomizedPartition(arr, low, high); RandomizedQuicksort (arr, low, q - 1); RandomizedQuicksort (arr, q + 1 , high); &#125; return arr; &#125; public static int RandomizedPartition(int[] arr, int low, int high) &#123; Random random = new Random(System.currentTimeMillis()); int i = random.nextInt(high-low)+low; Swap(arr, i, low); return Partition(arr, low, high); &#125;&#125; python代码12345678910111213141516171819202122232425def QuickSort(arr,firstIndex,lastIndex): if firstIndex&lt;lastIndex: divIndex=Partition(arr,firstIndex,lastIndex) QuickSort(arr,firstIndex,divIndex) QuickSort(arr,divIndex+1,lastIndex) else: returndef Partition(arr,firstIndex,lastIndex): i=firstIndex-1 for j in range(firstIndex,lastIndex): if arr[j]&lt;=arr[lastIndex]: i=i+1 arr[i],arr[j]=arr[j],arr[i] arr[i+1],arr[lastIndex]=arr[lastIndex],arr[i+1] return iarr=[1,4,7,1,5,5,3,85,34,75,23,75,2,0]print(\"initial array:\\n\",arr)QuickSort(arr,0,len(arr)-1)print(\"result array:\\n\",arr)","tags":[{"name":"算法","slug":"算法","permalink":"http://wenbo.fun/tags/算法/"}]},{"title":"递归与分治---合并排序","date":"2017-12-22T01:55:32.000Z","path":"2017/12/22/sf002/","text":"合并排序 (归并排序)问题描述 计算机算法设计与分析(第四版)，王晓东 时间复杂度分析 java代码1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950public class MergeSort &#123; public static void main (String[] args) &#123; int[] array = &#123;1,3,4,6,2,7,5&#125;; int[] arr = new MergeSort().sort(array); for (int i = 0; i &lt; arr.length; i++) &#123; System.out.print(arr[i]); &#125; &#125; public int[] sort(int[] arrs)&#123; if(arrs.length &lt; 2)&#123; return arrs; &#125; int middle = arrs.length % 2 == 0 ? arrs.length / 2 : (arrs.length - 1) / 2; int[] left = Arrays.copyOfRange(arrs, 0, middle); int[] right = Arrays.copyOfRange(arrs, middle, arrs.length); int[] lres = sort(left); // 递归 int[] rres = sort(right); return merge(lres, rres); &#125; private int[] merge(int[] lres, int[] rres) &#123; // 合并左右数组 int[] res = new int[lres.length + rres.length]; int l = 0; int r = 0; int c = 0; while(l &lt; lres.length &amp;&amp; r &lt; rres.length)&#123; if(lres[l] &lt; rres[r])&#123; res[c++] = lres[l++]; &#125; else &#123; res[c++] = rres[r++]; &#125; &#125; if(l == lres.length)&#123; while(r &lt; rres.length)&#123; res[c++] = rres[r++]; &#125; return res; &#125; if(r == rres.length)&#123; while(l &lt; lres.length)&#123; res[c++] = lres[l++]; &#125; return res; &#125; return res; &#125;&#125; python 代码一般方法123456789101112131415161718192021222324252627282930def merge_sort(seq): if len(seq)== 1: return seq else: middle = len(seq)//2 left = merge_sort(seq[:middle]) right = merge_sort(seq[middle:]) i = 0 // left j = 0 // right k = 0 // all while i &lt; len(left) and j &lt; len(right): if left[i] &lt; right[j]: seq[k] = left[i] i += 1 k += 1 else: seq[k] = right[j] j += 1 k += 1 // remain of left or right remain = left if i&lt;j else right r = i if remain == left else j while r&lt;len(remain): seq[k] = remain[r] r += 1 k += 1 return seq pop123456789101112131415161718def merge_sort_w(seq): if len(seq) &lt;= 1: return seq def merge(left,right): merged = [] while left and right: merged.append(left.pop(0) if left[0] &lt;= right[0] else right.pop(0)) // remain while left: merged.append(left.pop(0)) while right: merged.append(right.pop(0)) return merged middle = int(len(seq) / 2) left = merge_sort_w(seq[:middle]) right = merge_sort_w(seq[middle:]) return merge(left,right) heapq12345678910from heapq import mergedef merge_sort_h(seq): if len(seq) &lt;= 1: return seq else: middle = len(seq)//2 left = merge_sort_h(seq[:middle]) right = merge_sort_h(seq[middle:]) return list(merge(left, right)) # heapq.merge","tags":[{"name":"算法","slug":"算法","permalink":"http://wenbo.fun/tags/算法/"}]},{"title":"递归与分治---棋盘覆盖","date":"2017-12-20T01:38:17.000Z","path":"2017/12/20/sf001/","text":"棋盘覆盖问题描述 计算机算法设计与分析(第四版)，王晓东 在一个2^k * 2^k个方格组成的棋盘中,若有一个方格与其他方格不同,则称该方格为一特殊方格,且称该棋盘为一个特殊棋盘。显然特殊方格在棋盘上出现的位置有4^k种情形. 因而对任何k ≥ 0,有 4^k 种不同的特殊棋盘。下图所示的特殊棋盘为 k=2 时 16 个特殊棋盘中的一个。 在棋盘覆盖问题中，要用下图中 4 中不同形态的 L 型骨牌覆盖一个给定的特殊棋牌上除特殊方格以外的所有方格，且任何 2 个 L 型骨牌不得重叠覆盖。易知，在任何一个 2^k * 2^k 的棋盘中，用到的 L 型骨牌个数恰为 (4^k-1)/3 。 求解棋盘问题，可利用分治的策略。当 k&gt;0 时，将 2^k 2^k 棋盘分割为 4 个 2^(k-1) 2^(k-1) 子棋盘，如下图所示。 特殊方格必位于 4 个较小子棋盘之一中，其余 3 个子棋盘中无特殊方格。用一个 L 型骨牌覆盖这 3 个较小的棋盘的 汇合处 ，如图所示，将这 3 个无特殊方格的子棋盘转化为 特殊棋盘，从而将原问题化为 4 个较小规模的棋盘覆盖问题。递归的使用 这种分割，直至棋盘简化为 1x1 棋盘。 时间复杂度分析 java 代码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125package dgfz;import java.awt.Color;import java.awt.Container;import java.awt.Graphics;import javax.swing.JFrame;public class qpfg &#123; public static void main (String[] args) &#123; new DrawPanel(); &#125;&#125;class DrawPanel extends JFrame &#123; //设定特殊棋子位置坐标 private int dx = 2; private int dy = 1; //棋盘大小 private int dsize = 8; // 2^k * 2^k //小方块边长 private static final int c = 50; // 窗口大小 int a = dsize*c+50; // L型骨牌号 private static int tile = 0; private Graphics jp; /** * DrawPanel构造方法 */ public DrawPanel() &#123; Container p = getContentPane(); setBounds(100, 100, a+100, a+100); setVisible(true); p.setBackground(Color.WHITE); setLayout(null); setResizable(false); this.setDefaultCloseOperation(JFrame.EXIT_ON_CLOSE); try &#123; // 等待时间 Thread.sleep(500); &#125;catch (Exception e) &#123; e.printStackTrace(); &#125; jp = this.getGraphics(); //绘制 painPanel(jp); &#125;public void painPanel(Graphics g) &#123; /** * 画棋盘 * */ try &#123; //画格子 g.setColor(Color.RED); g.drawRect(c, c, dsize*c, dsize*c); for (int i = 1; i &lt; dsize; i++) &#123; g.drawLine(c + (i * c), c, c + (i * c), a); g.drawLine(c,c + (i * c), a, c + (i * c)); &#125; //设定特殊棋子位置 g.setColor(Color.BLACK); g.fillRect(c*dx+5, c*dy+5, c-10, c-10); g.setColor(Color.RED); Thread.sleep(500); chess(1, 1, dx, dy, dsize, g); &#125;catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;public void chess(int tr, int tc, int dr, int dc, int size, Graphics g) &#123; /* * 棋盘分割 * tr,tc:棋盘左上角行列号 * dr,dc:特殊方格行列号 * size： size = 8，就是棋盘大小8 x 8 * */ try &#123; if (size == 1) return ; String tt = String.valueOf(tile++); int s = size / 2; //左上 if(dr &lt; tr + s &amp;&amp; dc &lt; tc + s) // 特殊方格在此棋盘中 chess(tr, tc, dr, dc, s, g); else &#123; // 特殊方格不在此棋盘中，用L型骨牌号 tt 填入右下角 g.drawString(tt, (tr + s - 1)*c+c/2, (tc + s - 1)*c+c/2); System.out.println((tr+s-1)+\",\"+(tc + s - 1)+\"---&gt;\"+tt); chess(tr, tc, tr+s-1, tc+s -1, s, g); &#125; //右上 if(dr &lt; tr + s &amp;&amp; dc &gt;= tc + s) chess(tr, tc + s, dr, dc, s, g); else &#123; g.drawString(tt, (tr + s - 1)*c+c/2, (tc + s)*c+c/2); System.out.println((tr+s-1)+\",\"+(tc + s)+\"---&gt;\"+tt); chess(tr, tc + s, tr+s-1, tc+s , s, g); &#125; //左下 if(dr &gt;= tr + s &amp;&amp; dc &lt; tc + s) chess(tr + s, tc, dr, dc, s, g); else &#123; g.drawString(tt, (tr + s )*c+c/2, (tc + s - 1)*c+c/2); System.out.println((tr+s)+\",\"+(tc + s - 1)+\"---&gt;\"+tt); chess(tr +s , tc, tr+s, tc+s -1, s, g); &#125; //右下 if(dr &gt;= tr + s &amp;&amp; dc &gt;= tc + s) chess(tr + s, tc + s, dr, dc, s, g); else &#123; g.drawString(tt, (tr + s)*c+c/2, (tc + s)*c+c/2); System.out.println((tr+s)+\",\"+(tc + s)+\"---&gt;\"+tt); chess(tr + s, tc + s, tr+s, tc+s , s, g); &#125; Thread.sleep (500 ) ; &#125; catch (InterruptedException ie)&#123; &#125; &#125;&#125; python 代码 12345678910111213141516171819202122232425262728293031323334353637383940// tr左上角行号，tc左上角列号。dr特殊方格行号，dc特殊方格列号def chessboard(board, size, tr, tc, dr, dc): if size &lt;= 1: return global tile tile += 1 current_tile = tile size //= 2 if dr &lt; tr + size and dc &lt; tc + size: chessboard(board, size, tr, tc, dr, dc) else: board[tr + size - 1][tc + size - 1] = current_tile chessboard(board, size, tr, tc, tr + size - 1, tc + size - 1) if dr &gt;= tr + size and dc &lt; tc + size: chessboard(board, size, tr + size, tc, dr, dc) else: board[tr + size][tc + size - 1] = current_tile chessboard(board, size, tr + size, tc, tr + size, tc + size - 1) if dr &lt; tr + size and dc &gt;= tc + size: chessboard(board, size, tr, tc + size, dr, dc) else: board[tr + size - 1][tc + size] = current_tile chessboard(board, size, tr, tc + size, tr + size - 1, tc + size) if dr &gt;= tr + size and dc &gt;= tc + size: chessboard(board, size, tr + size, tc + size, dr, dc) else: board[tr + size][tc + size] = current_tile chessboard(board, size, tr + size, tc + size, tr + size, tc + size)tile = 0chessboard_size = 4board = [[0 for x in range(chessboard_size)] for y in range(chessboard_size)]chessboard(board, chessboard_size, 0, 0, 1, 0)board = [[row[i] for row in board] for i in range(len(board[0]))]for lst in board: print(lst)","tags":[{"name":"算法","slug":"算法","permalink":"http://wenbo.fun/tags/算法/"}]},{"title":"Sklearn中Kmeans与可视化","date":"2017-12-13T14:02:42.000Z","path":"2017/12/13/kmeans-sklearn/","text":"KmeansK-means算法: 两个缺点: Inertia makes the assumption that clusters are convex and isotropic, which is not always the case. It responds poorly to elongated clusters, or manifolds with irregular shapes. Inertia is not a normalized metric: we just know that lower values are better and zero is optimal. But in very high-dimensional spaces, Euclidean distances tend to become inflated (this is an instance of the so-called “curse of dimensionality”). Running a dimensionality reduction algorithm such as PCA prior to k-means clustering can alleviate this problem and speed up the computations. 聚类算法：from sklearn.cluster import KMeans def init(self, n_clusters=8, init=’k-means++’, n_init=10, max_iter=300,tol=1e-4,precompute_distances=’auto’,verbose=0, random_state=None, copy_x=True, n_jobs=1): km_cluster = KMeans(n_clusters=num_clusters, max_iter=300, n_init=40,init=’k-means++’,n_jobs=-1) n_clusters : 分成的簇数也是要生成的质心数 init: 初始化质心,默认值：采用 k-means++ n_init: 设置选择质心种子次数，默认为10次。返回质心最好的一次结果（好是指计算时长短） max_ite: 每次迭代的最大次数 tol: 容忍的最小误差，当误差小于tol就会退出迭代（算法中会依赖数据本身）,默认值：le-4(0.0001) precompute_distances: 这个参数会在空间和时间之间做权衡，如果是True 会把整个距离矩阵都放到内存中，auto 会默认在数据样本大于featurs*samples 的数量大于12e6 的时候False, 默认值：“auto” verbose:是否输出详细信息,默认值：False random_state: 随机生成器的种子 ，和初始化中心有关, 默认值：None copy_x:bool 在scikit-learn 很多接口中都会有这个参数的，就是是否对输入数据继续copy 操作，以便不修改用户的输入数据。这个要理解Python 的内存机制才会比较清楚。(待研究) 默认值：True n_jobs: 使用进程的数量，与电脑的CPU有关.默认值: 1 Kmeans++ 能够解决kmeans对噪声敏感的问题。kmeans寻找种子点的时候计算该类中所有样本的平均值，如果该类中具有较为明显的离群点，会造成种子点与期望偏差过大。例如，A(1,1),B(2,2),C(3,3),D(1000,1000)，显然D点会拉动种子点向其偏移。这样，在下一轮迭代时，将大量不该属于该类的样本点错误的划入该类。为了解决这个问题，kmedoids方法采取新的种子点选取方式: 1）只从样本点中选；2）选取标准能够提高聚类效果，例如最小化J函数，或者自定义其他的代价函数。但是，kmedoids方法提高了聚类的复杂度。 k-means++算法选择初始seeds的基本思想就是：初始的聚类中心之间的相互距离要尽可能的远。 wiki上对该算法的描述如下: 从输入的数据点集合中随机选择一个点作为第一个聚类中心 对于数据集中的每一个点x，计算它与最近聚类中心(指已选择的聚类中心)的距离D(x) 选择一个新的数据点作为新的聚类中心，选择的原则是：D(x)较大的点，被选取作为聚类中心的概率较大 重复2和3直到k个聚类中心被选出来 利用这k个初始的聚类中心来运行标准的k-means算法 code import 123456from sklearn.cluster import KMeansfrom sklearn.externals import joblibimport numpy as npfrom time import timeimport matplotlib.pyplot as pltfrom sklearn.manifold import TSNE 12345678910111213141516171819// load data print('Load data...') dataSet = [] fileIn = open('./result1.txt')// result1 是一个10986x34行，一列的数据 for line in fileIn.readlines(): a = line.strip().split(' ') dataSet.append(float(a[0])) B = np.reshape(dataSet,(10986,34))// 聚类数 num_clusters = 26 km_cluster = KMeans(n_clusters=num_clusters, max_iter=300, n_init=40, \\ init='k-means++',n_jobs=-1)// 聚类label结果 result = km_cluster.fit_predict(B) print (\"Predicting result: \", result) np.savetxt(\"a.txt\",result) 降维(二向箔？)将高维的聚类数据在降到二维上显示，使用 sklearn 的 TSNE. code1： 12345678910111213141516171819202122232425262728// B 是34维 x 10986XX = B// result 是所聚的labelYY = result// 可视化def plot_embedding(X, title=None): x_min, x_max = np.min(X, 0), np.max(X, 0) X = (X - x_min) / (x_max - x_min) plt.figure() ax = plt.subplot(111) for i in range(X.shape[0]): plt.text(X[i, 0], X[i, 1], str(YY[i]), color=plt.cm.Set1(YY[i] / 10.), fontdict=&#123;'weight': 'bold', 'size': 9&#125;) plt.xticks([]), plt.yticks([]) if title is not None: plt.title(title)// t-SNE embedding of the digits datasetprint(\"Computing t-SNE embedding\")tsne = TSNE(n_components=2, init='pca', random_state=0)t0 = time()X_tsne = tsne.fit_transform(B)plot_embedding(X_tsne,\"t-SNE embedding of the digits (time %.2fs)\" % (time() - t0))// plot_embedding_3d(X_tsne,\"t-SNE embedding of the digits (time %.2fs)\" % (time() - t0))plt.show() 效果图：(分类太多看不清楚233333) code 2 :( 网路上 ) 12345678910111213141516...from sklearn.manifold import TSNEtsne = TSNE()tsne.fit_transform(data_zs) // 进行数据降维tsne = pd.DataFrame(tsne.embedding_, index = data_zs.index) //转换数据格式import matplotlib.pyplot as pltplt.rcParams['font.sans-serif'] = ['SimHei'] //用来正常显示中文标签plt.rcParams['axes.unicode_minus'] = False //用来正常显示负号//不同类别用不同颜色和样式绘图d = tsne[r[u'聚类类别'] == 0]plt.plot(d[0], d[1], 'r.')d = tsne[r[u'聚类类别'] == 1]plt.plot(d[0], d[1], 'go')d = tsne[r[u'聚类类别'] == 2]plt.plot(d[0], d[1], 'b*')plt.show()","tags":[{"name":"聚类","slug":"聚类","permalink":"http://wenbo.fun/tags/聚类/"},{"name":"机器学习","slug":"机器学习","permalink":"http://wenbo.fun/tags/机器学习/"},{"name":"Kmeans","slug":"Kmeans","permalink":"http://wenbo.fun/tags/Kmeans/"}]},{"title":"人脸识别04：识别是不是我","date":"2017-12-10T13:12:46.000Z","path":"2017/12/10/is-my-face/","text":"代码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657.........sess = tf.Session()saver.restore(sess, tf.train.latest_checkpoint('.'))def is_my_face(image): res = sess.run(predict, feed_dict=&#123;x: [image/255.0], keep_prob_5:1.0, keep_prob_75: 1.0&#125;) if res[0] == 1: return True else: return False// 使用dlib自带的frontal_face_detector作为我们的特征提取器detector = dlib.get_frontal_face_detector()cam = cv2.VideoCapture(0)// Set FONT_HERSHEY_SIMPLEX// font = cv2.FONT_HERSHEY_SIMPLEXwhile True: _, img = cam.read() gray_image = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) dets = detector(gray_image, 1) if not len(dets): //print('Can`t get face.') cv2.imshow('img', img) key = cv2.waitKey(30) &amp; 0xff if key == 27: sys.exit(0) for i, d in enumerate(dets): x1 = d.top() if d.top() &gt; 0 else 0 y1 = d.bottom() if d.bottom() &gt; 0 else 0 x2 = d.left() if d.left() &gt; 0 else 0 y2 = d.right() if d.right() &gt; 0 else 0 face = img[x1:y1,x2:y2] // 调整图片的尺寸 face = cv2.resize(face, (size,size)) print('Is this my face? %s' % is_my_face(face)) cv2.rectangle(img, (x2,x1),(y2,y1), (255,0,0),3) //cv2.rectangle(im, (x-22,y-90), (x+w+22, y-22), (0,255,0), -1) //cv2.putText(img, 'right', (x2,x1-40), font, 2, (255,255,255), 3) cv2.imshow('image',img) key = cv2.waitKey(30) &amp; 0xff if key == 27: sys.exit(0)sess.close()","tags":[{"name":"CNN","slug":"CNN","permalink":"http://wenbo.fun/tags/CNN/"},{"name":"TensorFlow","slug":"TensorFlow","permalink":"http://wenbo.fun/tags/TensorFlow/"},{"name":"人脸识别","slug":"人脸识别","permalink":"http://wenbo.fun/tags/人脸识别/"}]},{"title":"人脸识别03：Tensorflow+CNN训练","date":"2017-12-08T14:05:35.000Z","path":"2017/12/08/cnn-train-faces/","text":"使用Tensorflow+CNN训练神经网络 程序代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179import tensorflow as tfimport cv2import numpy as npimport osimport randomimport sysfrom sklearn.model_selection import train_test_splitmy_faces_path = './my_faces'other_faces_path = './other_faces'size = 64imgs = []labs = []def getPaddingSize(img): h, w, _ = img.shape top, bottom, left, right = (0,0,0,0) longest = max(h, w) if w &lt; longest: tmp = longest - w //表示整除符号 left = tmp // 2 right = tmp - left elif h &lt; longest: tmp = longest - h top = tmp // 2 bottom = tmp - top else: pass return top, bottom, left, rightdef readData(path , h=size, w=size): for filename in os.listdir(path): if filename.endswith('.jpg'): filename = path + '/' + filename img = cv2.imread(filename) top,bottom,left,right = getPaddingSize(img) // 将图片放大，,给源图像增加边界 img = cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_CONSTANT, value=[0,0,0]) img = cv2.resize(img, (h, w)) imgs.append(img) labs.append(path)readData(my_faces_path)readData(other_faces_path)// 将图片数据与标签转换成数组imgs = np.array(imgs)labs = np.array([[0,1] if lab == my_faces_path else [1,0] for lab in labs])// 随机划分测试集与训练集 5%的测试集train_x,test_x,train_y,test_y = train_test_split(imgs, labs, test_size=0.05, random_state=random.randint(0,100))// 参数：图片数据的总数，图片的高、宽、通道// shape[0]行数train_x = train_x.reshape(train_x.shape[0], size, size, 3)test_x = test_x.reshape(test_x.shape[0], size, size, 3)// 将数据转换成小于1的数train_x = train_x.astype('float32')/255.0test_x = test_x.astype('float32')/255.0print('train size:%s, test size:%s' % (len(train_x), len(test_x)))// 图片块，每次取100张图片batch_size = 100num_batch = len(train_x) // batch_size// 保存数据 特定格式x = tf.placeholder(tf.float32, [None, size, size, 3])y_ = tf.placeholder(tf.float32, [None, 2])keep_prob_5 = tf.placeholder(tf.float32)keep_prob_75 = tf.placeholder(tf.float32)def weightVariable(shape): // 标准差 0.01 的正态分布 init = tf.random_normal(shape, stddev=0.01) return tf.Variable(init)def biasVariable(shape): init = tf.random_normal(shape) return tf.Variable(init)def conv2d(x, W): return tf.nn.conv2d(x, W, strides=[1,1,1,1], padding='SAME')def maxPool(x): return tf.nn.max_pool(x, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')def dropout(x, keep): return tf.nn.dropout(x, keep)def cnnLayer(): // 第一层 W1 = weightVariable([3,3,3,32]) // 卷积核大小(3,3)， 输入通道(3)， 输出通道(32) b1 = biasVariable([32]) // 卷积 // 作用是计算激活函数relu，即max(features, 0)。 conv1 = tf.nn.relu(conv2d(x, W1) + b1) // 池化 pool1 = maxPool(conv1) // 减少过拟合，随机让某些权重不更新 drop1 = dropout(pool1, keep_prob_5) // 第二层 W2 = weightVariable([3,3,32,64]) b2 = biasVariable([64]) conv2 = tf.nn.relu(conv2d(drop1, W2) + b2) pool2 = maxPool(conv2) drop2 = dropout(pool2, keep_prob_5) // 第三层 W3 = weightVariable([3,3,64,64]) b3 = biasVariable([64]) conv3 = tf.nn.relu(conv2d(drop2, W3) + b3) pool3 = maxPool(conv3) drop3 = dropout(pool3, keep_prob_5) // 全连接层 Wf = weightVariable([8*16*32, 512]) bf = biasVariable([512]) drop3_flat = tf.reshape(drop3, [-1, 8*16*32]) dense = tf.nn.relu(tf.matmul(drop3_flat, Wf) + bf) dropf = dropout(dense, keep_prob_75) // 输出层 Wout = weightVariable([512,2]) bout = weightVariable([2]) //out = tf.matmul(dropf, Wout) + bout out = tf.add(tf.matmul(dropf, Wout), bout) return outdef cnnTrain(): out = cnnLayer() cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=out, labels=y_)) train_step = tf.train.AdamOptimizer(0.01).minimize(cross_entropy) // 比较标签是否相等，再求的所有数的平均值，tf.cast(强制转换类型) accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(out, 1), tf.argmax(y_, 1)), tf.float32)) // 将loss与accuracy保存以供tensorboard使用 tf.summary.scalar('loss', cross_entropy) tf.summary.scalar('accuracy', accuracy) merged_summary_op = tf.summary.merge_all() // 数据保存器的初始化 saver = tf.train.Saver() with tf.Session() as sess: sess.run(tf.global_variables_initializer()) summary_writer = tf.summary.FileWriter('./tmp', graph=tf.get_default_graph()) for n in range(10): // 每次取128(batch_size)张图片 for i in range(num_batch): batch_x = train_x[i*batch_size : (i+1)*batch_size] batch_y = train_y[i*batch_size : (i+1)*batch_size] // 开始训练数据，同时训练三个变量，返回三个数据 _,loss,summary = sess.run([train_step, cross_entropy, merged_summary_op], feed_dict=&#123;x:batch_x,y_:batch_y, keep_prob_5:0.5,keep_prob_75:0.75&#125;) summary_writer.add_summary(summary, n*num_batch+i) // 打印损失 print(n*num_batch+i, loss) if (n*num_batch+i) % 100 == 0: // 获取测试数据的准确率 acc = accuracy.eval(&#123;x:test_x, y_:test_y, keep_prob_5:1.0, keep_prob_75:1.0&#125;) print(n*num_batch+i, acc) // 准确率大于0.98时保存并退出 if acc &gt; 0.98 and n &gt; 2: saver.save(sess, './train_faces.model', global_step=n*num_batch+i) sys.exit(0) print('accuracy less 0.98, exited!')cnnTrain() 相关函数tf.nn.dropout1tf.nn.dropout(x, keep) dropout(x, keep_prob, noise_shape=None, seed=None, name=None) x: 一个Tensor。keep_prob: 一个 Python 的 float 类型。表示元素是否放电的概率。noise_shape: 一个一维的Tensor，数据类型是int32。代表元素是否独立的标志。seed: 一个Python的整数类型。设置随机种子。name: （可选）为这个操作取一个名字。 tf.nn.conv2d1return tf.nn.conv2d(x, W, strides=[1,1,1,1], padding=&apos;SAME&apos;) tf.nn.conv2d(input, filter, strides, padding, use_cudnn_on_gpu=None, name=None) 除去name参数用以指定该操作的name，与方法有关的一共五个参数： 第一个参数input：指需要做卷积的输入图像，它要求是一个Tensor，具有[batch, in_height, in_width, in_channels]这样的shape，具体含义是[训练时一个batch的图片数量, 图片高度, 图片宽度, 图像通道数]，注意这是一个4维的Tensor，要求类型为float32和float64其中之一。 第二个参数filter：相当于CNN中的卷积核，它要求是一个Tensor，具有[filter_height, filter_width, in_channels, out_channels]这样的shape，具体含义是[卷积核的高度，卷积核的宽度，图像通道数，卷积核个数]，要求类型与参数input相同，有一个地方需要注意，第三维in_channels，就是参数input的第四维 第三个参数strides：卷积时在图像每一维的步长，这是一个一维的向量，长度4 第四个参数padding：string类型的量，只能是”SAME”,”VALID”其中之一，这个值决定了不同的卷积方式，表示是否要保留不完全卷积的部分。 第五个参数：use_cudnn_on_gpu:bool类型，是否使用cudnn加速，默认为true 结果返回一个Tensor，这个输出，就是我们常说的feature map，shape仍然是[batch, height, width, channels]这种形式。 tf.nn.max_pool1tf.nn.max_pool(x, ksize=[1,2,2,1], strides=[1,2,2,1], padding=&apos;SAME&apos;) tf.nn.max_pool(value, ksize, strides, padding, name=None) 参数是四个，和卷积很类似：第一个参数value：需要池化的输入，一般池化层接在卷积层后面，所以输入通常是feature map，依然是[batch, height, width, channels]这样的shape第二个参数ksize：池化窗口的大小，取一个四维向量，一般是[1, height, width, 1]第三个参数strides：和卷积类似，窗口在每一个维度上滑动的步长，一般也是[1, stride,stride, 1]第四个参数padding：和卷积类似，可以取’VALID’ 或者’SAME’ 返回一个Tensor，类型不变，shape仍然是[batch, height, width, channels]这种形式 train_test_splitCross Validation 基本思想是把在某种意义下将原始数据(dataset)进行分组,一部分做为训练集(train set),另一部分做为验证集(validation set or test set),首先用训练集对分类器进行训练,再利用验证集来测试训练得到的模型(model),以此来做为评价分类器的性能指标。 sklearn.model_selection.train_test_split随机划分训练集和测试集官网文档：http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html#sklearn.model_selection.train_test_split train_test_split是交叉验证中常用的函数，功能是从样本中随机的按比例选取train data和testdata. 形式为： X_train,X_test, y_train, y_test = cross_validation.train_test_split(train_data,train_target,test_size=0.4, random_state=0) 参数解释： train_data：所要划分的样本特征集 train_target：所要划分的样本标签 test_size：样本占比，如果是整数的话就是样本的数量（0.4就是测试集占40%） random_state：是随机数的种子。 随机数种子： 其实就是该组随机数的编号，在需要重复试验的时候，保证得到一组一样的随机数。比如你每次都填1，其他参数一样的情况下你得到的随机数组是一样的。但填0或不填，每次都会不一样。随机数的产生取决于种子，随机数和种子之间的关系遵从以下两个规则： 1.种子不同，产生不同的随机数 2.种子相同，即使实例不同也产生相同的随机数。","tags":[{"name":"CNN","slug":"CNN","permalink":"http://wenbo.fun/tags/CNN/"},{"name":"TensorFlow","slug":"TensorFlow","permalink":"http://wenbo.fun/tags/TensorFlow/"},{"name":"人脸识别","slug":"人脸识别","permalink":"http://wenbo.fun/tags/人脸识别/"}]},{"title":"人脸识别02：dlib批量识别其他图片集","date":"2017-12-08T13:52:15.000Z","path":"2017/12/08/set-other-people/","text":"dlib批量识别其他图片集 处理方式跟上一个程序一样。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354import sysimport osimport cv2import dlibinput_dir = './input_img'output_dir = './other_faces'size = 64if not os.path.exists(output_dir): os.makedirs(output_dir)//使用dlib自带的frontal_face_detector作为我们的特征提取器detector = dlib.get_frontal_face_detector()index = 1for (path, dirnames, filenames) in os.walk(input_dir): for filename in filenames: if filename.endswith('.jpg'): print('Being processed picture %s' % index) img_path = path+'/'+filename //从文件读取图片 img = cv2.imread(img_path) //转为灰度图片 gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) // 使用detector进行人脸检测 dets为返回的结果 dets = detector(gray_img, 1) \"\"\" 使用enumerate 函数遍历序列中的元素以及它们的下标 下标i即为人脸序号 left：人脸左边距离图片左边界的距离 ；right：人脸右边距离图片左边界的距离 top：人脸上边距离图片上边界的距离 ；bottom：人脸下边距离图片上边界的距离 \"\"\" for i, d in enumerate(dets): x1 = d.top() if d.top() &gt; 0 else 0 y1 = d.bottom() if d.bottom() &gt; 0 else 0 x2 = d.left() if d.left() &gt; 0 else 0 y2 = d.right() if d.right() &gt; 0 else 0 // img[y:y+h,x:x+w] face = img[x1:y1,x2:y2] // 调整图片的尺寸 face = cv2.resize(face, (size,size)) cv2.imshow('image',face) // 保存图片 cv2.imwrite(output_dir+'/'+str(index)+'.jpg', face) index += 1 key = cv2.waitKey(30) &amp; 0xff if key == 27: sys.exit(0)","tags":[{"name":"人脸识别","slug":"人脸识别","permalink":"http://wenbo.fun/tags/人脸识别/"}]},{"title":"人脸识别01：dlib获取图片集","date":"2017-12-08T03:09:34.000Z","path":"2017/12/08/face-recognition-dlib/","text":"Dlib Dlib 是一个跨平台的C++公共库，除了线程支持，网络支持，提供测试以及大量工具等等优点，Dlib还是一个强大的机器学习的C++库，包含了许多机器学习常用的算法，而且它还不依赖其他库。 程序详解 get_my_faces_dlib.pyDlib 自带的 frontal_face_detector 特征提取器结合了现在经典的 梯度方向直方图 (Histogram of Oriented Gradients feature) Wikipedia, 参考 线性分类器 (linear classifier) Wikipedia,线性分类器与非线性分类器 图像金字塔 an image pyramid Wikipedia, 参考 滑窗检测 sliding window detection . 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475import cv2import dlibimport osimport sysimport randomoutput_dir = './my_faces'size = 64if not os.path.exists(output_dir): os.makedirs(output_dir)// 改变图片的亮度与对比度def relight(img, light=1, bias=0): w = img.shape[1] h = img.shape[0] //image = [] for i in range(0,w): for j in range(0,h): for c in range(3): tmp = int(img[j,i,c]*light + bias) if tmp &gt; 255: tmp = 255 elif tmp &lt; 0: tmp = 0 img[j,i,c] = tmp return img//使用dlib自带的frontal_face_detector作为我们的特征提取器detector = dlib.get_frontal_face_detector()//打开摄像头 参数为输入流，可以为摄像头或视频文件camera = cv2.VideoCapture(0)index = 1while True: if (index &lt;= 1000): print('Being processed picture %s' % index) // 从摄像头读帧 success, img = camera.read() // 转为灰度图片 gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) // 使用detector进行人脸检测 \"\"\" The 1 in the second argument indicates that we should upsample the image 1 time. This will make everything bigger and allow us to detect more faces. \"\"\" dets = detector(gray_img, 1) print(\"Number of faces detected: &#123;&#125;\".format(len(dets))) for i, d in enumerate(dets): print(\"Detection &#123;&#125;: Left: &#123;&#125; Top: &#123;&#125; Right: &#123;&#125; Bottom: &#123;&#125;\".format( i, d.left(), d.top(), d.right(), d.bottom())) x1 = d.top() if d.top() &gt; 0 else 0 y1 = d.bottom() if d.bottom() &gt; 0 else 0 x2 = d.left() if d.left() &gt; 0 else 0 y2 = d.right() if d.right() &gt; 0 else 0 // 裁剪 face = img[x1:y1,x2:y2] // 调整图片的对比度与亮度， 对比度与亮度值都取随机数，这样能增加样本的多样性 face = relight(face, random.uniform(0.5, 1.5), random.randint(-50, 50)) face = cv2.resize(face, (size,size)) cv2.imshow('image', face) cv2.imwrite(output_dir+'/'+str(index)+'.jpg', face) index += 1 key = cv2.waitKey(30) &amp; 0xff // Esc if key == 27: break else: print('Finished!') break","tags":[{"name":"人脸识别","slug":"人脸识别","permalink":"http://wenbo.fun/tags/人脸识别/"}]},{"title":"人脸识别：概述","date":"2017-12-07T13:42:08.000Z","path":"2017/12/07/人脸识别1/","text":"主环境 opencv TensorFlow python3 dlib 具体搭建方式— 传送门 流程1. Get_faces 获取图片集获取所要识别的人脸的图片集最快的方法就是使用程序用摄像头拍照，要获取足够多的照片，得坐在电脑面前不停得给自己的脸摆各种姿势，这样可以提高训练后识别自己的成功率，这里先后采取了opencv和dlib自带人脸检测采集，虽然opencv识别比较快，但实际应用中会识别出一些奇怪的东西当作人脸, 所以程序采用了dlib来识别人脸。 dlib识别人脸获取图片集 2. Set_other_people 获取其他人脸图片集人脸图片集网上有很多，这里给出其中一个: 网址 下载 下载解压到项目目录下的input_img目录下,接下来使用dlib来批量识别图片中的人脸部分，并保存到指定目录other_faces下. dlib批量识别其他图片集 3. Train_faces 训练模型通过 CNN 训练数据,训练之后的数据会保存在当前目录下。 CNN 训练数据 4. Is_my_face s使用模型进行识别使用摄像头拍拍摄，在画面中标注出人脸。 进行识别","tags":[{"name":"环境配置","slug":"环境配置","permalink":"http://wenbo.fun/tags/环境配置/"},{"name":"人脸识别","slug":"人脸识别","permalink":"http://wenbo.fun/tags/人脸识别/"}]},{"title":"Windows配置opencv3+python3+tensorflow环境","date":"2017-11-24T07:21:04.000Z","path":"2017/11/24/Win10配置opencv3+python3环境/","text":"Requirements WIN10/WIN7 64bit Anaconda3-5.0.1-Windows-x86_64 opencv_python-3.3.1+contrib-cp35-cp35m-win_amd64.whl python3.5 numpy-1.13.3+mkl-cp35-cp35m-win_amd64机器学习相关 ↓ dlib-18.17.100-cp35-none-win_amd64.whl tensorflow-1.4.0-cp35-cp35m-win_amd64.whl scikit_learn-0.19.1-cp35-cp35m-win_amd64.whl 注： 版本可以自己选择，只要注意版本号对上就行。其他额外依赖自行下载。 Anaconda概述Anaconda是一个用于科学计算的Python发行版，支持 Linux, Mac, Windows系统，提供了包管理与环境管理的功能，可以很方便地解决多版本python并存、切换以及各种第三方包安装问题。Anaconda利用工具/命令conda来进行package和environment的管理，并且已经包含了Python和相关的配套工具。 这里先解释下conda、anaconda这些概念的差别。conda可以理解为一个工具，也是一个可执行命令，其核心功能是包管理与环境管理。包管理与pip的使用类似，环境管理则允许用户方便地安装不同版本的python并可以快速切换。Anaconda则是一个打包的集合，里面预装好了conda、某个版本的python、众多packages、科学计算工具等等，所以也称为Python的一种发行版。其实还有Miniconda，顾名思义，它只包含最基本的内容——python与conda，以及相关的必须依赖项，对于空间要求严格的用户，Miniconda是一种选择。 conda的设计理念——conda将几乎所有的工具、第三方包都当做package对待，甚至包括python和conda自身！因此，conda打破了包管理与环境管理的约束，能非常方便地安装各种版本python、各种package并方便地切换。 Conda的常用操作1234567891011121314151617181920212223242526272829# 查看当前环境下已安装的包conda list# 查看某个指定环境的已安装包conda list -n python34# 查找package信息conda search numpy# 安装packageconda install -n python34 numpy# 如果不用-n指定环境名称，则被安装在当前活跃环境# 也可以通过-c指定通过某个channel安装# 更新packageconda update -n python34 numpy# 删除packageconda remove -n python34 numpy# 更新conda，保持conda最新conda update conda# 更新anacondaconda update anaconda# 更新pythonconda update python# 假设当前环境是python 3.4, conda会将python升级为3.4.x系列的当前最新版本 创建python环境 123456789101112# 创建环境conda create -n your_env_name python=X.X（2.7、3.6等）#使用激活(或切换不同python版本)的虚拟环境,使用python --version可以检查当前python版本是否为想要的。activate your_env_name#虚拟环境中安装额外的包conda install -n your_env_name [package]#关闭虚拟环境。deactivate#删除虚拟环境conda remove -n your_env_name --all#删除环境中的某个包conda remove --name $your_env_name $package_name 配置创建一个python3.5环境安装完成后，打开Anaconda Navigator程序,创建一个新环境。 选择一个python版本，这里选择3.5.环境名称可以随便取。 鼠标左键点击进入 terminal。 进入你下载的文件夹进行pip安装numpy,注意版本要匹配。 安装opencvpip安装opencv_python。 进入python，import cv2测试，没报错一般就是成功了。 安装 dlib 和 scikit_learn同上，略。 注：dlib 建议下载最新版本。 安装 TensorFlowpip安装 tensorflow-1.4.0-cp35-cp35m-win_amd64. 测试：","tags":[{"name":"环境配置","slug":"环境配置","permalink":"http://wenbo.fun/tags/环境配置/"},{"name":"Opencv","slug":"Opencv","permalink":"http://wenbo.fun/tags/Opencv/"},{"name":"Python","slug":"Python","permalink":"http://wenbo.fun/tags/Python/"},{"name":"TensorFlow","slug":"TensorFlow","permalink":"http://wenbo.fun/tags/TensorFlow/"}]},{"title":"GIT与GITHUB的使用","date":"2017-09-22T09:59:54.000Z","path":"2017/09/22/GIT与GITHUB的使用/","text":"绑定 SSH在本地电脑安装完GIT、Hexo或jekyll，在自己GITHUB账户建立了一个yourname.github.io的respository后，我们可以绑定 ssh来更加方便地push或pull我们的代码。 获取并绑定 SSH key打开git bash，输入ssh-keygen -t rsa命令，指定 RSA算法生成密钥，回车三次，密码不用输，生成两个隐藏文件id_rsa和id_rsa.pub，一般在下面位置。 Linux 系统：~/.ssh Mac 系统：~/.ssh Windows 系统：C:\\Documents and Settings\\username.ssh 接下来就把公钥id_rsa.pub的内容拷贝到 GitHub需要的地方就可以了。如下： 验证是否成功，在Git Bash中输入ssh -T git@github.com输入yes进行测试： 如图就是成功。 用 GIT 提交代码到 Github情况1：本地没 Git 仓库直接将远程仓库clone到本地。通过clone命令创建的本地仓库，其本身就是一个 Git 仓库了，不用咱们再进行init初始化操作啦，而且自动关联远程仓库。咱们只需要在这个仓库进行修改或者添加等操作，然后commit即可。 引用于通过 Git 将代码提交到 GitHub 情况2： 本地有 Git 仓库我们新建一个仓库做演示，进入 git bash，初始化仓库。进入该“仓库”，git init初始化,然后，输入git remote add origin https://github.com/yourname/test.git命令，关联远程仓库，其中origin为远程仓库的名字(默认，可以自己取)： git pull origin master命令，同步远程仓库和本地仓库： 若第一次使用，会叫你设置邮箱和名字，按照说明做就是。 再回到本地仓库，会发现远程的README已经同步到了本地，接下来，在本地仓库新建一个名为test.txt的测试文件,输入git add和git commit命令，将文件test.txt添加并提交到本地仓库，再输入git push origin master命令，将本地仓库修改（或者添加）的内容提交到远程仓库： 最后看看是否提交成功： 更多关于Git： Git Community Book 中文版 强调：在咱们向远程仓库提交代码的时候，一定要先进行pull操作，再进行push操作，防止本地仓库与远程仓库不同步导致冲突的问题，尤其是第二种提交代码的情况，很容易就出现问题。","tags":[{"name":"GIT","slug":"GIT","permalink":"http://wenbo.fun/tags/GIT/"}]}]